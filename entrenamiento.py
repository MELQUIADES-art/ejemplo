print("Hola, este script se ejecuta en Azure ML üöÄ")

for i in range(3):
    print(f"Iteraci√≥n {i}")
import pandas as pd
import numpy as np
import datetime
from datetime import timedelta
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
from scipy import stats
from sklearn.preprocessing import RobustScaler, MinMaxScaler
datos=pd.read_excel(r"C:\Users\MELQUIADES\Desktop\datos.xlsx")
datos["Extensi√≥n"]=np.array(datos["Extensi√≥n"],dtype='str')
datos["a√±o"]=np.array(datos["a√±o"],dtype='str')
# -*- coding: utf-8 -*-
"""AnalisisUsoRecursosDipVall.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uEC0GfEWocC0FvS8dFrLQxoizpdr8TAz

# AN√ÅLISIS EXPLORATORIO DE DATOS Y TECNICAS DE MACHINE LEARNING PARA DETENCI√ìN DE ANOMALIAS EN EL SECTOR P√öBLICO

## Resumen
El objetivo principal de este proyecto es detectar posibles anomal√≠as en los datos de uso de tel√©fonos por parte de trabajadores p√∫blicos, proporcionados por la Diputaci√≥n de Valladolid.

En primer lugar se realizar√° un an√°lisis exploratorio de datos, examinando la estructura de nuestro dataframe, el tipo de  variables y la existencia de valores faltantes. A continuaci√≥n se efectuar√° una descripci√≥n estad√≠stica de las caracter√≠sticas principales de los datos, incluyendo el c√°lculo de la media, frecuencia, varianza, percentiles y mediana para los datos num√©ricos y para los posibles datos categ√≥ricos, la moda y la frecuencia.

Normalmnete una descripci√≥n estadistica es insuficiente para comprender con detalle el comportamiento de nuestras variables, por se graficar√°  la distribuci√≥n de las variables. Se emplear√°n histogramas y graficos de barras para datos num√©ricos y categ√≥ricos respectivamente. Si las funciones de distribuci√≥n de las variables no son normales, se utilizar√°n gr√°ficos de caja o bigotes (boxplots) para analizar los posibles valores at√≠picos (outliers) y tambi√©n graficos de violin para mostrar como se comportan los datos de manera mas visual a partir de la informaci√≥n que nos proporcionan las anteriores gr√°ficas. Adem√°s, se realizar√° un an√°lisis multivariante mediante matrices de correlaci√≥n, gr√°ficos de dispersi√≥n y mapas de calor, con el fin de identificar relaciones relevantes entre variables. Para la comparaci√≥n entres datos num√©ricos y categ√≥ricos har√© un diagrama de dispersi√≥n distinguiendo el tipo de clase al que pertenece cada dato. Dependiendo tambi√©n del contexto de los datos, se har√°n tambi√©n otras graficas como diagramas de sectores, mapas de calor, gr√°ficas de pareto o graficos de radares.


Una vez analizados rigurosamente los datos, se proceder√° a una limpieza  de los mismos, detectando valores at√≠picos o vac√≠os. Dependiendo del modelo que se aplique o de la importancia de la variable, se eliminar√° la fila correspondiente o se sustituir√° el valor faltante por la media o la mediana de los valores restantes.

Dado que ciertos modelos de machine learning dependen de la distancia entre observaciones, se aplicar√° una normalizaci√≥n de los datos que depender√° de la existencia de outliers y de la distribuci√≥n nuestras caracter√≠sticas.

Una vez aplicados los algoritmos de machine learning, se analizar√°n y visualizar√°n los resultados para explicar las razones por las que determinados terminales han sido identificados como an√≥malos.

Finalmente se proceder√° a una investigaci√≥n exhaustiva sobre el uso de algoritmos de machine learning para la detenci√≥n de anomalias en el sector p√∫blico, comparando los modelos implementados en este estudio con los descritos en la literatura especializada.

A partir de esta comparaci√≥n, se extraer√°n conclusiones sobre los algoritmos de aprendizaje no supervisado m√°s utilizados en la detecci√≥n de anomal√≠as y las razones de su efectividad en contextos p√∫blicos.

## Analisis de datos
La diputaci√≥n de Valladolid nos ha proporcionado cort√©smente datos sobre el n√∫mero, duraci√≥n y coste de las llamadas de todos sus trabajadores al mes desde enero de 2024 a julio de 2025. Pasemos a una visualizaci√≥n de las primeras 10 filas de nuestro dataframe.
"""



# Librer√≠as
import pandas as pd
import numpy as np
import datetime
from datetime import timedelta
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
from scipy import stats
from sklearn.preprocessing import RobustScaler, MinMaxScaler
import seaborn as sns
from matplotlib.patches import Patch
from mpl_toolkits.mplot3d import Axes3D
from scipy.interpolate import griddata
import plotly.graph_objects as go

datos.head(10)

len(datos.columns)

"""donde nuestro dataframe tiene 255424 datos distribuidos en 26 variables y 9824 registros que corresponden a cada terminal o trabajador de la diputaci√≥n de valladolid en un mes y a√±o concreto.

"""

print("\nN√∫mero total de datos:",datos.size)
print("\n\n Tama√±o de los datos (Entradas,Caracter√≠sticas):",datos.shape)

"""cada caracter√≠stica representa lo siguiente:

| **Variable** | **Descripci√≥n** |
|---------------|----------------|
| **Extensi√≥n** | N√∫mero identificador del trabajador o terminal de la Diputaci√≥n de Valladolid. |
| **Metropolitanas** | Cantidad de llamadas realizadas dentro del √°rea metropolitana de Valladolid. |
| **Provinciales** | Cantidad de llamadas realizadas dentro de la provincia de Valladolid. |
| **Nacionales** | Cantidad de llamadas realizadas a destinos dentro de Espa√±a. |
| **Internacionales** | Cantidad de llamadas realizadas a o desde otros pa√≠ses. |
| **M√≥viles** | N√∫mero de llamadas efectuadas desde o hacia tel√©fonos m√≥viles. |
| **Otras** | Llamadas no clasificadas en las categor√≠as anteriores (por ejemplo, de emergencia, corporativas, gratuitas o de tarificaci√≥n adicional). |
| **Desconocidas** | Llamadas cuyo n√∫mero de origen o destino no se muestra (ocultas o privadas). |
| **Salientes Duraci√≥n** | Duraci√≥n total de las llamadas salientes (en formato hh:mm:ss). |
| **Salientes Duraci√≥n (s)** | Duraci√≥n total de las llamadas salientes en segundos. |
| **Entrantes Num.LLamadas** | N√∫mero total de llamadas entrantes recibidas por los trabajadores. |
| **Entrantes Duraci√≥n** | Duraci√≥n total de las llamadas entrantes (hh:mm:ss). |
| **Entrantes Duraci√≥n (s)** | Duraci√≥n total de las llamadas entrantes en segundos. |
| **Internas Num.LLamadas** | N√∫mero de llamadas internas entre extensiones de la Diputaci√≥n. |
| **Internas Duraci√≥n** | Duraci√≥n total de las llamadas internas (hh:mm:ss). |
| **Internas Duraci√≥n (s)** | Duraci√≥n total de las llamadas internas en segundos. |
| **Entrantes** | Total de llamadas recibidas desde fuera de la organizaci√≥n. |
| **Salientes** | Total de llamadas realizadas hacia fuera de la organizaci√≥n. |
| **Internas entrantes** | Llamadas internas recibidas dentro de la organizaci√≥n. |
| **Internas salientes** | Llamadas internas realizadas dentro de la organizaci√≥n. |
| **Num.LLamadas** | N√∫mero total de llamadas realizadas y recibidas. |
| **Duraci√≥n** | Duraci√≥n global de todas las llamadas (hh:mm:ss). |
| **Duraci√≥n (s)** | Duraci√≥n global de todas las llamadas en segundos. |
| **Coste** | Coste econ√≥mico total asociado a las llamadas. |
| **a√±o** | A√±o en el que se registraron los datos. |
| **mes** | Mes en el que se registraron los datos. |

Veamos el tipo de dato de cada caracter√≠stica y si hay valores nulos
"""

info_df = pd.DataFrame({
    'Columna': datos.columns,
    'Tipo de dato': datos.dtypes,
    'Valores no nulos': datos.notnull().sum(),
    'Valores nulos': datos.isnull().sum(),
    'Porcentaje nulos (%)': round(datos.isnull().mean() * 100, 2)
})

info_df.style.background_gradient(cmap='Blues')

"""Vemos que ninguna variable tiene valores nulos. Tenemos 5 variables de tipo object, 1 de tipo float y el resto de tipo int. Por el contexto de los datos, nos inetersa que las variables de duraci√≥n sean tratadas como variables temporales para ver cuantas horas minutos segundos y d√≠as ha estado un trabajador o terminal al telefono y ver cuales han estado mas y cuales menos. Tambi√©n nos interesa que las varibles registro y a√±o sean de tipo object."""

datos["Extensi√≥n"]=np.array(datos["Extensi√≥n"],dtype='str')
datos["a√±o"]=np.array(datos["a√±o"],dtype='str')
def funcionTiempo(x):
    if (x.count('h')==0):
        if(x.count('m')==0):
            x='00:00:'+x
            x=x.replace('s','')
        else:
            x='00:'+x
            if(x.count('s')==0):
                x=x.replace('m',':')
                x=x+'00'
            else:
                x=x.replace('m ',':')
                x=x.replace('s','')
    else:
        if(x.count('m')==0):
            if(x.count('s')==0):
                x=x.replace('h',':00:00')
            else:
                x=x.replace('h ',':00:')
                x=x.replace('s','')
        else:
            if(x.count('s')==0):
                x=x.replace('h ',':')
                x=x.replace('m',':00')
            else:
                x=x.replace('h ',':')
                x=x.replace('m ',':')
                x=x.replace('s','')

    h, m, s = map(int, x.split(':'))
    x = timedelta(hours=h, minutes=m, seconds=s)
    return x
datos["Duraci√≥n"]=datos["Duraci√≥n"].apply(funcionTiempo)
datos["Entrantes Duraci√≥n"]=datos["Entrantes Duraci√≥n"].apply(funcionTiempo)
datos["Internas Duraci√≥n"]=datos["Internas Duraci√≥n"].apply(funcionTiempo)
datos["Salientes Duraci√≥n"]=datos["Salientes Duraci√≥n"].apply(funcionTiempo)
info_df = pd.DataFrame({
    'Columna': datos.columns,
    'Tipo de dato': datos.dtypes,
    'Valores no nulos': datos.notnull().sum(),
    'Valores nulos': datos.isnull().sum(),
    'Porcentaje nulos (%)': round(datos.isnull().mean() * 100, 2)
})

info_df.style.background_gradient(cmap='Blues')

"""donde timedelta64 es un tipo de variable del paquete datetime que se usa para indicar la duraci√≥n en d√≠as, horas, minutos y segundos.
Tratar los datos no consiste solamente ver si faltan valores o si las carcater√≠sticas tienen el tipo de datos que queremos, sino que tambi√©n tenemos que ver si nuestro dataframe cumple lo que dijimos en las caracter√≠sticas de nuestros datos. En este caso tenemos que ver si la suma de todos los tipos de llamadas es igual para cada terminal el Num.LLamadas y si los tipos de duraci√≥n son la Duraci√≥n y Duraci√≥n (s).


"""

n1=datos.shape[0]
S=0
campos_num_llamadas = [
    "Metropolitanas",
    "Provinciales",
    "Nacionales",
    "Internacionales",
    "M√≥viles",
    "Otras",
    "Desconocidas",
    "Entrantes Num.LLamadas",
    "Internas Num.LLamadas",
    "Entrantes",
    "Salientes",
    "Internas entrantes",
    "Internas salientes"
]
suma_campos = datos[campos_num_llamadas].sum(axis=1)

if (suma_campos == datos['Num.LLamadas']).all():
    print("Todos los registros est√°n bien puestos por el contexto de nuestros datos en cuanto a n√∫mero de llamadas")
else:
    print("Hay registros que no est√°n bien puestos por el contexto de nuestros datos en cuanto a n√∫mero de llamadas")

"""No hay ningun problema en cuanto al n√∫mero de llamadas. Veamos si los hay con la duraci√≥n de las mismas."""

n1=datos.shape[0]
S=0
campos_duracions = [
    "Entrantes Duraci√≥n (s)",
    "Salientes Duraci√≥n (s)",
    "Internas Duraci√≥n (s)"
]
campos_duracion = [
    "Entrantes Duraci√≥n",
    "Salientes Duraci√≥n",
    "Internas Duraci√≥n"
]
suma_campos = datos[campos_duracions].sum(axis=1)

if (suma_campos == datos['Duraci√≥n (s)']).all():
    print("Todos los registros est√°n bien puestos por el contexto de nuestros datos en cuanto a duraci√≥n de las llamadas en segundos")
else:
    print("Hay registros que no est√°n bien puestos por el contexto de nuestros datos en cuanto a duraci√≥n de las llamadas en segundos")

suma_campos = datos[campos_duracion].sum(axis=1)

if (suma_campos == datos['Duraci√≥n']).all():
    print("Todos los registros est√°n bien puestos por el contexto de nuestros datos en cuanto a duraci√≥n de las llamadas\t")
else:
    print("Hay registros que no est√°n bien puestos por el contexto de nuestros datos en cuanto a duraci√≥n de las llamadas")

"""Pasemos ahora a una descripci√≥n estad√≠stica general de los datos. Primero pasemos a la descripci√≥n estadistica de los datos categ√≥ricos (a√±o, mes y extensi√≥n)"""

from IPython.display import HTML

tabla = datos.describe(include='object').to_html(classes="table table-striped table-hover")

HTML("""
<link rel="stylesheet"
href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
""" + tabla)


#count indica el n√∫mero de datos de cada variable categ√≥rica

#unique indica el n√∫mero de tipos de datos que hay de cada variable categ√≥rica, es decir, en el caso de a√±o hay solo 2 datos distintos, 2024 y 2025.

#top es la moda de cada variable categ√≥rica

#freq es la frecuencia con la que se repite esta variable categ√≥rica

"""Tambi√©n podemos mostrar la frecuencia absoluta y relativa de la variable a√±o,"""

from IPython.display import display, HTML

abs_table = datos["a√±o"].value_counts().rename("Frecuencia absoluta").to_frame()
rel_table = datos["a√±o"].value_counts(normalize=True).rename("Frecuencia relativa").to_frame()

display(HTML(f"""
<table>
<tr>
<td>{abs_table.to_html()}</td>
<td>{rel_table.to_html()}</td>
</tr>
</table>
"""))

"""y mes"""

from IPython.display import display, HTML

abs_table = datos["mes"].value_counts().rename("Frecuencia absoluta").to_frame()
rel_table = datos["mes"].value_counts(normalize=True).rename("Frecuencia relativa").to_frame()

display(HTML(f"""
<table>
<tr>
<td>{abs_table.to_html()}</td>
<td>{rel_table.to_html()}</td>
</tr>
</table>
"""))

"""Gracias a las tablas de frecuencias absolutas de a√±o y mes y a la descripci√≥n estad√≠stica de las variables categ√≥ricas puedo concluir que faltan registros de terminales en cada mes.

En el resumen estad√≠stico de las caracter√≠sticas categ√≥ricas, me aparec√≠an que hab√≠an en total 612 registros diferentes. Parecer√≠a l√≥gico que en cada mes y a√±o hubieran 612 registros. Esto nos dar√≠a por los meses repetidos en 2024 y 2025 un total de 612*2=1224 registros, sin embargo el mes qeu mas registros tiene es febrero con 1050.

La supusici√≥n mas l√≥gica que se pueda hacer es que esos registros que no aparecen sean rellenados por 0 y sus correspondientes mes y a√±o. Sin embargo al hacer eso no podr√© dectertar terminales de poco uso al implementar tecnicas de machine learning. Otra idea es sustituir estos valores por la mediana pero no afectar√≠a para bien el rendimiento de los algoritmos. He pensado que la mejor opci√≥n es dejar el dataframe como estaba y para un posterior estudio crear otro dataframe y calcular las llamadas acumuladas. A partir de ese nuevo dataframe volver√© a aplicar tecnicas de machine learnig para la detenci√≥n de anomalias correspondientes a terminales de poco uso.
"""

datos.columns

import pandas as pd
import numpy as np
from IPython.display import display, HTML

# ===============================
# PARTE 1: C√ÅLCULO DE REGISTROS FALTANTES
# ===============================

datos2 = datos.copy()

dt = pd.crosstab(
    datos["Extensi√≥n"],
    [datos["a√±o"], datos["mes"]]
)

dt1 = dt.cumsum()

m1 = len(dt1.columns)
n1 = len(dt1.index)

dt11 = pd.DataFrame(columns=["Extensi√≥n", "a√±o", "mes"])
datos2 = pd.DataFrame(columns=datos.columns)

for j in range(m1):
    for i in range(1, n1):
        if dt1.loc[dt1.index[i], dt1.columns[j]] == dt1.loc[dt1.index[i-1], dt1.columns[j]]:
            dt2 = pd.DataFrame({
                'Extensi√≥n': [dt1.index[i]],
                'a√±o': [dt1.columns[j][0]],
                'mes': [dt1.columns[j][1]]
            })

            dt112 = pd.DataFrame({
                'Extensi√≥n': [dt1.index[i]],
                'Metropolitanas': 0,
                'Provinciales': 0,
                'Nacionales': 0,
                'Internacionales': 0,
                'M√≥viles': 0,
                'Otras': 0,
                'Desconocidas': 0,
                'Salientes Duraci√≥n': 0,
                'Salientes Duraci√≥n (s)': 0,
                'Entrantes Num.LLamadas': 0,
                'Entrantes Duraci√≥n': 0,
                'Entrantes Duraci√≥n (s)': 0,
                'Internas Num.LLamadas': 0,
                'Internas Duraci√≥n': 0,
                'Internas Duraci√≥n (s)': 0,
                'Entrantes': 0,
                'Salientes': 0,
                'Internas entrantes': 0,
                'Internas salientes': 0,
                'Num.LLamadas': 0,
                'Duraci√≥n': 0,
                'Duraci√≥n (s)': 0,
                'Coste': 0,
                'a√±o': [dt1.columns[j][0]],
                'mes': [dt1.columns[j][1]]
            })

            dt11 = pd.concat([dt11, dt2], ignore_index=True)
            datos2 = pd.concat([datos2, dt112], ignore_index=True)

import pandas as pd
import numpy as np
from IPython.display import display, HTML

# ===============================
# PREPARACI√ìN DE DATOS
# ===============================

dt11["a√±o"] = dt11["a√±o"].astype(int).astype(str)

meses = [
    "enero", "febrero", "marzo", "abril", "mayo", "junio",
    "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre"
]

year = ['2024', '2025']

# ===============================
# MOSTRAR TABLAS 100% HORIZONTALES
# ===============================

print("\n=== TABLAS QUE MUESTRAN LOS REGISTROS FALTANTES POR MES Y A√ëO ===\n")

for y in year:
    html = f"<h2>A√±o {y}</h2>"

    # NO flex-wrap ‚Üí una sola fila horizontal
    html += "<div style='display:flex; flex-wrap:nowrap; overflow-x:auto;'>"

    for m in meses:
        df1 = dt11[(dt11["a√±o"] == y) & (dt11["mes"] == m)]

        if df1.empty:
            continue

        df_style = (
            df1.style
            .set_properties(**{
                'background-color': '#fdfdfd',
                'border': '1px solid #ccc',
                'padding': '6px',
                'font-size': '11px'
            })
            .set_table_styles([
                {
                    'selector': 'th',
                    'props': [
                        ('background-color', '#4A90E2'),
                        ('color', 'white'),
                        ('padding', '6px')
                    ]
                }
            ])
            .set_caption(f"{m.capitalize()} {y}")
        )

        html += f"""
        <div style="margin-right:20px; min-width:300px;">
            {df_style.to_html()}
        </div>
        """

    html += "</div>"
    display(HTML(html))

"""Pasemos a la descripci√≥n estad√≠stica del resto de datos. Tanto para los datos num√©ricos como los de deltatime utilizaremos las mismas estadisticas descriptivas. Pero antes nos interesa que las variables de tipo int sean de tipo float."""

datos_num = datos.select_dtypes(include=['int', 'float'])
datos_num=datos_num.astype(float)
datos_num.describe()

"""Obviamente por el contexto de los datos todas las variables num√©ricas son positivas. Por la descripci√≥n estad√≠stica, las variables Metropolitanas, Internacionales y Desconocidas son nulas.  

Por otro lado en las variables Provinciales, Otras e Internas entrantes los cuartiles Q1 y Q3 son nulos. Por consiguiente, aplicando el text de Turkey, puedo concluir que los terminales que hayan hecho llamadas de alguno de estos tres tipos son at√≠picos y por consiguinte las variables num√©ricas son nulas.

Estas variables no ser√°n utlizadas para la implementaci√≥n de los algoritmos de machine learning para detenci√≥n de anomalias ni siquera ser√°n utilizadas para el analisis univariado o multivariado al igual que las variables nulas (Metropolitanas, Internacionales y Desconocidas). Sin embagro ser√°n empleadas para el analisis multivariado entre variables categ√≥ricas y num√©ricas.
Visualizemos otra vez el resumen estad√≠stico quitando las varibles nulas.
"""

datos=datos.drop("Metropolitanas",axis=1)
datos=datos.drop("Internacionales",axis=1)
datos=datos.drop("Desconocidas",axis=1)
datos_num2 = datos.drop(columns=["Provinciales","Otras","Internas entrantes"])
datos_num2.describe()

"""La media normalmente nos indica el valor t√≠pico o significativo de esa variable, y la desviaci√≥n nos indica que tan agrupados est√°n los datos de la media, sin embargo la informaci√≥n que nos proporcionen puede verse afectada por posibles outliers. En este caso, y si la distribuci√≥n de los datos es sesgada, puede resultarnos mas util la mediana y los quartiles que nos dan una informaci√≥n √∫til de c√≥mo se concentran la mayor√≠a de los datos.
Podemos observar en la tabla anterior que hay outliers en nuestras variables num√©ricas ya que el maximo dista bastante del tercer cuartil. Sin embargo en la secci√≥n de analisis univariado esto se apreciar√° mejor.

Para los tipos de llamadas al menos el 25% son nulas, un porcentaje bastante alto. Por la mediana y los cuartiles se puede concluir que en la mayor√≠a de los casos se han realizado pocas llamadas salvo para Internas Num.LLamadas.

Las desviaciones con respecto a la media son elevadas pero esto no necesariamente indica que los datos sigan una distribucion muy alargada. Este dato esta siendo afectado por los outliers. Es por lo tanto que se necesitan visualizar histogramas para comprender mejor el comportamiento de estas variables. Tambi√©n se necesitaran visualizar diagramas de bigotes para entender como estan repartidos los datos, detectar valores at√≠picos visualemnte y ver que tan sim√©trica es la distribuci√≥n y que tan largas son sus colas. En la secci√≥n de analisis univariado combinaremos la informaci√≥n que nos proporciona los diagramas de bigotes y los histogramas a partir de violin plot, que muestra donde se concentarn los datos al igual que el diagrama de bigotes pero con la diferencia de que se puede visualizar como se distribuyen a partir de la adici√≥n de un diagrama de densidad kernel rotado a cada lado.

La media de la variable coste no se ve tan afectada por los posibles outliers en comparaci√≥n con las variables Num.Llamadas y Duraci√≥n (s). Seguramente esto se deba por la diferencia de escala entre las tres variables.

Para todas variables menos el coste hay una gran diferencia entre la media o la mediana lo cual puede ser signo de asimetr√≠a, sin embargo, como dije antes, la media no es una medida confiable por la presencia de outliers.  
La mayor√≠a de terminales habr√°n realizado como mucho 150 llamadas por el text de Turkey. (Q3+(Q3-Q1)*1.5)~150), donde la mayor√≠a de tipos de llamadas, salvo Internas Num.LLamadas, no sobrepasan las 10.


 Valores muy por encima ser√≠an casos excepcionales (outliers). Como se puede observar la mayor√≠a de terminales no han supuesto un coste grande, como mucho ha sido menos de 1 euro, sin embargo, han habido casos de terminales que han sobrepasado los 10 euros. Porbablemente por el contexto de los datos estos terminales habr√°n relizado otros tipos de llamadas que no sean internas y salientes.

 Tambi√©n tenemos las variables que indican la duraci√≥n en segundo de las llamadas y la duraci√≥n total. Para este tipo de variables prefiero analizarlas segun los d√≠as, horas, minutos y segundos.

 Tambi√©n podemos observar la gran diferencia que hay entre el valor maximo y la media o mediana en todas las variables. Esto ser√° un inconveniete para graficar histogramas y boxplots, ya que al hacer el calculo de intervalos de frecuencia en el caso de histogramas o los rangos intercuartilicos en el caso del boxplots, no se apreciara la forma de la distribuci√≥n porque python intenta graficar todos los resultados incluidos los valores extremos. Es por eso que en la siguiente secci√≥n graficaremos los histogrmas y boxplots generales antes y despu√©s de aplicar la funci√≥n logaritmo, funci√≥n muy utilizada tanto en estadistica y analisis de datos como en matem√°tica aplicada, que logra reducir el impacto visual que ocasionan valores muy grandes.

 En el resumen estad√≠stico se aprecia una gran diferencia de escala entre las variables que describen el n√∫mero de llamadas, las variables que describen la duraci√≥n de las mimas y la variable coste. Es por lo tanto que en el momento de aplicar m√©todos de machine learning para detencion de outliers, dependientes del calculo de distancias, se aplicar√°n tecnicas de normalizaci√≥n de variables para reducir el impacto de las variables de duraci√≥n frente a las de coste y n√∫mero de llamadas. Dependiendo del impacto de los outliers y de la distribuci√≥n de las variables se emplear√° la tecnica de escalado de variables adecuada.
"""

datos.describe(include='timedelta64[ns]')

"""AL igual que pasaba en los anteriores datos, la media se desv√≠a bastante de la mediana por posiblemente la existencia de outliers. En el analisis univariado mostrar√© los histogramas y gr√°ficos de viol√≠n para comprender mejor la asimetr√≠a, sesgos y colas que poseen todas las variables num√©ricas. Lo mas tipico es que las llamadas no sobrepasen la hora y quarto, pero ha habido casos que han llegado a durar d√≠as a lo largo del mes. Seguramente uno de los terminales es centralita, que es el sistema (f√≠sico o digital) que gestiona todas las comunicaciones telef√≥nicas internas y externas de la instituci√≥n, sirviendo como punto de conexi√≥n entre los diferentes departamentos y con el p√∫blico. Se puede apreciar en el resumen estad√≠stico que las llamadas mas duraderas son las internas, mientras que las otras no tienen mucha duraci√≥n. Esto se pueda deber a que las llamadas que reciben o realizan los trabajadores a personal no perteneciente a la diputaci√≥n hayan sido peque√±as consultas poco duraderas, pero los tramites realizados dentro de la diputaci√≥n seguramente lleven bastante mas tiempo. Sin embargo al haber varios tipos de llamadas registradas en nuestros datos, se puede apreciar en la anterior tabla que se realizaron mas llamadas externas que internas.

Una descripci√≥n estad√≠stica de los datos siempre nos da informaci√≥n valiosa pero no suficiente para comprender del todo los datos. Es por tanto que son necesarias visualizaciones.

## Analisis Univariado

Pasemos ahora a una descripci√≥n visual de las variables num√©ricas. Como dije al principio con la descripci√≥n estad√≠stica, se pod√≠a observar la gran diferencia que hab√≠a entre el maximo y el tercer quartil en varias variables. Eso hace que python por norma tome anchos muy grandes para visualizar las frecuencias en los histogramas, lo cual ocasionar√° que solo se pueda ver una barra en las variables donde hay enormes diferencias entre el maximo y tercer quartil.
"""

plt.figure(figsize=(3,3))
axes = datos_num2.hist(
    figsize=(12, 10),
    bins=20,
    color='tomato',
    edgecolor='black'
)


for ax in axes.flatten():
    ax.set_title(ax.get_title(), fontsize=14, fontweight='bold')
    ax.set_xlabel(ax.get_xlabel(), fontsize=12)
    ax.set_ylabel("Frecuencia", fontsize=12)


plt.suptitle("Distribuci√≥n de Variables Num√©ricas", fontsize=18, y=1.02)

plt.tight_layout()
plt.show()

"""Como coment√© el ancho de banda est√° en torno a cero ya que la mayor√≠a de valores distan bastante del m√°ximo. Tambi√©n en la tabla del analisis estad√≠stico de las variables num√©ricas se pod√≠a observar como el primer cuartil se alejaba bastante del tercero y mas si intentamos calcular el limite superior utilizado en el test de Turkey. Esto har√° que python visualice los boxplots como cajas casi apreciables en comparaci√≥n con los bigotes."""

plt.figure(figsize=(10,10))
ax = datos_num2.boxplot(figsize=(12, 8), patch_artist=True)


for box in ax.artists:
    box.set(facecolor='lightblue', edgecolor='black', alpha=0.7)


for line in ax.lines:
    line.set(color='black', linewidth=1)



plt.title("Distribuci√≥n de Variables Num√©ricas (Boxplots)", fontsize=18, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.ylabel("Valor", fontsize=12)

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""Es por tanto que aplicar√© la funci√≥n logaritmo para cada variable. Gracias a esto se reducir√° el efecto que tienen los grandes outliers frente al resto de valores y por consiguiente podremos sacar mejor informaci√≥n sobre el comportamiento de las variables. En estas gr√°ficas no visualizar√© las variables  Provinciales, Otras e Internas entrantes ya que por lo que coment√© son nulas salvo que haya algun valor at√≠pico, es decir, un terminal que haya realizado alguna llamada de ese tipo. Primero empecemos con los histogramas"""

datos_num_log=datos_num2.copy()
columnasnumlog=datos_num_log.select_dtypes(include=['int', 'float']).columns
datos_num_log[columnasnumlog]=datos_num_log[columnasnumlog].apply(lambda x:np.log(x+1))#sumo 1 para evitar el problema log(0)

plt.figure(figsize=(3,3))
axes = datos_num_log.hist(
    figsize=(12, 10),
    bins=20,
    color='tomato',
    edgecolor='black'
)


for ax in axes.flatten():
    ax.set_title(ax.get_title(), fontsize=14, fontweight='bold')
    ax.set_xlabel(ax.get_xlabel(), fontsize=12)
    ax.set_ylabel("Frecuencia", fontsize=12)


plt.suptitle("Distribuci√≥n de Variables Num√©ricas", fontsize=18, y=1.02)

plt.tight_layout()
plt.show()

"""Vemos que hay para cada variable una gran cantidad de valores cercanos al cero. ESto har√° que los algoritmos de machine learning se centren en detectar telefonos que se hayan utlizado mucho en vez de detectar telefonos que se usen anormalmente poco. Es por tanto que para la detecni√≥n de anomalias sumar√© la duraci√≥n y el n√∫mero de llamadas que realiza cada terminal durante los 19 meses.

Vemos que las variables nacionales y m√≥viles siguen una distribuci√≥n muy parecida, ambas asim√©tricas, con una larga cola hacia la derecha. La variable salientes duraci√≥n (s) est√° sesgada a la izquierda a diferencia de las otras dos pero con la diferencia que, a parte del 0, tiene un leve pico intermedio. Esto puede significar que la variable saliente duraci√≥n(s) sigue una distribucci√≥n gaussiana.
La variable Entrantes Num.LLamadas esta sesgada hacia la derecha como las variables nacionales y m√≥viles, pero su cola no es tan larga, adem√°s se pueden apreciar picos cercanos a 0. Esto nos puede dar a entender que pueda ser la mediana de dicha variable.

Las variables entrantes duraci√≥n(s) e internas duraci√≥n (s) siguen al igual que salientes duraci√≥n(s), una dsitribucci√≥n gausiana, pero con la diferencia que en la variable entrantes duraci√≥n(s) tiene un pico poco pronunciado y la segunda es bastante mas pronunciado. La variable Internas n√∫mero de llamadas est√° algo sesgada hacua la derecha, pero las colas son muy cortas en comparaci√≥n con el resto de variables que he comentado, es por tanto que puede seguir una distribuccion de poisson. LAs variables entrantes, saleintes, internas salientes y coste siguen la misma distribucci√≥n que las variables nacionales y m√≥viles, pero la variable entrante tiene una cola menos alargada.

La variable n√∫m de llamadas tiene grandes picos que tienden hacia la derecha, con una peque√±a cola hacia la izquierda. Esto puede indicar que dicha variable pueda seguir una distribucci√≥n de possion, pero a diferencia de la variable Internas Num.llamadas, esta pos√© menor desviaci√≥n t√≠pica, lo cual hace que la mayor√≠a de valores se concentren en torno a la media y no est√©n mas dispersos.

La variable duraci√≥n sigue al igual que las otras de duraci√≥n, sigue una distribucci√≥n gamma pero con un pico mas pronunciado. Gracias a los histogramas hemos visto visualmente la simetr√≠a de esas variables, que valor es mas com√∫n y qu√© tan apuntada o achatada es la forma de dichos datos. SIn embagro no es un buen gr√°fico para visualizar los posibles outliers. Es por tanto que utilizar√© los diagramas de bigotes para ver que variables son las que tienen mas outliers.

"""

plt.figure(figsize=(10,10))
ax = datos_num_log.boxplot(figsize=(12, 8), patch_artist=True)


for box in ax.artists:
    box.set(facecolor='lightblue', edgecolor='black', alpha=0.7)


for line in ax.lines:
    line.set(color='black', linewidth=1)



plt.title("Distribuci√≥n de Variables Num√©ricas (Boxplots)", fontsize=18, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.ylabel("Valor", fontsize=12)

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""Con estos boxplots podemos ver como est√°n distribuidos los outliers. Vemos que para la variable nacionales hay una gran cantidad de otliers cercanos al limite superior establecido por el text de Tukey. Para moviles se puede apreciar que dichos outliers est√°n aun mas proximos. Lo mismo ocurre con las variables coste, salientes e internas salientes, pero quizas por intentar comparar todas las varaibles y por la diferencia de escala, no se pueda apreciar con detalle. LAs variables que no contienen outlires son Salientes Duraci√≥n(s), Entrantes Duraci√≥n (s), internas Num de llamadas e internas duraci√≥n (s), es por tanto que medidas como la media y la desviaci√≥n t√≠pica pueden ser mas √∫tiles que lo que se podr√≠a pensar al principio. Por ejemplo para salientes duraci√≥n, la media si puede ser un valor mas com√∫n de lo que pensaba y el hecho de que la desviaci√≥n t√≠pica sea muy alargada podr√≠a indicar que hay mucha dispersi√≥n, tal y como se muestran en los histogramas. Lo mismo ocurre para entrantes duraci√≥n(S). Para internas num.llamdas e internas duraci√≥n (s) no hay tanta dispersi√≥n de los datos. En estos casos tambi√©n la diferencia entre la media y la mediana puede indicar que tan sim√©tricas son las variables.
Para las variables entrantes num de llamadas y entranteshay muchos outliers cerca del limite superior del test de turkey pero hay unos cuantos alejados de √©l. En estos casos la media y la desviaci√≥n t√≠pica pueden cobrar poca importancia en la forma de los datos. Al igual que estas variables, la variable  Num.llamadas  tiene outlires muy grandes en comparaci√≥n con el limitie superior del test de tukey.  La variable que mas me ha llamado la atenci√≥n  sobre detenci√≥n de outliers es duraci√≥n (s) ya que es la √∫nica que tiene outliers por debajo, pero esto se pod√≠a haber visto en el resumen estadistico del principio donde se observaba que salvo la variabl duraci√≥n (s), los primeros cuartiles eran 0 o muy cercanos a 0.
Los diagramas de bigotes adem√°s de detectar outliers pueden ser utiles para tener una idea clara sobre la distribuci√≥n y concebtraci√≥n de datos.

AMbas variables, nacionales y m√≥viles, tienen la caja hacia la izquierda del todo, lo cual implica que son muy sesgadas hacia la derecha, como se aprecia en los histogramas. A diferencia de las anteriores, las variables salientes duarcion en segundos y entrantes duraci√≥n en segundos  no muestran una asmiter√≠a hacia la derecha. Seguramente se deba a la cantidad de llamadas de duraci√≥n 0 que pueda haber. Sin embargo con los boxplots se puede garantizar que tan larga es la cola. En este caso la cola de entrantes duraci√≥n es algo mas larga.

La variable entrantes es la que tiene los outliers mas dispersos con respecto a salientes e internas salientes, pero seguramente sea la mas concentrada, con la cola menos largas, a diferencia de internas salientes, que ser√≠a la mas alargada. Ambas son sim√©tricas por la derecha como se muestra en los histogramas.

La variable internas num.llamadas esta mas sesgada hacia la derecha y en este caso tiene una cola hacia la derecha bastante mas larga pero una cola hacia la izquierda existente. La variabla Internas duraci√≥n es algo mas proporcional pero con una cola mas alargada hacia la izquierda como se puede apreciar tanto en el boxplot e histograma. LA variableNum.llamadas est√° mas alargada hacia la derecha pero con cola hacia la izquierda, mostrando la misma simetr√≠a que el resto de variables de duraci√≥n salvo salientes. La varaible duraci√≥n (s) esta iguial de alargada por ambos lados.
La variable coste es muy asim√©trica hacia la izquierda, mostrando mayor dispresi√≥n que el resto de variables y una cola pronunciada hacia la derecha.

Tambi√©n la caja sin bigotes nos puede dar informaci√≥n de como se concentran lso datos, al tener la media hacia la derecha, en duraci√≥n, Num LLamadas,Internas duraci√≥n , Internas Numero de llamadas, Entrantes duraci√≥n y salientes duraci√≥n. Otras variables como Nacionales, Moviles, Entrantes, Salientes y Coste, se concentran hacia la izquierda. Esto explica por que hay ciertos picos pronunciados en nuestros histogramas.

Una gr√°fica que recoge la informaci√≥n visualizada con mayor detalle, tanto de los histogramas como los diagramas de bigotes, son los diagramas de violin. Basicamente son diagramas de cajas pero con  informaci√≥n mejorada mediante la adici√≥n de un diagrama de densidad kernel rotado a cada lado. Sin embargo este diagrama de densidad no ser√° muy fiable para variablescon outliers, sobre todo aqueyas que tiene outlieres muy alejados del limite superior del test de tukey, ya que el KDE es un estimador de la densidad que depende de las muestras proporcionadas.
"""

# Figura m√°s grande
plt.figure(figsize=(10, 6))

# Violinplot con mejoras est√©ticas
sns.violinplot(
    data=datos_num_log,
    inner="box",         # muestra caja interna con mediana
    linewidth=1,         # bordes m√°s definidos
    palette="viridis",   # paleta moderna (puedes probar: "coolwarm", "mako", "rocket")
    cut=0,               # evita extender los violines m√°s all√° de los datos
    bw=0.3               # suaviza la forma
)

# T√≠tulos y etiquetas
plt.title("Distribuciones Log-transformadas de Variables Num√©ricas", fontsize=16, weight="bold", pad=15)
plt.xlabel("Variables", fontsize=13)
plt.ylabel("Valor Log-transformado", fontsize=13)

# Opcional: rotar etiquetas del eje X si hay muchas variables
plt.xticks(rotation=30, ha="right")

# Ajuste final
sns.despine()  # quita bordes innecesarios
plt.tight_layout()
plt.show()

"""donde la rayita blanca es la mediana de los valores. Como se apreciaban en los histogramas, hay una concentraci√≥n de 0's para cada variable, pero dicha concentraci√≥n es mayor para las variables de tipo duraci√≥n y para la variable coste. Para las de tipo duraci√≥n no hay tanta concentraci√≥n debido a que se cuentan los segundos de las llamdas, es decir, aunque hayan habido registros que no hayan hecho llamadas, ha habido otros cuantos uqe han durado poco durante la llamada. Es por tanto que no hay tanta concentraci√≥n de 0's, que se distribuye mejor la duraci√≥n. Como dije en los boxplots, algunas variables como Num.llamadas, Entrantes y Entrantes. Num.Llamadas tienen colas muy pronunciadas en relacion con otras variables. Otra cosa interesante es que en la mayor√≠a de variables, la mediana difiere bastante de la moda, salvo para aquellas variables donde la mayor√≠a de valores son 0 o muy cercanos.

Como visualizaciones adicionales puedo hacer un diagrama de pie donde se observe que porcentaje visual tienen las llamadas nacionales, moviles, provinciales, otras y dem√°s tipos de llamadas.
"""

import matplotlib.pyplot as plt

# --- Datos (usa los tuyos reales) ---
Numllamadas = [
    datos["Provinciales"].sum(),
    datos["Nacionales"].sum(),
    datos["M√≥viles"].sum(),
    datos["Otras"].sum(),
    datos["Entrantes Num.LLamadas"].sum(),
    datos["Internas Num.LLamadas"].sum(),
    datos["Entrantes"].sum(),
    datos["Salientes"].sum(),
    datos["Internas entrantes"].sum(),
    datos["Internas salientes"].sum()
]
colores= np.array(["red","green","blue","yellow","pink","black","orange","purple","beige","brown"])

columnas1 = [
    "Provinciales", "Nacionales", "M√≥viles", "Otras",
    "Entrantes Num.LLamadas", "Internas Num.LLamadas",
    "Entrantes", "Salientes", "Internas Entrantes", "Internas Salientes"
]


plt.figure(figsize=(8,8))
plt.pie(
    Numllamadas,
    startangle=90,
    colors=colores,
    wedgeprops={'linewidth': 1, 'edgecolor': 'white'}
)

plt.legend(columnas1, title="Tipo de llamada", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
plt.title("Distribuci√≥n porcentual del n√∫mero de llamadas en 2024", fontsize=14, fontweight='bold')
plt.axis('equal')
plt.show()

"""La mayor√≠a de llamadas que se han realizado a lo largo de estos 19 meses son entrantes. Aproximadamente representa el 50%. LA que la sigue son el numero de llamdas internas que aproximadamente representa el 25%. Despu√©s estan el resto siendo la menos frecuente las internas entrantes seguidas de las salientes y despu√©s las internas salientes

Veamos ahora como se distribuyen las duraciones de las llamadas
"""

import matplotlib.pyplot as plt

# --- Datos (usa los tuyos reales) ---
Numllamadas = [
    datos["Salientes Duraci√≥n (s)"].sum(),
    datos["Entrantes Duraci√≥n (s)"].sum(),
    datos["Internas Duraci√≥n (s)"].sum()
]

columnas1 = [
    "Salientes Duraci√≥n (s)","Entrantes Duraci√≥n (s)","Internas Duraci√≥n (s)"
]


plt.figure(figsize=(8,8))
plt.pie(
    Numllamadas,
    startangle=90,

    colors=colores,
    wedgeprops={'linewidth': 1, 'edgecolor': 'white'}
)

plt.legend(columnas1, title="Duracion (s) de los tipos de llamadas", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
plt.title("Distribuci√≥n porcentual de la duraci√≥n de las llamadsa en 2024", fontsize=14, fontweight='bold')
plt.axis('equal')
plt.show()

"""Es curioso este grafico. A pesar de que el n√∫mero de llamadas que predominan son las entrantes, la duraci√≥n de las mismas no. Como se apreciaba en en el anterior diagrama de pie, no hab√≠a muchas llamadas salientes que se hubieran realizado, sin embargo la duraci√≥n de las mismas es similar a la de las entrantes e internas. Estos dos diagramas de pie nos indican que aunque la duraci√≥n total sea similar, las salientes normalmente han durado bastante m√°s que las internas y las internas mas que las entrantes, siendo estas √∫ltimas las de menos duraci√≥n.

Tambi√©n podr√≠amos visualizar los top 30 terminales que mas llamadas totales han realizado.
"""

X=datos["Extensi√≥n"].unique()
N=len(X)
L=list()
L1=list()
L2=list()
for i in range(N):
    dt=datos[datos["Extensi√≥n"]==X[i]]
    L.append(dt["Coste"].sum())
    L1.append(dt["Num.LLamadas"].sum())
    L2.append(dt["Duraci√≥n (s)"].sum())
X1=pd.DataFrame({"Extensi√≥n":X, "Coste":L})
X2=pd.DataFrame({"Extensi√≥n":X, "Num.LLamadas":L1})
X3=pd.DataFrame({"Extensi√≥n":X, "Duraci√≥n (s)":L2})
X11=X1.sort_values(by=['Coste'],ascending=False)
X12=X2.sort_values(by=['Num.LLamadas'],ascending=False)
X13=X3.sort_values(by=['Duraci√≥n (s)'],ascending=False)

plt.figure(figsize=(10,5))
bars = plt.bar(X12["Extensi√≥n"][:30], X12["Num.LLamadas"][:30], color='green')
plt.xlabel("Extensi√≥n")
plt.ylabel("N√∫mero de Llamadas")
plt.title("Top 30 extensiones seg√∫n n√∫mero de llamadas")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Vemos que hay una gran diferencia entre el primero (7100) y el resto de los casos. En este caso se cumple el principio de Pareto (Aproximadamente el 80 % de los efectos proviene del 20 % de las causa, que en nuestro caso se traduce a El 20 % de las extensiones hacen el 80 % de las llamadas). Este principio permite determinar la diferencia entre el outlier de gran tama√±o con respecto al resto y cuanta influencia tiene en la variable. Podemos hacer el diagrama de pareto en esacala logaritmica para apreciar como evolucionan el n√∫mero de llamadas."""

plt.figure(figsize=(10,5))
bars = plt.bar(X12["Extensi√≥n"][:30], X12["Num.LLamadas"][:30], color='green')
plt.plot(X12["Extensi√≥n"][:30], X12["Num.LLamadas"][:30].cumsum(), color='red',marker='o')
plt.xlabel("Extensi√≥n")
plt.ylabel("N√∫mero de Llamadas")
plt.title("Top 30 extensiones seg√∫n n√∫mero de llamadas y curva de Pareto")
plt.xticks(rotation=45)
plt.tight_layout()
plt.yscale('log')
plt.show()

"""En esta gr√°fica de pareto se observa que los terminales con mas llamadas predominan en el conteo del n√∫mero total de llamadas que se produjeron durante estos 19 meses frente al resto de terminales. Entonces seguramente la distribucion de los tipos de llamadas puedan depender de los terminales que mas llamadas han hecho.

Tambi√©n podr√≠amos aplicar el principio de Pareto sobre la duraci√≥n de llamadas.
"""

plt.figure(figsize=(10,5))
bars = plt.bar(X13["Extensi√≥n"][:30], X13["Duraci√≥n (s)"][:30], color='green')
plt.plot(X13["Extensi√≥n"][:30], X13["Duraci√≥n (s)"][:30].cumsum(), color='red',marker='o')
plt.xlabel("Extensi√≥n")
plt.ylabel("Duraci√≥n (s)")
plt.title("Top 30 extensiones seg√∫n la Duraci√≥n (s) y curva de Pareto")
plt.xticks(rotation=45)
plt.tight_layout()
plt.yscale('log')
plt.show()

"""Vemos que el primer terminal coincide con la anterior gr√°fica y que no hay grandes cambios de posici√≥n con los terminales mas llamados. Al igual que para el N√∫mero de llamadas se sigue cumpliendo el principio de Pareto pero en este caso no es tan apreciable. Los terminales que mas llamadas han hecho influyen bastante en el diagrama de pie para duraci√≥n pero no tanto como lo hac√≠an en el de llamadas.

Tambi√©n podr√≠amos hacer un diagrama de Pareto para la variable coste
"""

plt.figure(figsize=(10,5))
bars = plt.bar(X11["Extensi√≥n"][:30], X11["Coste"][:30], color='green')
plt.plot(X11["Extensi√≥n"][:30], X11["Coste"][:30].cumsum(), color='red',marker='o')
plt.xlabel("Extensi√≥n")
plt.ylabel("Coste")
plt.title("Top 30 extensiones seg√∫n el Coste y curva de Pareto")
plt.xticks(rotation=45)
plt.tight_layout()
plt.yscale('log')
plt.show()

"""Sigue apreciandose el principio de Pareto (la curva se va aplanando bastante en las √∫ltimas terminales) pero con bastante menos fuerza que en ek resto de los casos. Adem√°s los terminales que mas costes han generado con sus llamadas no son necesariamente los que mas han llamado o durado con las llamadas realizadas. SIn embargo siguen apreciandose algunos terminales que han llamado muchas veces y por tanto han generado bastantes costes (como 7777).

## Analisis Multivariado

### Categ√≥ricas vs Categ√≥ricas

Como dije al principio, cada terminal determina un registro dentro de un mes y a√±o. Tambi√©n como dije antes, los datos que nos proporciono la diputaci√≥n de Valladolid eran registrs de llamadas de trabajadores por mes y a√±o, desde enero de 2024 hasta julio de 2025. Con esta informaci√≥n no es necesario hacer un analisis bivariado entre variables categ√≥ricas.

### Categ√≥ricas vs Num√©ricas.

En este analisis podr√≠amos extraer gr√°ficas de bastante interes donde nos muestren comparativas entre a√±os  y meses y tambi√©n como se distribuyen las variables segun el mes y a√±o, y que diferencia hay al respecto. Empecemos visualizando los histogramas de las variables num√©ricas pero esta vez diferenciando el a√±o.
"""

import warnings
warnings.filterwarnings("ignore")
num_vars = datos_num_log.select_dtypes(include=['int', 'float']).columns
grupo = "a√±o"

fig, axes = plt.subplots(4, 4, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

for i, var in enumerate(num_vars):
    ax = axes[i]

    # Selecci√≥n correcta de datos
    grupo1 = datos_num_log[datos_num_log[grupo] == "2024"][var].dropna()
    grupo2 = datos_num_log[datos_num_log[grupo] == "2025"][var].dropna()

    # Rango com√∫n
    bins = 20
    rango = (min(datos_num_log[var]), max(datos_num_log[var]))

    # Histogramas
    hist1, bins_edges = np.histogram(grupo1, bins=bins, range=rango)
    hist2, _ = np.histogram(grupo2, bins=bins, range=rango)

    # Graficar espejo
    ax.barh(bins_edges[:-1], hist1, height=np.diff(bins_edges),
            alpha=0.7, label='2024')
    ax.barh(bins_edges[:-1], -hist2, height=np.diff(bins_edges),
            alpha=0.7, label='2025')

    # L√≠nea vertical (0)
    ax.axvline(0, color='black', linewidth=0.8)

    ax.set_title(var, fontsize=13)
    ax.set_xlabel("Frecuencia (2024 / 2025)")
    ax.set_ylabel("Valor")
    ax.grid(alpha=0.3)
    axes[i].legend()
    xticks = ax.get_xticks()               # Obtiene las posiciones actuales
    ax.set_xticklabels([abs(int(x)) for x in xticks])  # Etiquetas absolutas
# Ocultar ejes vac√≠os
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

fig.suptitle("Pir√°mides comparativas de las distribuciones de variable num√©rica por a√±o (2024 vs 2025)", fontsize=18)
plt.tight_layout()
plt.show()

"""Los comportamientos son los mismos para los dos a√±os, salvo que los picos no son tan pronunciados en el a√±o 2025, sobretodo en las variables de duraci√≥n. Para ver con mas detalle la diferencia entre los a√±os para cada variable, visualiaremos diagramas de bigotes y de violin."""

import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid")

fig, axes = plt.subplots(4, 4, figsize=(20, 12))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

for i, var in enumerate(num_vars):

    ax = axes[i]

    sns.boxplot(
        data=datos_num_log,
        x='a√±o',
        y=var,
        palette='Set2',
        width=0.6,
        linewidth=1.2,
        fliersize=3,
        ax=ax
    )

    ax.set_title(f"Distribuci√≥n de {var} (2024 vs 2025)", fontsize=11)
    ax.set_xlabel("")
    ax.set_ylabel(var, fontsize=10)
    ax.grid(alpha=0.25)
    ax.tick_params(axis='x', rotation=0, labelsize=9)

# Ocultar subplots vac√≠os
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

# T√≠tulo general
fig.suptitle(
    "Boxplots comparativos de variable num√©rica (2024 vs 2025)",
    fontsize=20,
    fontweight="bold",
    y=1.02  # separa el t√≠tulo de la figura
)

# Ajuste crucial para evitar solapamientos
plt.subplots_adjust(
    hspace=0.6,  # incrementa espacio vertical entre subplots
    wspace=0.3   # espacio horizontal
)

plt.tight_layout(rect=[0, 0, 1, 0.96])  # deja √°rea libre para el t√≠tulo general
plt.show()

"""Todas las variables presentan el mismo comportamiento para cada a√±o. Las √∫nicas diferencias menores ser√≠an que en 2024 las colas son un poco m√°s largas  y que normalmente en 2024 los datos est√°n un poco mas dispersos. Eso implica que tendr√°n mayor sesgo. Tambi√©n hay outliers mas grandes en 2024 que en 2025. Podr√≠amos ver una mejor visualizaci√≥n a partir de diagramas de violines"""

import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid")

fig, axes = plt.subplots(4, 4, figsize=(20, 12))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

for i, var in enumerate(num_vars):

    ax = axes[i]

    sns.violinplot(
        data=datos_num_log,
        x='a√±o',
        y=var,
        inner='box',
        palette='Set2',
        ax=ax
    )

    ax.set_title(f"Distribuci√≥n de {var} (2024 vs 2025)", fontsize=11)
    ax.set_xlabel("")
    ax.set_ylabel(var, fontsize=10)
    ax.grid(alpha=0.25)
    ax.tick_params(axis='x', rotation=0, labelsize=9)

# Ocultar subplots vac√≠os
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

# T√≠tulo general
fig.suptitle(
    "Violinplot comparativos de variable num√©rica (2024 vs 2025)",
    fontsize=20,
    fontweight="bold",
    y=1.02  # separa el t√≠tulo de la figura
)

# Ajuste crucial para evitar solapamientos
plt.subplots_adjust(
    hspace=0.6,  # incrementa espacio vertical entre subplots
    wspace=0.3   # espacio horizontal
)

plt.tight_layout(rect=[0, 0, 1, 0.96])  # deja √°rea libre para el t√≠tulo general
plt.show()

"""Como dije al pirnicpio la distribuci√≥n general de los datos es similar para los dos a√±os para cada variable, solo que para algunas variables (como Entrantes) la cola es mas larga.

En la variable coste la cola de 2025 es mas larga que la de 2024, sin embargo los diagramas de bigotes supuestamente dicen lo contrario. Esto no es una contradicci√≥n, simplemente python al graficar los diagramas de violines utiliza KDE, un estimador de la densidad que tiene en cuenta los outliers. SI nos fijamos bien en el diagrama de bigotes, podemos observar que hay un outlier de mayor tama√±o en 2025 que los de 2024 en la variable coste, sin embargo si no tenemos en cuenta las anomalias producidas,la variable coste en 2024 tiende a ser mas alargada en 2025.Es solo que los diagramas de violines tambi√©n representan los outliers al intentar graficar la funci√≥n de densidad.

Tambi√©n podriamos hacer una comparativa entre los meses de cada a√±o de como se comportan cada variable con boxplots y violinplots. (No emplear√© histogramas debido a que al ser 12 meses para 2024 (7 para 2025), puede ser poco visual).
"""

import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid")

for j in ['2024','2025']:
    fig, axes = plt.subplots(4, 4, figsize=(20, 12))
    plt.subplots_adjust(wspace=0.3, hspace=0.4)
    axes = axes.flatten()

    for i, var in enumerate(num_vars):
        ax = axes[i]

        sns.boxplot(
            data=datos_num_log[datos_num_log["a√±o"] == j],
            x='mes',
            y=var,
            palette='Set2',
            width=0.6,
            linewidth=1.2,
            fliersize=3,
            ax=ax
        )

        ax.set_title(f"Distribuci√≥n de {var} ({j})", fontsize=11)
        ax.set_xlabel("")
        ax.set_ylabel(var, fontsize=10)
        ax.grid(alpha=0.25)
        ax.tick_params(axis='x', rotation=45, labelsize=9)

    # Ocultar subplots vac√≠os
    for k in range(i + 1, len(axes)):
        axes[k].set_visible(False)

    # T√≠tulo general de la figura
    fig.suptitle(
        f"Boxplots comparativos de variable num√©rica para cada mes del a√±o {j}",
        fontsize=20,
        fontweight="bold",
        y=1.02
    )

    # Ajuste de espacio
    plt.subplots_adjust(hspace=0.6, wspace=0.3)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()  # Se muestra una figura antes de la siguiente

"""En el a√±o 2024, la variable nacionales tiene mayor dispersi√≥n en el mes de octubre.  En todos los meses la mediana tiende hacia el primer quartil, lo cual implica que hay una asimetr√≠a positiva y por tanto las colas tienden hacia la derecha. En el mes donde menos llamadas nacionales se producen es agosto. La mayoria se acumulan en torno a 0. Adem√°s ees el mes que mas outliers posee. Como la mallor√≠a de trabajadores se tomar√°n vacaciones en ese mes y otros pocos no, se producxe ese comportamiento. Ademas de agosto, tambi√©n se observan outliers en enero, marzo, julio y septiembre.

En la variable M√≥viles los meses de enero, febrero, abril, mayo y nombiembre, la mediana esta centrada, lo cual significa que hay simetr√≠a en la concentraci√≥n de datos cercana a 0 pero no es una simetr√≠a global, ya que hay un pronunciado sesgo hacia la derecha. En el mes donde mas se concentran las llamadas hacia el 0 es en agosto y es el √∫nico mes donde se observa outliers.

Para la distribuci√≥n salientes duraci√≥n (s) y entrantes duraci√≥n (s) hay una gran concentraci√≥n de datos hacia la derecha con colas tambi√©n hacia la derecha pero no tan largas como en las anteriores variables.

En las distribuci√≥n entrantes Num.Llamdas, entrantes, salientes internas salientes y costes, en el mes donde mas concentraci√≥n de datos hay es en agosto. Los datos se acumulan en torno a 0 en todos los meses y adem√°s est√°n muy sesgados hacia la derecha. En todos los meses para la variable Num.LLamadas se obesrvan una gran concentraci√≥n de outlires cerca del limite superior del test de tukey y unos pocos. En la variable internas salientes hay pocos outliers proximos al limite superior del test de tukey. Para el resto de variables hay mas outliers concentrados fuera del bigote.

Para las variables Internas Num.LLamadas e internas duraci√≥n (s) se aprecia una cola hacia la izquierda (mas alargada en la variable internas duraci√≥n (s)) a diferencia del resto de variables que comente, menos para el mes de agosto. Sin embargo en dicho mes se concentran mas los datos en torno al tercer quartil y no tanto en el 0. Estas variables no muestran outliers en ning√∫n mes.

Para la variable num.llamadas ya todos los meses tienen una cloa hacia la izquierda, con presencia de pocos ooutliers.

Para la variable duraci√≥n, ya las variables est√°n mas sesgadas hacia la derecha, observandose ootliers en la parte inferior y no en la superior. La variable para cada mes es mas sim√©trica que las anteriores pero con un sesgo hacia la izquierda.


Para el a√±o 2025, el comportamiento de las variables para cada mes  (de enero a julio) es el mismo, salvo que hay muchos menos outliers, excepto para moviles donde  a diferencia de 2024, hay outlieres en febrero y marzo. Todas las avriables tienen el mismo sesgo y simetr√≠a con respecto a las de 2024. Las variables internas numero de llaamdas e internas duraci√≥n presentan una cola a la izquierda excepto para el mes de julio. El resto de las caracter√≠sticas son similares a las del 2024.

Para ver mejor el comportamiento de las variables y la distribuci√≥n que estas siguen para cada mes y a√±o concreto, podemos visualizar un diagrama de violin.
"""

import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid")

for j in ['2024','2025']:
    fig, axes = plt.subplots(4, 4, figsize=(20, 12))
    plt.subplots_adjust(wspace=0.3, hspace=0.4)
    axes = axes.flatten()

    for i, var in enumerate(num_vars):
        ax = axes[i]

        sns.violinplot(
            data=datos_num_log[datos_num_log["a√±o"] == j],
            x='mes',
            y=var,
            palette='Set2',
            width=0.6,
            linewidth=1.2,
            ax=ax
        )

        ax.set_title(f"Distribuci√≥n de {var} ({j})", fontsize=11)
        ax.set_xlabel("")
        ax.set_ylabel(var, fontsize=10)
        ax.grid(alpha=0.25)
        ax.tick_params(axis='x', rotation=45, labelsize=9)

    # Ocultar subplots vac√≠os
    for k in range(i + 1, len(axes)):
        axes[k].set_visible(False)

    # T√≠tulo general de la figura
    fig.suptitle(
        f"Violinplot comparativos de variable num√©rica para cada mes del a√±o {j}",
        fontsize=20,
        fontweight="bold",
        y=1.02
    )

    # Ajuste de espacio
    plt.subplots_adjust(hspace=0.6, wspace=0.3)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()  # Se muestra una figura antes de la siguiente

"""Los comportamientos de las variables que describ√≠ en los diagramas de bigotes son similares a los observados en los violines, solo que habr√° algunas variaciones sobre las colas por la presencia de outliers.

Podemos ver como se distribuyen los tipos de llamadas con repescto al a√±o en un diagrama de pie.
"""

year = ['2024','2025']
colores = ["red","green","blue","yellow","pink","black","orange","purple","beige","brown"]

columnas1 = [
    "Provinciales", "Nacionales", "M√≥viles", "Otras",
    "Entrantes Num.LLamadas", "Internas Num.LLamadas",
    "Entrantes", "Salientes", "Internas Entrantes", "Internas Salientes"
]

fig, axes = plt.subplots(1, 2, figsize=(16, 8))
plt.subplots_adjust(wspace=0.3, hspace=0.4)  # dos gr√°ficos lado a lado

for i, val in enumerate(year):
    Numllamadas = [
        datos[datos['a√±o']==val]["Provinciales"].sum(),
        datos[datos['a√±o']==val]["Nacionales"].sum(),
        datos[datos['a√±o']==val]["M√≥viles"].sum(),
        datos[datos['a√±o']==val]["Otras"].sum(),
        datos[datos['a√±o']==val]["Entrantes Num.LLamadas"].sum(),
        datos[datos['a√±o']==val]["Internas Num.LLamadas"].sum(),
        datos[datos['a√±o']==val]["Entrantes"].sum(),
        datos[datos['a√±o']==val]["Salientes"].sum(),
        datos[datos['a√±o']==val]["Internas entrantes"].sum(),
        datos[datos['a√±o']==val]["Internas salientes"].sum()
    ]

    axes[i].pie(
        Numllamadas,
        startangle=90,
        colors=colores,
        wedgeprops={'linewidth': 1, 'edgecolor': 'white'}
    )

    axes[i].set_title(f"Distribuci√≥n porcentual del n√∫mero de llamadas en {val}", fontsize=14, fontweight='bold')
    axes[i].legend(columnas1, title="Tipo de llamada", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
    axes[i].axis('equal')  # para que el gr√°fico sea circular

plt.tight_layout()
plt.show()

"""Vemos que la distribuci√≥n del tipo de llamadas es la misma que en el caso general. Veamos l distribuci√≥n del tipo de duraci√≥n de llamada"""

year = ['2024','2025']
colores = ["red","green","blue","yellow","pink","black","orange","purple","beige","brown"]

columnas1 = [
    "Salientes Duraci√≥n (s)","Entrantes Duraci√≥n (s)","Internas Duraci√≥n (s)"
]


fig, axes = plt.subplots(1, 2, figsize=(16, 8))
plt.subplots_adjust(wspace=0.3, hspace=0.4)  # dos gr√°ficos lado a lado

for i, val in enumerate(year):
    Numllamadas = [
    datos["Salientes Duraci√≥n (s)"].sum(),
    datos["Entrantes Duraci√≥n (s)"].sum(),
    datos["Internas Duraci√≥n (s)"].sum()
  ]
    axes[i].pie(
        Numllamadas,
        startangle=90,
        colors=colores,
        wedgeprops={'linewidth': 1, 'edgecolor': 'white'}
    )

    axes[i].set_title(f"Distribuci√≥n porcentual de la duraci√≥n (s) de las llamadas en {val}", fontsize=14, fontweight='bold')
    axes[i].legend(columnas1, title="Tipo de llamada", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
    axes[i].axis('equal')  # para que el gr√°fico sea circular

plt.tight_layout()
plt.show()

"""AL igual que el anterior diagrama, no var√≠a con respecto al diagrama de pie general.

Otra gr√°fica que podriamos visualizar son las llamadas acumuladas por mes para cada a√±o
"""

duracionacumulada2024mes=list()
duracioninternaacumulada2024mes=list()
duracionsalienteacumulada2024mes=list()
duracionentranteacumulada2024mes=list()
duracionacumulada2025mes=list()
duracioninternaacumulada2025mes=list()
duracionsalienteacumulada2025mes=list()
duracionentranteacumulada2025mes=list()
datos_2024=datos[datos['a√±o']=='2024']
datos_2025=datos[datos['a√±o']=='2025']
meses = [
    "enero", "febrero", "marzo", "abril", "mayo", "junio",
    "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre"
]


for i in meses:
    duracionacumulada2024mes.append(datos_2024[datos_2024["mes"]==i]["Duraci√≥n (s)"].sum())
    duracioninternaacumulada2024mes.append(datos_2024[datos_2024["mes"]==i]["Internas Duraci√≥n (s)"].sum())
    duracionsalienteacumulada2024mes.append(datos_2024[datos_2024["mes"]==i]["Salientes Duraci√≥n (s)"].sum())
    duracionentranteacumulada2024mes.append(datos_2024[datos_2024["mes"]==i]["Entrantes Duraci√≥n (s)"].sum())

for i in meses[:7]:
    duracionacumulada2025mes.append(datos_2025[datos_2025["mes"]==i]["Duraci√≥n (s)"].sum())
    duracioninternaacumulada2025mes.append(datos_2025[datos_2025["mes"]==i]["Internas Duraci√≥n (s)"].sum())
    duracionsalienteacumulada2025mes.append(datos_2025[datos_2025["mes"]==i]["Salientes Duraci√≥n (s)"].sum())
    duracionentranteacumulada2025mes.append(datos_2025[datos_2025["mes"]==i]["Entrantes Duraci√≥n (s)"].sum())

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
plt.subplots_adjust(wspace=0.3, hspace=0.4)

# Gr√°fico 2024
axes[0].bar(meses, duracionacumulada2024mes, color='mediumseagreen', edgecolor='black')
axes[0].set_title("Duraci√≥n acumulada 2024", fontsize=14, fontweight='bold')
axes[0].set_xlabel("Meses", fontsize=12)
axes[0].set_ylabel("Duraci√≥n acumulada (segundos)", fontsize=12)
axes[0].tick_params(axis='x', rotation=45)


# Gr√°fico 2025
axes[1].bar(meses[:7], duracionacumulada2025mes, color='mediumseagreen', edgecolor='black')
axes[1].set_title("Duraci√≥n acumulada 2025", fontsize=14, fontweight='bold')
axes[1].set_xlabel("Meses", fontsize=12)
axes[1].set_ylabel("Duraci√≥n acumulada (segundos)", fontsize=12)
axes[1].tick_params(axis='x', rotation=45)

# Ajustamos espacio entre gr√°ficos
plt.tight_layout()
plt.show()

"""En 2024 se pasa mucho tiempo  al lado del telefono en enero y m√°s en febrero, pero hay un baj√≥n en marzo (posiblemente por semana santa). Hay una gran subida en abril y a aprtir de hay va decreciendo hasta llegar a a agosto (mes donde la mayor√≠a no trabajan). A partir de ah√≠ hay una subida brusca en septiembre y octubre (mes donde m√°s han durado las llamadas), donde empieza a bajar hasta llegar a diciembre, el mes donde menos duran las llamadas.

En 2025, a diferencia del anterior a√±o, febrero y marzo son los meses donde menos tiempo se ha pasado en llamada. Despu√©s hay un crecimiento moderado hasta mayo, con un peque√±o baj√≥n en los meses de junio y julio.

Veamos si tiene un comportamiento parecido el n√∫mero de llamadas acumuladas
"""

# Listas para almacenar los valores acumulados de n√∫mero de llamadas por mes
numllamadasacumulada2024mes = []
numinternasacumulada2024mes = []
numsalientesacumulada2024mes = []
numentrantesacumulada2024mes = []
numerootrasacumuladas2024mes=[]

numllamadasacumulada2025mes = []
numinternasacumulada2025mes = []
numsalientesacumulada2025mes = []
numentrantesacumulada2025mes = []
numerootrasacumuladas2025mes=[]


# Suponiendo que tienes una lista llamada 'meses' con los nombres o n√∫meros de los meses

# ---- Datos 2024 (12 meses completos) ----
for i in range(12):
    numllamadasacumulada2024mes.append(datos_2024[datos_2024["mes"] == meses[i]]["Num.LLamadas"].sum())
    numinternasacumulada2024mes.append(datos_2024[datos_2024["mes"] == meses[i]]["Internas Num.LLamadas"].sum()+datos_2024[datos_2024["mes"] == meses[i]]["Internas entrantes"].sum()+datos_2024[datos_2024["mes"] == meses[i]]["Internas salientes"].sum())
    numsalientesacumulada2024mes.append(datos_2024[datos_2024["mes"] == meses[i]]["Salientes"].sum())
    numentrantesacumulada2024mes.append(datos_2024[datos_2024["mes"] == meses[i]]["Entrantes"].sum()+datos_2024[datos_2024["mes"] == meses[i]]["Entrantes Num.LLamadas"].sum())
    numerootrasacumuladas2024mes.append(datos_2024[datos_2024["mes"] == meses[i]]["Provinciales"].sum()+datos_2024[datos_2024["mes"] == meses[i]]["Nacionales"].sum()+datos_2024[datos_2024["mes"] == meses[i]]["M√≥viles"].sum()+datos_2024[datos_2024["mes"] == meses[i]]["Otras"].sum())
# ---- Datos 2025 (por ejemplo, 7 meses disponibles) ----
for i in range(7):
    numllamadasacumulada2025mes.append(datos_2025[datos_2025["mes"] == meses[i]]["Num.LLamadas"].sum())
    numinternasacumulada2025mes.append(datos_2025[datos_2025["mes"] == meses[i]]["Internas Num.LLamadas"].sum()+datos_2025[datos_2025["mes"] == meses[i]]["Internas entrantes"].sum()+datos_2025[datos_2025["mes"] == meses[i]]["Internas salientes"].sum())
    numsalientesacumulada2025mes.append(datos_2025[datos_2025["mes"] == meses[i]]["Salientes"].sum())
    numentrantesacumulada2025mes.append(datos_2025[datos_2025["mes"] == meses[i]]["Entrantes"].sum()+datos_2025[datos_2025["mes"] == meses[i]]["Entrantes Num.LLamadas"].sum())
    numerootrasacumuladas2025mes.append(datos_2025[datos_2025["mes"] == meses[i]]["Provinciales"].sum()+datos_2025[datos_2025["mes"] == meses[i]]["Nacionales"].sum()+datos_2025[datos_2025["mes"] == meses[i]]["M√≥viles"].sum()+datos_2025[datos_2025["mes"] == meses[i]]["Otras"].sum())

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
plt.subplots_adjust(wspace=0.3, hspace=0.4)

# Gr√°fico 2024
axes[0].bar(meses, numllamadasacumulada2024mes, color='mediumseagreen', edgecolor='black')
axes[0].set_title("Numero de llamadas acumuladas en 2024 por mes", fontsize=14, fontweight='bold')
axes[0].set_xlabel("Meses", fontsize=12)
axes[0].set_ylabel("N√∫mero de llamadas acumuladas", fontsize=12)
axes[0].tick_params(axis='x', rotation=45)


# Gr√°fico 2025
axes[1].bar(meses[:7], duracionacumulada2025mes, color='mediumseagreen', edgecolor='black')
axes[1].set_title("Numero de llamadas acumuladas en 2025 por mes", fontsize=14, fontweight='bold')
axes[1].set_xlabel("Meses", fontsize=12)
axes[1].set_ylabel("N√∫mero de llamadas acumuladas", fontsize=12)
axes[1].tick_params(axis='x', rotation=45)

# Ajustamos espacio entre gr√°ficos
plt.tight_layout()
plt.show()

"""A diferencia de la duraci√≥n de las llamadas, enero es el mes donde menos llamadas se han hecho. A medida que pasa va produciendo un crecimiento moderado (en febrero hay un subida repntina) hasta mayo.
YA cuando empiezael verano hay un gran baj√≥n. Sin embargo, a diferencia de la duraci√≥n de llamadas, agosto es el mes donde m√°s llamadas se han producido. Me parece demasiado curioso ya que en ese mes faltan la mayor√≠a de trabajadores.
Pienso que esto pueda deberse a los pocos terminales que hicieron mas llamadas, ya que por la curva de pareto, veiamos que esos terminales tenian mucha influencia sobre la variable, m√°s que  en la variable duraci√≥n. Durante el resto de meses el comportamiento es similar al de duraciones.

En 2025 no hay grandes diferencias con respecto al diagrama de barras que hicimos para la variable duraci√≥n. El mes donde mas se han hecho las llamadas es enero, hay un gran bajon en febrero y marzo y despu√©s crece levemente, establizandose en verano.

Tambi√©n podriamos realizar el mismo gr√°fico de barras para la variable coste
"""

# Listas para almacenar los valores acumulados de n√∫mero de llamadas por mes
coste2024mes = []

coste2025mes = []

# Suponiendo que tienes una lista llamada 'meses' con los nombres o n√∫meros de los meses

# ---- Datos 2024 (12 meses completos) ----
for i in range(12):
    coste2024mes.append(datos_2024[datos_2024["mes"] == meses[i]]["Coste"].sum())
    # ---- Datos 2025 (por ejemplo, 7 meses disponibles) ----
for i in range(7):
   coste2025mes.append(datos_2025[datos_2025["mes"] == meses[i]]["Coste"].sum())

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
plt.subplots_adjust(wspace=0.3, hspace=0.6)

# Gr√°fico 2024
axes[0].bar(meses, coste2024mes, color='mediumseagreen', edgecolor='black')
axes[0].set_title("Costes acumulados en 2024 por mes", fontsize=14, fontweight='bold')
axes[0].set_xlabel("Meses", fontsize=12)
axes[0].set_ylabel("Costes acumulados", fontsize=12)
axes[0].tick_params(axis='x', rotation=45)


# Gr√°fico 2025
axes[1].bar(meses[:7], coste2025mes, color='mediumseagreen', edgecolor='black')
axes[1].set_title("Costes acumulados en 2025 por mes", fontsize=14, fontweight='bold')
axes[1].set_xlabel("Meses", fontsize=12)
axes[1].set_ylabel("Costes acumulados", fontsize=12)
axes[1].tick_params(axis='x', rotation=45)

# Ajustamos espacio entre gr√°ficos
plt.tight_layout()
plt.show()

"""El comportamiento de la acumulaci√≥n de costes a lo largo de los meses en 2025 es similar al de la duraci√≥n (s) yel N√∫mero de llamadas, pero con la diferencia que los meses predominantes son abril, mayo y junio.

Em 2024 el comportamiento de la acumulaci√≥n de costes se asemeja mas al de duraciones que al de n√∫mero de llamadas. Sin embargo la variaci√≥n con respecto a los meses es m√°s moderada.  Posiblemente se deba por el diagrama de pareto, donde comente antes que los valores at√≠picos de mayor tama√±o (los registros que mas costes ocasionaron) no tienen tanto efecto en la variable coste, como si lo ten√≠an en la variable duraci√≥n (s) y m√°s en la variable Num.LLamadas.

Podr√≠amos tambi√©n representar una evoluci√≥n temporal acumulativa de las variables duraci√≥n, n√∫mero de llamadas y coste.
"""

datos_2024=datos[datos["a√±o"]=="2024"]
duracion_2024=list()
coste_2024=list()
numllamadas_2024=list()
datos_2025=datos[datos["a√±o"]=="2025"]
duracion_2025=list()
coste_2025=list()
numllamadas_2025=list()
d1=0
c1=0
m1=0
d2=0
c2=0
m2=0
for i in meses:
    d1=d1+datos_2024[datos_2024["mes"]==i]["Duraci√≥n (s)"].sum()
    duracion_2024.append(d1)
    c1=c1+datos_2024[datos_2024["mes"]==i]["Coste"].sum()
    coste_2024.append(c1)
    m1=m1+datos_2024[datos_2024["mes"]==i]["Num.LLamadas"].sum()
    numllamadas_2024.append(m1)
for i in meses[0:7]:
    d2=d2+datos_2025[datos_2025["mes"]==i]["Duraci√≥n (s)"].sum()
    duracion_2025.append(d2)
    c2=c2+datos_2025[datos_2025["mes"]==i]["Coste"].sum()
    coste_2025.append(c2)
    m2=m2+datos_2025[datos_2025["mes"]==i]["Num.LLamadas"].sum()
    numllamadas_2025.append(m2)

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))  # Tama√±o mejorado
plt.style.use("ggplot")

# ---- Estilo de las l√≠neas ----
plt.plot(
    meses,
    duracion_2024,
    color='seagreen',
    marker='o',
    markersize=7,
    linewidth=2,
    label='A√±o 2024'
)

plt.plot(
    meses[0:7],
    duracion_2025,
    color='indianred',
    marker='s',
    markersize=7,
    linewidth=2,
    label='A√±o 2025'
)

# ---- T√≠tulos y etiquetas ----
plt.title("Duraci√≥n acumulativa de las llamadas por mes", fontsize=12, fontweight='bold')
plt.xlabel("Mes", fontsize=10)
plt.ylabel("Duraci√≥n (s) ", fontsize=10)

# ---- Escala, rejilla y rotaci√≥n ----
plt.yscale('log')
plt.xticks(rotation=45)
plt.grid(alpha=0.3, linewidth=0.7)

# ---- Leyenda ----
plt.legend(loc="upper left", fontsize=9)

plt.tight_layout()
plt.show()

"""En el a√±o 2024, en los primeros meses  hay un gran crecimiento de la acumulaci√≥n de llamadas de forma logar¬¥timica hasta llegar a agosto. Esto se debe por lo que coment√© en los graficos de barras de llamadas acumuladas por mes. Durante los primeros meses se observaban peque√±as variaciones en los gr√°ficos de barras, hasta llegar a agosto donde hubo una gran disminuci√≥n.

Ya a partir de agosto se aprecia una subida en la gr√°fica de la duraci√≥n acumultaiva y va creciendo bastante hasta llegar a invierno donde se estabiliza. Este comportamiento se debe por lo observado  por las gr√°ficas de barras anteriores, donde hubo una gran subida entre septiembre y agosto y despu√©s octubre. Una vez que empeco el invierno la duraci√≥n acumulada de las llamdas disminuyo  considerablemente, hasta llegar a diciembre, el mes donde menos llamadas se ha producido.

En el a√±o 2025, se aprecia un crecimiento m√°s lento 3en los primeros meses y despu√©s mas pronunciado hasta que se estabiliza en el √∫ltimo mes. Esto se debe a  que en enero se alcanz√≥ el pick de duraci√≥n de llamadas acumuladas, despu√©s se produjo una gran disminuci√≥n en febrero y marzo, seguido de un gran aumento hasta estabilizarse en verano.

Una observaci√≥n de interes en esta gr√°fica es la diferencia entre 2024 y 2025 a lo largo de los meses, de enero a julio. Vemos en un principio que las dos curvas est√°n muy proximas, pero en febrero y marzo se alejan bastante. Esto se debe a que en la gr√°fiacde barras en 2024 no hab√≠a mucha diferencia entre enero, febrero y marzo pero en 2025 se observaba un gran decrecimiento. Sin embargo la curva de 2025 a partir de abril se va acercando mas a la curva de 2024 debido al crecimiento observado en el gr√°fico de barras de 2025 entre marzo y abril.

Tambi√©n podemos hacer el gr√°fico de crecimiento temporal acumultaivo para el n√∫mero de llamadas y el coste.
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))  # Tama√±o mejorado
plt.style.use("ggplot")

# ---- Estilo de las l√≠neas ----
plt.plot(
    meses,
    numllamadas_2024,
    color='seagreen',
    marker='o',
    markersize=7,
    linewidth=2,
    label='A√±o 2024'
)

plt.plot(
    meses[0:7],
    numllamadas_2025,
    color='indianred',
    marker='s',
    markersize=7,
    linewidth=2,
    label='A√±o 2025'
)

# ---- T√≠tulos y etiquetas ----
plt.title("N√∫mero de llamadas acumuladas por mes", fontsize=12, fontweight='bold')
plt.xlabel("Mes", fontsize=10)
plt.ylabel("Numero de llamadas ", fontsize=10)

# ---- Escala, rejilla y rotaci√≥n ----
plt.yscale('log')
plt.xticks(rotation=45)
plt.grid(alpha=0.3, linewidth=0.7)

# ---- Leyenda ----
plt.legend(loc="upper left", fontsize=9)

plt.tight_layout()
plt.show()

"""En 2024 hay un gran crecimiento entre enero y febrero debido a a la diferencia entre esos meses en el gr√°fico de barras, y por la peque√±a diferencia entre febreo y los meses de primavera, seguido de un decreciemitno en los primeros meses de verano, la curva se estabiliza en  julio. Ya  a partir de ah√≠, debido a que agosto es el mes donde m√°s llamadas se han realizado, por lo mostrado en el gr√°fico de barras, hay un cambio brusco en esta curva que luego se va estabilizando , con peque√±os cambios de direcci√≥n en los primeros meses de oto√±o, ya que por lo observado en el gr√°fico de barras hay un decreciemiento entre agosto y septiembre y despu√©s a una peque√±a subida seguida de un gran decreciemiento en invierno.

Para el a√±o 2025 el comportamiento de la curva es similar al de la variable duraci√≥n ya que los gr√°ficos de barras eran parecidos.

La diferencia entre 2024 y 2052 es bastante interesante. En enero se observa una gran diferencia a favor de 2025 en cuanto al n√∫mero de llamadas acumuladas. Dicha diferencia va disminuyendo hasta tal punto que en abril se alcanzan el mismo n√∫mero de llamadas acumuladas en 2024 y 2025. Ya en primavera 2024 predomina sobre 2025 pero sin que la diferencia vaya aumentando. Esto se debe a que en los gr√°ficos de barras para 2024 no hab√≠a un gran variaic√≥n en los √∫ltimos meses de primavera , al igual que en 2025. Dicha diferencia se acorta en verano, porque en la grafica de barras de 2024 hay un gran decrecimiento en verano mientras en 2025 el decrecimiento es mas moderado.

Podemos representar la misma cruva en la variable coste
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))  # Tama√±o mejorado
plt.style.use("ggplot")

# ---- Estilo de las l√≠neas ----
plt.plot(
    meses,
    coste_2024,
    color='seagreen',
    marker='o',
    markersize=7,
    linewidth=2,
    label='A√±o 2024'
)

plt.plot(
    meses[0:7],
    coste_2025,
    color='indianred',
    marker='s',
    markersize=7,
    linewidth=2,
    label='A√±o 2025'
)

# ---- T√≠tulos y etiquetas ----
plt.title("Costes acumulados por mes", fontsize=12, fontweight='bold')
plt.xlabel("Mes", fontsize=10)
plt.ylabel("Numero de llamadas ", fontsize=10)

# ---- Escala, rejilla y rotaci√≥n ----
plt.yscale('log')
plt.xticks(rotation=45)
plt.grid(alpha=0.3, linewidth=0.7)

# ---- Leyenda ----
plt.legend(loc="upper left", fontsize=9)

plt.tight_layout()
plt.show()

"""En 2024, se aprecia un gran creciiento en los primeros meses y despu√©s se va estabilizando en agosto debido a las peque√±as variaciones observadas en el gr√°fico de barras en los primeros meses y un posterior decrecimiento significativo en verano. Ya a partir de agosto se observa un gran crecimiento, algo m√°s leve que en el de duraciones, debido a que la diferencia entre barras no es tan grande entre septiembre y agosto, estabilizandose en invierno.

La curva de 2025 para coste es similar a las anteriores debido a ala similitud entre los gr√°ficos de barras de las tres variables.

Se observa peque√±a diferencia entre 2024 y 2025 en enero que va aumentando hasta abril y posteriormente, sobretodo en verano, se estabiliza esa diferencia, debido a que en 2025 hab√≠a mucha diferencia entre enero y febreo y marzo  pero en 2024 hay una leve variaci√≥n entre estos tres meses. Ya en abril hasta julio en 2025 no se aprecian variacionos significativas en el gr√°fico de barras, a diferencia de 2024 donde se aprecia una considerable disminuci√≥n entre mayo y junio.

Tambi√©n podriamos visualizar un gr√°fico de barras de las variables que componen duraci√≥n y num.llamadas a lo largo de los meses en 2024 y 2025
"""

width = 0.25
mes = np.arange(12)

pos1 = mes + width
pos2 = mes
pos3 = mes - width

fig, axes = plt.subplots(2, 1, figsize=(18, 10))
fig.subplots_adjust(wspace=5)
axes = axes.flatten()

axes[0].bar(pos1, duracioninternaacumulada2024mes, width=width,
            color='tab:red', alpha=0.8, edgecolor='black', label='Interna')
axes[0].bar(pos2, duracionentranteacumulada2024mes, width=width,
            color='tab:blue', alpha=0.8, edgecolor='black', label='Entrante')
axes[0].bar(pos3, duracionsalienteacumulada2024mes, width=width,
            color='gold', alpha=0.8, edgecolor='black', label='Saliente')

# Ejes y etiquetas
axes[0].set_xticks(mes)
axes[0].set_xticklabels(meses, rotation=45, ha='right', fontsize=10)
axes[0].set_ylabel("Duraci√≥n acumulada (segundos)", fontsize=11)
axes[0].set_xlabel("Meses", fontsize=11)
axes[0].set_title("Distribuci√≥n acumulada de duraci√≥n de llamadas (2024)", fontsize=14, weight='bold')

# Rejilla y leyenda
axes[0].grid(axis='y', linestyle='--', alpha=0.4)
axes[0].legend(title="Tipo de llamada")

axes[1].bar(pos1[:7], duracioninternaacumulada2025mes[:7], width=width,
            color='tab:red', alpha=0.8, edgecolor='black', label='Interna')
axes[1].bar(pos2[:7], duracionentranteacumulada2025mes[:7], width=width,
            color='tab:blue', alpha=0.8, edgecolor='black', label='Entrante')
axes[1].bar(pos3[:7], duracionsalienteacumulada2025mes[:7], width=width,
            color='gold', alpha=0.8, edgecolor='black', label='Saliente')

axes[1].set_xticks(mes[:7])
axes[1].set_xticklabels(meses[:7], rotation=45, ha='right', fontsize=10)

axes[1].set_ylabel("Duraci√≥n acumulada (segundos)", fontsize=11)
axes[1].set_xlabel("Meses", fontsize=11)
axes[1].set_title("Distribuci√≥n acumulada de duraci√≥n de llamadas (2025)", fontsize=14, weight='bold')

axes[1].grid(axis='y', linestyle='--', alpha=0.4)
axes[1].legend(title="Tipo de llamada")

plt.tight_layout()

plt.show()

"""En 2024, se han realizado mas llamadas entrantes durante la mayor√≠a de los meses salvo en noviembre y diciembre , donde se han realizadon m√°s llamadas internas, es decir, entre trabajadores de la diputaci√≥n. Los terminales han estado menos al telefono en llamadas externas. Los tres tipos de llaamadas tienen el mismo comportamiento a lo largo de los meses de 2024, se estabilizan las barras durante primavera y los primeros meses de ivierno, salvo un leve baj√≥n en marzo, empezando a decrecer las tres barras hasta llegar a agosto.  Ya en oto√±o se produce un gran crecimiento y a partir de octubre un decrecimiento en invierno.

En 2025, siguen siendo las llamadas externas las que menos duraci√≥n tienen, a lo largo de todos los meses. A diferencia del anterior gr√°fico de barars, predominan en la mayor√≠a de meses las llamadas internas que las entrantes en cuanto a duraci√≥n de las mismas. Los tres tipos de llamadas se comportan igual a lo largo del tiempo, un decrecimiento de enero a marzo, seguido de un crecimiento pronunciado hasta mayo. A partir de este mes, se estabilizan.

Tambi√©n podemos hacer el mismo gr√°fico de barras en cuanto al n√∫mero de tipos de llamadas recibidas.
"""

width = 0.2  # m√°s peque√±o
mes = np.arange(12)

pos1 = mes - 1.5*width
pos2 = mes - 0.5*width
pos3 = mes + 0.5*width
pos4 = mes + 1.5*width

fig, axes = plt.subplots(2, 1, figsize=(18, 10))
fig.subplots_adjust(wspace=5)
axes = axes.flatten()

axes[0].bar(pos1, numinternasacumulada2024mes, width=width,
            color='tab:red', alpha=0.8, edgecolor='black', label='Interna')
axes[0].bar(pos2, numentrantesacumulada2024mes, width=width,
            color='tab:blue', alpha=0.8, edgecolor='black', label='Entrantes')
axes[0].bar(pos3, numsalientesacumulada2024mes, width=width,
            color='gold', alpha=0.8, edgecolor='black', label='Salientes')
axes[0].bar(pos4, numerootrasacumuladas2024mes, width=width,
            color='green', alpha=0.8, edgecolor='black', label='OtrasLLAMADAS')

# Ejes y etiquetas
axes[0].set_xticks(mes)
axes[0].set_xticklabels(meses, rotation=45, ha='right', fontsize=10)
axes[0].set_ylabel("Numero llamadas acumuladas", fontsize=11)
axes[0].set_xlabel("Meses", fontsize=11)
axes[0].set_title("N√∫mero de llamadas acumuladas al mes en 2024", fontsize=14, weight='bold')

# Rejilla y leyenda
axes[0].grid(axis='y', linestyle='--', alpha=0.4)
axes[0].legend(title="Tipo de llamada")

axes[1].bar(pos1[:7], numinternasacumulada2025mes, width=width,
            color='tab:red', alpha=0.8, edgecolor='black', label='Interna')
axes[1].bar(pos2[:7],  numentrantesacumulada2025mes, width=width,
            color='tab:blue', alpha=0.8, edgecolor='black', label='Entrantes')
axes[1].bar(pos3[:7],numsalientesacumulada2025mes, width=width,
            color='gold', alpha=0.8, edgecolor='black', label='Salientes')
axes[1].bar(pos4[:7], numerootrasacumuladas2025mes, width=width,
            color='green', alpha=0.8, edgecolor='black', label='OtrasLLAMADAS')

axes[1].set_xticks(mes[:7])
axes[1].set_xticklabels(meses[:7], rotation=45, ha='right', fontsize=10)
axes[1].set_ylabel("Numero llamadas acumuladas", fontsize=11)
axes[1].set_xlabel("Meses", fontsize=11)
axes[1].set_title("N√∫mero de llamadas acumuladas al mes en 2025", fontsize=14, weight='bold')

axes[1].grid(axis='y', linestyle='--', alpha=0.4)
axes[1].legend(title="Tipo de llamada")

plt.tight_layout()

plt.show()

"""Vemos que tanto en 2024 como 2025, el n√∫mero de llamadas salientes es insignificante frente al resto de llamadas durante todos los meses, sin embargo, la duraci√≥n de las misma predominaba en las anteriores gr√°ficas de barras. Esto implica que ha habido pocos terminales que hayan hecho llamadas salientes pero estas llamadas han sido de mucha duraci√≥n. El n√∫mero de llamadas que mas predomina son las entrantes segudas de internas y despu√©s otros tipos de llamadas (moviles, nacionales ,otras,etc). Todos los tipos de llamadas presentan el mismo comportammiento temporal tanto en 2024 como 2025.

En 2024, se aprecia un crecimiento moderado del n√∫mero de llamdas hasta mayo, seguida de un gran decrecimiento a principios de verano y una subida repentina en agosto, mes donde m√°s se realizan llamadas entrantes pero menos internas y demas llamadas. Ya para entrantes se produce un decreciemiento en septiembre y un crecimiento en octubre, seguido de un decrecimiento para todas las llamadas hasta diciembre.

En 2025, sobrretodo en enero, predominan las llamadas entrantes sobre las demas. Todos los tipos de llamadas presentan el mismo comportamiento temporal, un decricimiento brusco de enero a febrero, pero dicho decrecimeinto est√° m√°s moderado durante el resto de meses.

Tambi√©n podriamos hacer una comparaci√≥n directa entre los meses en cuanto al n√∫mero de llamads acumuladas, duraci√≥n acumulada y coste acumulado entre los dos a√±os, para ver que mes predomina sobre el mismo mes de otro a√±o. Esta comparaci√≥n la haremos a partir de un headmap.
"""

pivot = datos.pivot_table(index='a√±o', columns='mes', values='Num.LLamadas', aggfunc='sum')
pivot=pivot[meses[:7]]

fig, axes = plt.subplots(3, 1, figsize=(18, 10))
fig.subplots_adjust(hspace=0.4)
axes = axes.flatten()


sns.heatmap(pivot, annot=False, cmap="YlGnBu",ax=axes[0])
axes[0].set_title("N√∫mero de llamadas por mes y a√±o")



pivot = datos.pivot_table(index='a√±o', columns='mes', values='Duraci√≥n (s)', aggfunc='sum')
pivot=pivot[meses[:7]]

# Graficar heatmap
sns.heatmap(pivot, annot=False, cmap="YlGnBu",ax=axes[1])
axes[1].set_title("Duraci√≥n (s) por mes y a√±o")



pivot = datos.pivot_table(index='a√±o', columns='mes', values='Coste', aggfunc='sum')
pivot=pivot[meses[:7]]

# Graficar heatmap
sns.heatmap(pivot, annot=False, cmap="YlGnBu",ax=axes[2])
axes[2].set_title("Coste por mes y a√±o")

plt.tight_layout()
plt.show()

"""En este mapa de calor el color m√°s palido indica menor cantidad. En 2024 la mayor√≠a de n√∫mero de llamadas se acumulan entre febreo y mayo en los primeros 7 meses del a√±o tal y como lo apreciabamos en los gr√°ficos de barras, lo mismo ocurre para coste y duraci√≥n. En 2025 la mayor√≠a de llamadas se han concentrado en enero y en duraci√≥n y coste se concentraron en enero pero tambi√©n en abril, mayo, junio y julio.

En el n√∫mero de llamadas enero de 2025 predomina sobre enero de 2024 con mucha diferencia, pero en el resto de los meses, 2024 predomina sobre 2025, por eso en la curva temporal se aprecia esa diferencia al principio. Este mapa nos da a indicar que podriamos encontar terminales de poco uso antes en 2025 que en 2024, salvo enero, pero puede estar influenciado por outlieres de gran tama√±o tal y como mostrabamos en la gr√°fica de Pareto.

Tanto en la duraci√≥n de llamadas como en coste de las mismas, 2024 predomina sobre 2025, dandonos a entender que combinando duraci√≥n y/o coste con n√∫mero de llamadas, puede ser mas probable a pesar de existencia de outliers de gran tama√±o, (de hecho en ambos a√±os, 2024 y 2025, y todos los meses, encontramos outliers de gran tama√±o en estas variables pero no muy predominantes en duraci√≥n y coste que en n√∫mero de llamadas),

### Num√©ricas vs Num√©ricas

Pasemos ahora al analisis multivariado de variables num√©ricas. El objetivo principal es ver que relaciones lineales o polinomiales pueden haber entre las distintas variables num√©ricas tanto para n√∫mero de llamadas como duraci√≥n (s), incluidas las de coste. Veamos que relaci√≥n lineal hay entre las variables matem√°ticamente y si a partir de ah√≠ podriamos extraer las variables que definan todo nuestro data set quitando las mas irrelevantes. Ya sabemos por el contexto de nuestro data set que la suma del n√∫mero de tipo de llamadas y de tipo duraci√≥n dan lugar a las variables NUm.LLamadas y Duraci√≥n (s). LA primera idea matem√°tica es ver que rango pose√© la matriz de covarianzas y a partir de ah√≠ deducir cuantas variables necesitaremos quitar en la subsecci√≥n analisis de componentes principales de machine learning para detenci√≥n de anomalias. Empecemos calculando la matriz de covarianzas
"""

dt=datos_num2.select_dtypes(include=[int,float])
cov_matriz=dt.cov()
print(cov_matriz)

"""Veamos el tipo de datos que es matriz_cov"""

print("Tipo de dato de nuestra matriz de covarianzas:", type(cov_matriz))

"""Para saber si es singular o no nuestra matriz (se da por hecho que lo es por el contexto de nuestros datos, aunque no vendr√≠a mal demostralo para dejarlo claro), necesitamos transformarla a un array de numpy, calcular su inversa y posteriormente su rango. Primero transformemos el tipo de dato de nuestra matriz"""

n=len(dt.columns)
cov_matriz=np.array(cov_matriz)
print("Dimensi√≥n de nuestra matriz de covarianzas:",cov_matriz.shape)
print("Tipo de dato de nuestra matriz de covarianzas:",type(cov_matriz))

"""Ahora podremos ver si nuestra matriz es singular y poder calcular nuestro rango y las posibles variables que podr√≠amos eliminar para nuestro objetivo principal, detenci√≥n de terminales de poco uso."""

from numpy import linalg
def singular(df):
  B=False
  det=linalg.det(df)
  if det==0:
    B=True
  return B
if singular(cov_matriz)==0:
  print('nuestra matriz de covarianzas es singular')
else:
  print('nuestra matriz de covarianzas es singular')

"""Pasemos a calcular el rango de nuestra matriz de covarianzas."""

rango=linalg.matrix_rank(cov_matriz)
print("Rango de la matriz: ",rango)
print("N√∫mero m√≠nimo de columnas innecesarias: ",n-rango)

"""No salieron 2 o m√°s columnas, ya que antes quit√© las columnas innecesarias para mi analisis de datos (provinciales, otras desconocidas, internacionales, metropolitanas, etc). Quizas por el simple hecho de que era poco comun que los registros hubiesen realizado llamadas provinciales o internacionales, se podr√≠an necesitar quitar m√°s de una variable a nuestro data set para aplicar nuestros algoritmos de machine learning. Lo veremos con detalle en la secci√≥n de reducci√≥n de dimensionalidad.

Pasemos ahora a la visualizaci√≥n de las matrices de correlaciones, que al igual que la de covarianzas, nos indica la dependencia lineal entre pares de variables. Haremos dos matrices de covarianzas, una para nuestros datos sin la funci√≥n log,  y otra para nuestros datos lognormalizados. La primera nos indicar√° que relaci√≥n lineal hay entre nuestras pares de variables y la segunda que relaci√≥n polinomial de grado al menos 2 hay.
"""

import seaborn as sns
import matplotlib.pyplot as plt

dtlog = dt.apply(lambda x: np.log(x + 1))
df1 = dt.corr()
df2 = dtlog.corr()

fig, axes = plt.subplots(1, 2, figsize=(25, 10))
sns.set_theme(style="white")

# --- Heatmap 1 ---
sns.heatmap(df1,
            ax=axes[0],
            cmap="coolwarm",
            annot=False,
            square=True,
            linewidths=0.3,
            cbar_kws={"shrink": 0.8}
           )

axes[0].set_title("Correlaciones (Original)", fontsize=18, pad=20)
axes[0].tick_params(axis='x', labelrotation=90, labelsize=10)
axes[0].tick_params(axis='y', labelsize=10)

# --- Heatmap 2 ---
sns.heatmap(df2,
            ax=axes[1],
            cmap="coolwarm",
            annot=False,
            square=True,
            linewidths=0.3,
            cbar_kws={"shrink": 0.8}
           )

axes[1].set_title("Correlaciones (Log-transformadas)", fontsize=18, pad=20)
axes[1].tick_params(axis='x', labelrotation=90, labelsize=10)
axes[1].tick_params(axis='y', labelsize=10)

plt.tight_layout()
plt.show()

"""Normalmente en python a partir de dos pares de variables aleatorias X e Y de longitud $n$ se caclula la correlaci√≥n entre estas variables a partir de este estimador:
$$
\frac{
    \displaystyle\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
}{
    \displaystyle\sum_{i=1}^{n} (x_i - \bar{x})^{2}\displaystyle\sum_{i=1}^{n} (y_i - \bar{y})^{2}
}
$$
La existencia de al menos un outlier en las variables X e Y hace que estos estimadores no sean fiables a la hora de estudiar la correlaci√≥n.
Supongamos en un principio que no hay correlaci√≥n entre las variables, es decir, $\rho(X,Y)=0,$ que las medias de las variables sean cero y las varianzas 1. Si a√±adimos a nuestras variables un valor at√≠pico $L=(a_1,a_2)$ tenemos que las varianzas de las variables modificadas  por el outlier $X_c$ e $Y_c$ son$$
Var(X_c) \approx 1 + \frac{a_1^2}{n}
$$


$$
Var(Y_c) \approx 1 + \frac{a_2^2}{n}
$$
y la covarianza es

$$\rho(X_c,Y_c)\approx \frac{a_1a_2}{(n+a_1^2)^\frac{1}{2}(n+a_2^2)^\frac{1}{2}}
$$

donde se ha tenido en cuenta que $n$ es muy grande por lo que $n\approx(n+1)$.
Esta expresi√≥n muestra que si las coordenadas de nuestro outlier son muy grandes en comparaci√≥n con $n$, el coeficiente de correlaci√≥n tiende a 1 o -1 dependiendo de si las coordenadas tienen signo opuesto o el mismo respectivamente. Tambi√©n un valor at√≠pico puede disminuir el valor absoluto de la correlaci√≥n de las variables, es decir, que aunque ambas variables est√©n relacionadas linealmente, dicho valor at√≠pico puede estropear dicha relaci√≥n.
"""

import numpy as np
import matplotlib.pyplot as plt
n=500
A = np.random.randn(n)
B = np.random.randn(n)

corr_low = np.corrcoef(A, B)[0, 1]
A=np.append(A,50)
B=np.append(B,50)
corr_lowout=np.corrcoef(A, B)[0, 1]
plt.scatter(A, B)
plt.annotate(
    'Outlier',                   # texto
    xy=(50,50),                 # punto al que apunta la flecha
    xytext=(50, 40),             # posici√≥n del texto (arriba) inicialmente
    ha='center',                 # centra el texto horizontalmente
    va='bottom',                 # coloca el texto debajo de xytext
    fontsize=10,
    color='red',
    arrowprops=dict(arrowstyle='->', color='red', lw=1)
)
plt.title(f"Correlaci√≥n sin outlier: {corr_low:.2f}, con outlier: {corr_lowout:.3f}")
plt.show()

"""En este caso hemos creado dos variables totalemnte incorreladas $A$ y $B$, agrupadas en una bola centrada en 0 con radio menor que 5. cor indica la correlaci√≥n entre ellas sin introducir el outlier. Introducciendo el outlier hemos visto que ha aumentado el grado de correlaci√≥n aunque no haya niguna relaci√≥n lineal entre ellas. Tambi√©n podemos ver el otro caso"""

import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)  # para reproducibilidad
n = 500  # tama√±o de la muestra

# ===========================
# 1. Variables con ALTA correlaci√≥n
# ===========================

# Generamos X como normal est√°ndar
X = np.random.randn(n)

# Y ser√° una combinaci√≥n lineal de X + poco ruido
Y = 3*X + np.random.randn(n)
corr_high = np.corrcoef(X, Y)[0, 1]  # ruido muy peque√±o
X=np.append(X,50)
Y=np.append(Y,-10)
corr_highout = np.corrcoef(X, Y)[0, 1]


plt.scatter(X,Y)
plt.annotate(
    'Outlier',                   # texto
    xy=(50, -10),                 # punto al que apunta la flecha
    xytext=(50, -7),             # posici√≥n del texto (arriba) inicialmente
    ha='center',                 # centra el texto horizontalmente
    va='bottom',                 # coloca el texto debajo de xytext
    fontsize=10,
    color='red',
    arrowprops=dict(arrowstyle='->', color='red', lw=1)
)
plt.title(f"Correlaci√≥n sin outlier: {corr_high:.2f}, con outlier: {corr_highout:.3f}") # activa la leyenda
plt.show()

"""En este caso la correlaci√≥n ha disminuido bastante al a√±adir un outlier que se aleja bastante del centro. cor indica la correlaci√≥n original entre variables que se acerca bastante al 0, es decir, est√°n casi relacionadas linealmente, sin embargo al a√±adir el outlier (100,-10) ya no hay casi relaci√≥n, cosa que no es as√≠.

He hecho esta distinci√≥n ya que por los resultados obbservados antes como la curva de pareto, boxplots o incluso las colas de los violinplots, veiamos que habia outliers que afectaban bastante a la distribuci√≥n de nuestras variables, en concreto duraci√≥n (s) y num.llamadas. Una idea que tengo, a parte de utilizar otras medidas, es escalar los datos pero esta vez con un escalado robusto. Se sabe que dicho escalado no se ve influenciado por valores at√≠picos (de ah√≠ su nombre).  Consiste en la siguiente formula
$$\frac{X-X_{median}}{IQ_x}$$
donde $X_{median}$ es la mediana e $Iq_x$ el rango intercuart√≠lico.
Adem√°s se sabe en teor√≠a que la correlaci√≥n entre dos variables se mantiene igual aunque se haga un cambio de escala y centralizaci√≥n, es decir,
$$\rho(aX+b,cY+d)=\rho(X,Y)$$
para $a,b,c,d\in\mathbb{R}$, con $a,c\neq
0.$

Sin embargo tambi√©n su estimador es invariante frente a transformaciones lineales. Esto hace que el escalado robusto sea inservible para mejorar la calidad de nuestra matriz de correlaciones

Otra opci√≥n que he pensado es eliminar los ourliers univariados solamente para graficar la matriz de correlaciones, pero aunque la mayor√≠a de outliers bivariados est√©n asociados a los univariados, pueden haber unos pocos que no lo est√©n y de todas formas ocasionen un gran efecto sobre la matriz de correlaciones. Adem√°s tendr√≠amos que quitar outliers que puedan ofrecer una gran informaci√≥n.

Dos opciones que se me ocurren para estudiar la dispersi√≥n de mis datos y que son robustas frente a outliers influyentes, son la correlaci√≥n de Spearman y la correlaci√≥n robusta basada en el MDC (Minimum Covariance Determinant).

El coeficente de correlaci√≥n de Spearman, al igual que el de Pearson, mide la correlacion entre las variables, en concreto, la fuerza y direcci√≥n de una relaci√≥n mon√≥tona entre dos variables. Este coeficente no determina exactamente la linealidad de las variables, pero independiemente de que la relaci√≥n entre dos variables sea lineal o no, la correlaci√≥n de sperman dar√° la misma informaci√≥n. Por ejemplo, si tenemos que $Y=X^2$, el coeficiente de correlaci√≥n de Pearson no nos dar√° informaci√≥n alguna ya que no hay indicios de relaci√≥n lineal entre $X$ e $Y$, pero el coeficiente de Spearman nos indicar√° una fuerte relaci√≥n mon√≥tona. A diferencia de Pearson, la matriz de correlaciones de Spearman es insensible a outliers, ya que los calculos de los coeficientes se basan en rango y no en los valores de la variable, asi que no tiene en cuenta la presencia de outliers pero muestra una infromaci√≥n sobre todos los datos.

El estad√≠stico $\rho$ viene dado por la expresi√≥n:

$$
\rho = 1 - \frac{6 \sum D^{2}}{N \left( N^{2} - 1 \right)}
$$
donde $D$ es la diferencia entre los correspondientes estad√≠sticos de orden de $x - y$.  $N$ es el n√∫mero de parejas de datos. Grafiquemos la matriz de Spearman solamente para los datos originales, ya que para los transformados con la funci√≥n logaritmo, la matriz es identica al ser log una funci√≥n creciente. AL igual que los coeficientes de Pearson, los de spearman van de 1 a -1 pasando por 0. Si el coeficente esta muy proximo de 1 o -1 indica una monoton√≠a fuerte entre las dos variables. Cuando es positivo, hay una correlaci√≥n positiva entre variables, es decir, si una variable crece la otra tambi√©n, y si es negativo al reves. Cuando el coeficente de spearman es 0, significa que no hay monoton√≠a entre las dos variables.

Pasemos a visualizar la matriz de correlaciones de Spearman.
"""

import seaborn as sns
import matplotlib.pyplot as plt

df1 = dt.corr(method='spearman')

sns.set_theme(style="whitegrid")

plt.figure(figsize=(12, 10))
sns.heatmap(
    df1,
    cmap="YlGnBu",             # paleta azul-verde clara
    annot=True,                # mostrar valores
    fmt=".2f",
    square=True,
    linewidths=0.5,
    linecolor='white',         # l√≠neas blancas entre celdas
    cbar_kws={"shrink": 0.8, "label": "œÅ de Spearman"},
    annot_kws={"size":9, "weight":"bold", "color":"black"}
)

plt.title("Matriz de Correlaciones de Spearman", fontsize=18, pad=20, weight='bold')
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(rotation=0, fontsize=10)

plt.tight_layout()
plt.show()

""" Como era de esperar por el contexto de nuestros datos, hay monoton√≠a creciente entre todos los pares de cvariables num√©ricas, es decir, si una crece la otra tambi√©n. Esta matriz nos servir√° para indicra la fuerza de creciemiento entrre pares de variables.

 Se observan que los costes han sido repercutidos sobretodo por las llamadas m√≥viles y duraci√≥n de salientes. Otras variables como nacionales y las duraciones y n√∫mero total de llamadas, salvo las internas,han influido en el coste. Seguramente los terminles que mas costos han generado esten asociados a los terminales que m√°s llamadas m√≥viles hayan hecho y pueda darse perfectamente el caso de que haya habido terminales que hayan realizado muchas llamadas externas de gran duraci√≥n pero que muy pocas hayan sido fijas e internas y que adem√°s hayan recibido pocas llamadas. Obviamente la duraci√≥n y n√∫mero de llamadas influir√≠an en la monoton√≠a de coste, pero perfectamente se han dado casos de terminales que hayan usado no mucho el telefono pero que estos hayan estado bastante tiempo al telefono en la llamadas que realizaron y seguramente esas llamadas hayan sido moviles porque tambi√©n hay una fuerte relaci√≥n de monoton√≠a entre m√≥viles y duraci√≥n. Puede darse el caso adem√°s de algunos terminales que hayan preferido hacer llamadas internas de larga duraci√≥n y debido a eso no hayan sido cobradas. La conclusi√≥n, que si ha habido terminales con elevados costes a lo largo del mes, seguramente ese terminal haya realizado llamdas de pago de larga duraci√≥n y probablemte hayan sido via movil. Tambi√©n habra habido muchos terminales que hayan realizado pocas llamadas pero de larga duraci√≥n, lo cual ha generado elevados costes.

 Es l√≥gico que la variable duraci√≥n mantenga una fuerte relaci√≥n de crecimiento con el resto de variables. Con las que menos presenta esa fuerza de crecimiento es con las varaibles entrantes, salientes e internas salientes. Seguramente entre los trabajadores se hayan realizado bastantes llamadas pero estas no hayan sido de mucha duraci√≥n por consultas, es por seo que si hay crecimiento pero es mas moderado. Era obvio que habr√≠a una relaci√≥n de monoton√≠a fuerte entre duraci√≥n y los tipos de duraci√≥n debido al contexto de nuestros datos (la suma de los tipos de duraci√≥n es igual a la duraci√≥n total). Pero sobretodo y como era de esperar el n√∫mero de llamadas totales y duraci√≥n estan fuertemente relacionadas en el sentido de crecimiento. No obstante se observara en la secci√≥n de detenci√≥n de anomalia, terminales que hayan recibio muchas llamadas pero no hayan sido usados.

 El mismo comportamiento se aprecia para la variable Num.lllamadas, solo que esta mas influenciada por los tipos de llamadas que por la duraci√≥n de las mismas. Se observa que lo mas probable es que los terminales que hayan realizado o recibido muchas llamadas, tambi√©n han realizado o recibido bastantes llamadas internas. Las varaibles que menos repercusi√≥n tienen en el crecimiento de numero de llamadas son las internas salientes y entrantes. Seguramente el uso del telefono haya estado mas asociado a realizar llamadas internas que externas, de todas formas en la mayor√≠a de casoso los que m√°s hayan usado el telefono  seguramente tambi√©n hayan realizado y/o recibido llamadas externas.

 Los tipos de variables internas, tanto las asociadas al numero de llamadas como a la duraci√≥n, no est√°n muy fuertemente relacionadas en el sentido monotomo  con las otras, solo con ellas mismas. Eso  implica que en los terminales con masllmadas y duraci√≥n, la mayor√≠a de ellas no habr√°n sido internas salvo contados casos. De todas formas influyen positivamente en el crecimiento. Como era de epserar el n√∫mero de llamadas internas, internas salientes y duraci√≥n de internas est√°n fuertemente correlacionadas en el sentido monotono entre si. Podr√≠amos hace una distinci√≥n con numero de llamadas internas, que junto con la duracoi√≥n son las que mas influyen en el crecimiento de numero de llamadas. No obstante no se descartan terminales que hayan hecho muchas llamadas y que hayan hecho pocas internas, aunque seguramente sean la minoria de casos. Las variables entrantes y salientes  muestran una relaci√≥n monotona mas moderada con respecto al resto de variables. La variable entrantes se ve mas afectada  por entrantes numero de llamadas y entrantes duraci√≥n como era de esperar, como tambi√©n salientes y salientes duraci√≥n. Pero Me llama mas la atenci√≥n el caso de que el crecimiento del n√∫mero de llamadas recibidas (Salientes) este influenciado por el crecimeinto de moviles. Esto imploca que gran parte de terminales que hayan recibido las llmadas, seguramente sean por via telef√≥nica y a cuantas mas llmadas, probablemente hayan sido de tipo movil la mayor√≠a.

 Esta matriz de correlaciones es muy util para estudiar la tendencia de los datos y determinar como se comportan los terminales, por ejemplo, si ha habido terminales que solamente han hecho llamadas internas para consultas y alguna que otra llamada externa por telefono fijo, seguramente no haya generado costes o estos sean leves. No obstante estas matrices no determinan la linealidad y relaciones entre pares de variables. Tenemos que la matriz de Pearson anterior no es del todo √∫til por la justificaci√≥n a posterior que hice con experimentos. Es por tanto que hare uso de m√©todos de machine learning para calcular la matriz de correlacion que es el MCD.

 En este m√©todo lo que se hace es buscar el subconjunto de datos de tama√±o h (generalmente la mayor√≠a de los datos) cuya determinante de la matriz de covarianza sea m√≠nima. Se aplica este m√©todo ya que la covarianza determina la dispersi√≥n de datos. El determinante de la matriz de covarianzas se puede entender como el volumen que ocupan nuestros datos. Un volumen peque√±o implicar√≠a una gran concentraci√≥n de datos, sin tener en cuenta los outliers. Es por eso que al tener outliers mmultivariados de gran tama√±o con respecto al resto de nuestros datos, que el MCD ser√≠a la mejor aproximaci√≥n a la matriz de covarianzas. Calculemos la matriz tanto para los datos sin aplicar la tranformaci√≥n logaritmica como a los transformados.
"""

import numpy as np
import pandas as pd
from sklearn.covariance import MinCovDet
# Ajustar el estimador MCD
mcd = MinCovDet().fit(dt)
mcdlog = MinCovDet().fit(dtlog)
# Obtener la media y covarianza robustas
robust_mean = mcd.location_         # media robusta
robust_cov = mcd.covariance_        # matriz de covarianza robusta

robust_meanlog = mcdlog.location_         # media robusta
robust_covlog = mcdlog.covariance_        # matriz de covarianza robusta


# Desviaciones est√°ndar robustas
robust_std = np.sqrt(np.diag(robust_cov))

# Matriz de correlaciones robusta
robust_corr = robust_cov / np.outer(robust_std, robust_std)

# Desviaciones est√°ndar robustas
robust_stdlog = np.sqrt(np.diag(robust_covlog))

# Matriz de correlaciones robusta
robust_corrlog = robust_covlog / np.outer(robust_stdlog, robust_stdlog)

fig, axes = plt.subplots(1, 2, figsize=(25, 10))
sns.set_theme(style="white")

labels = dt.columns  # etiquetas para ambos heatmaps

# --- Heatmap 1 ---
sns.heatmap(robust_corr,
            ax=axes[0],
            cmap="coolwarm",
            annot=False,
            square=True,
            linewidths=0.3,
            xticklabels=dt.columns,
            yticklabels=dt.columns,
            cbar_kws={"shrink": 0.8}
           )

axes[0].set_title("Correlaciones (Original)", fontsize=18, pad=20)
axes[0].tick_params(axis='x', labelrotation=90, labelsize=10)
axes[0].tick_params(axis='y', labelsize=10)

# --- Heatmap 2 ---
sns.heatmap(robust_corrlog,
            ax=axes[1],
            cmap="coolwarm",
            annot=False,
            square=True,
            linewidths=0.3,
            xticklabels=dt.columns,
            yticklabels=dt.columns,
            cbar_kws={"shrink": 0.8}
           )

axes[1].set_title("Correlaciones (Log-transformadas)", fontsize=18, pad=20)
axes[1].tick_params(axis='x', labelrotation=90, labelsize=10)
axes[1].tick_params(axis='y', labelsize=10)

plt.tight_layout()
plt.show()

"""Por lo observado en la matriz de Spearman, la posible relaci√≥n de linealidad es positiva, es decir, si una variable crece, la otra tambi√©n. Vemos cambios curiosos con respecto a la anterior matriz de correlaciones que hicimos. Vemos que hay relaciones lineales  que antes no exist√≠an con la matriz de correlaciones tradicional como salientes duraci√≥n (s) y coste, Internas Num.llamadas y Numero de llamadas, internas duraci√≥n y duraci√≥n en segundos, moviles y salientes duraci√≥n, adem√°s la relacion lineal de coste con moviles y salientes duraci√≥n es mas intensa con respceto a la matriz de correlaciones tradicional. Esto implica la posible existencia de datos bivariados que hayan variado la relaci√≥n lineal. Seguramente hayan sido terminales que se hayan excedido en una cosa y en otra no, por ejemplo puede haber habido algunos terminales que hayan realizado muchas llamadas m√≥viles pero generado pocos costes o viceversa. Tambi√©n se aprecia en esta matriz de correlaciones que no hay mucha relaci√≥n lineal entreentrantes numero de llamadas y n√∫mero de llamadas, todo lo contrario a lo que se apreciaba en la matriz de correlaciones tradicional. Pueda deberse seguramente a un gran outlier que haya realizado muchas llamadas y una gran proporci√≥n de esas llamadaqs hayan sido internas. Como era de esperar hay una relaci√≥n lineal fuerte entre entrantes numero de llmadas y entrantes duraci√≥n, Y tambi√©n entre internas n√∫mero de llamadas e internas duraci√≥n. Tambi√©n hay fuerte relaci√≥n lineal entre n¬¥merode llamadas  y duraci√≥n pero no tanto xcomo se apreciaba en la matriz de correlaciones tradicional. Posiblemente haya habido algun terminal que haya usado mucho el telefono y adem√°s hay estado demasiado tiempo en comparaci√≥n con otros terminales.

Para la matriz de correlaciones lo transformadas podriamos apreciar alguna relaci√≥n lineal sin peso o polinomial. No se aprecian casi diferencias con respecto a la matriz de correlaciones original. Esto implica que si pueda haber una recta que explique nuestros datos o relaciones bilineales entre ellos, esta recta pasar√° por el 0, lo cual se traduce a existencia de terminales denulo uso, incluso para la variable duraci√≥n y n√∫mero de llamadas. Ya vimos en histogramas y boxplots que hab√≠an grandes concentraciones de 0's en muchas variables salvo en las dos anteriores que mencion√©. Esto junto con la similitude de las dos matrices de correlaciones implican que si un terminal ha sido 0 en una variable lo mas probable es que lo sea en la otra variable, por ejemplo, si un trabajador ha hecho no ha hecho llamadas m√≥viles, lo mas probable es que no haya ocasionado ningun coste o si un trabajador no ha realizado llamadas internas, lo mas probable es que haya realizado pocas llamadas en total, es decir, poco uso del telefono.

Hagamos ahora una matriz de gr√°ficas de dispersi√≥n de las variables para ver si realmente nuestro modelo para aproximarse a la matriz de correlaciones ha estado acertado y a partir de ah√≠ determinar las gr√°ficas de dispersi√≥n que mas relaciones proporcionan entre nuestras variables.

"""

import matplotlib.pyplot as plt
import seaborn as sns
import math

dt2=datos_num2.select_dtypes(include=['int', 'float'])
columnas = dt2.columns
n = len(columnas)

# N√∫mero total de scatterplots
total = n*(n-1)//2

# Tama√±o de la grilla (p.ej. 10√ó8 = 80)
cols = 5
rows = 5

fig, axes = plt.subplots(rows, cols, figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(2):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

cols = 5
rows = 4

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(2,4):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()
cols = 5
rows = 5

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(4,7):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()
cols = 4
rows = 4

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(7,n):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])




        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

"""Ambas matrices captan bien los casos comunes donde no hay correlaci√≥n entre variables ya que se aprecia que o los datos est√°n muy dispersos o se concentran en torno a 0 y aqu√≠ la matriz de correlaciones no detecta relaci√≥n lineal. Veamos las diferencias entre ambas matrices. Entre salientes duraci√≥n con moviles y nacionales  hay una tendencia lineal, solo que hay varios valores at√≠picos que distorsionan esa tendencia. Por ese motivo nuestra matriz de correlacion no muestra linealidad pero si lo hace la robusta al no verse afectada por outliers. Lo mismo ocurre entre salientes y moviles, hay algunos outliers que intentan incrementar la correlaci√≥n entre estas variables. Entre salientes duraci√≥n y cote hay una gran relaci√≥n lineal entre variables solo que la matriz de correlaciones no muestra dicha realici√≥n por presencia de outliers dispersos. Sin embargo ha habido casos donde la matriz de correlaciones tradicional ha mosttrado mejor el resultado que la robusta debido a posiblemente posiciones entre datos, o enmascaramiento, es decir, hay observaciones at√≠picas similares que se ocultan entre si. Estos casos afectan el rendimiento de la matriz de correlaciones robustas. Por ejemplo en nuestro caso la relaci√≥n entre entrantes duracion y duraci√≥n en segundos es casi lineal pero nuestra matriz de correlaci√≥n robusrta muestra que no debido a la superposici√≥n de datos. Tambi√©n muestra un gran fallo que no lo muestra la matriz de correlaciones tradicional, y es que no determina la relaci√≥n lineal entre numero de llamadas y entrantes numero de llamadas y tambi√©n entre internas numeros de llamadas y numero de llamdas, porque muestran relacion lineal y no lineal respectivamemnte nuestra matriz de correlaciones robustas pero la realidad es completamente diferente. Lo mismo ocurre entre entrantes duraci√≥bn y duraci√≥n en segundos. Todas nuestras relaciones entre variables muestran en general una relaci√≥n monotona creciente entre ellas.

La matriz de Pearson puede verse afectada por superposici√≥n de datos sin emabrgo para casos donde ha mostrado una gran relaci√≥n monotona creciente nuestra matriz de Pearson no ha fallado. Quizas pueda haber un fallo entre duraci√≥n y n√∫mero de llamadas pero seguramente la gr√°fica de dispersi√≥n se ve muy afectada por grandes outliers que distorsionan la perspectiva. Pude parecer que en la matriz de Pearson haya fuerte relaci√≥n enytre por ejemplo numero de llamadas y nacionales pero que en la gr√°fica apenas se aprecia dicha relaci√≥n. A veces Spearman rinde mal cuando hay gran superposici√≥n de datos, es decir, por ejemplo si una variable recorre muchos valores pero la otra se mantiene 0. SIn embargo esto no es necesaraiemnte as√≠ ya que si aumenta nacionales por nuestro contexto de nuestros datos aumenta numero de llamadas. Solo implica que pueda haber casoso donde el incremento de nacionales no afecte tanto al crecimiento de numero de llamadas, pero esto nos lo refleja bien nuestra matriz de spearman. Esta visualizaci√≥n de una gran concentraci√≥n de ceros se pueda deber a la distorsi√≥n visual que ocasionan nuestras anomalias en nuestros datos. Para ver el crecimiento o decrecimiento de nuestros datos lo mejor ser√≠a implementar la funci√≥n logaritmo.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import math

dt2=datos_num2.select_dtypes(include=['int', 'float']).apply(lambda x:np.log(x+1))
columnas = dt2.columns
n = len(columnas)

# N√∫mero total de scatterplots
total = n*(n-1)//2

# Tama√±o de la grilla (p.ej. 10√ó8 = 80)
cols = 5
rows = 5

fig, axes = plt.subplots(rows, cols, figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(2):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

cols = 5
rows = 4

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(2,4):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()
cols = 5
rows = 5

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(4,7):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()
cols = 4
rows = 4

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(7,n):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])




        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

"""Ya se pueden apreciar mejor las relaciones de monoton√≠a entre variables. Aunque haya alguna que otra superposici√≥n entre valores, la matriz de Spearman muestra bastante bien (no perfectamnte por superposici√≥n o concentraci√≥n de valores, por ejemplo ceros en una misma fila o columna, pero se aproxima a  la realidad por la gran cantidad de otros datos que hacen que esos casos sean poco apreciables) la realidad. Para algunas variables no muestra toda la relaci√≥n monotona debido a concentraci√≥mn de 0's. Por ejemplo hay una relaci√≥n monotona muy fuerte entre duraci√≥n y coste. Aunque la matriz de Spearman muestra un valor alto (0.72), seguramente la relaci√≥n de monoton√≠a sea mas fuerte, pero spearman ha reducido bastante esa relaci√≥n debido la concentraci√≥n de terminales que no fueron cobrados al hacer llamadas telef√≥nicas, como 7777 que uso mucho el telefono durante estos 19 meses pero no aparecio entre los 30 terminales que mas gastos generaron.  Seguramente las llamadas telef√≥nicas internas no generan costes.

Para las variables duraci√≥n e internas salientes o salientes se aprecia una fuerte relaci√≥n lineal pero no tanto como entre duraci√≥n y n√∫mero de llamadas. Aunque spearman muestre relaci√≥n monotna positiva medio alta (mas de 0.5) seguramente sea mas fuerte pero spearman en este caso se vio perjudicada por concentraci√≥n de terminales que seguramente recibieron bastantes llamadas pero ninguna fue conntestada, es por tanto que se acumulan muchos terminales de nulo uso pero con bastantes llamadas salientes en el diagrama de dispersi√≥n.

En general salvo peque√±as concenrtraciones de 0's que coresponden seguramente a terminales inactivos, la matriz de speramn no se ha visto muy perjudicada a la hora de determinar la relaci√≥n de monoton√≠a entre pares de variables.

Tambi√©n por el efecto de outliers no podemos apreciar con claridad que relaci√≥n lineal hay entre nuestros datos, es por tanto que considero que la mejor forma para reducir el efecto visual que ocasionan nuestros outliers pero consrevar la relaci√≥n lineal es realizar un escalado robusto para nuestras variables conservando las relaciones lineales.
"""

from sklearn.preprocessing import RobustScaler
dt2=datos_num2.select_dtypes(include=['int', 'float'])
columnas = dt2.columns
scaler =RobustScaler()
robust_df = scaler.fit_transform(dt2)
robust_df = scaler.fit_transform(robust_df)
robust_df = pd.DataFrame(robust_df, columns=columnas)
dt2=robust_df
n = len(columnas)

# N√∫mero total de scatterplots
total = n*(n-1)//2

# Tama√±o de la grilla (p.ej. 10√ó8 = 80)
cols = 5
rows = 5

fig, axes = plt.subplots(rows, cols, figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(2):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

cols = 5
rows = 4

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(2,4):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()
cols = 5
rows = 5

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(4,7):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])



        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()
cols = 4
rows = 4

fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))

axes = axes.flatten()  # para indexar f√°cilmente

sns.set_theme(style="white")

k = 0  # √≠ndice del subplot actual

for i in range(7,n):
    for j in range(i+1, n):

        ax = axes[k]
        k += 1

        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])




        # est√©tica
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

# esconder ejes sobrantes
for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

datos_num.columns

"""Otra gr√°fica mas precisa es hexbinplot, una variante de scatter que indica la densidad de concentraci√≥n de puntos y que sirve sobretodo para detenci√≥n de anomalias, ya que se podr√≠a porducir enmascaramientos, es decir, muchos registros toman el mismo valor o similares. Por ejemplo puede haber muchos terminales que hayan hecho el mismo n√∫mero de llamadas y la misma duraci√≥n. En nuestro caso concreto, por los diagramas de dispersi√≥n, boxplots e incluso histogramas, habr√° muchos terminales, sobretodo en las variables de tipo de llamnadas, que no habr√°n realizado un tipo de llamada, ya sea movil o nacional. Para este hexbinplot utilizaremos la escala logaritmica, ya que nos interesa de esta gr√°fica la densidad y detenci√≥n de outliers, es decir, terminales de mucho y poco uso, no la linealidad o relaci√≥n entre estos, y porque outliers de gran tama√±o distorsionan la gr√°fica. La transformaci√≥n logaritmica no eliminar√°."""

from sklearn.preprocessing import RobustScaler
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Escalado de datos
dt2 = datos_num2.select_dtypes(include=['int', 'float']).apply(lambda x:np.log(x+1))
columnas = dt2.columns
n = len(columnas)

# ---------- Primer grid ----------
cols = 5
rows = 5
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
sns.set_theme(style="white")
k = 0

for i in range(2):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1

        hb = ax.hexbin(
            dt2[columnas[j]],
            dt2[columnas[i]],
            gridsize=30,
            cmap="plasma",   # Colores m√°s visibles
            mincnt=1,
            linewidths=0.5
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

        cb = fig.colorbar(hb, ax=ax)
        cb.set_label("Densidad")

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas Hexbin", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# ---------- Segundo grid ----------
cols = 5
rows = 4
fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))
axes = axes.flatten()
sns.set_theme(style="white")
k = 0

for i in range(2,4):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1

        hb = ax.hexbin(
            dt2[columnas[j]],
            dt2[columnas[i]],
            gridsize=30,
            cmap="plasma",
            mincnt=1,
            linewidths=0.5
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

        cb = fig.colorbar(hb, ax=ax)
        cb.set_label("Densidad")

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas Hexbin", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# ---------- Tercer grid ----------
cols = 5
rows = 5
fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))
axes = axes.flatten()
sns.set_theme(style="white")
k = 0

for i in range(4,7):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1

        hb = ax.hexbin(
            dt2[columnas[j]],
            dt2[columnas[i]],
            gridsize=30,
            cmap="plasma",
            mincnt=1,
            linewidths=0.5
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

        cb = fig.colorbar(hb, ax=ax)
        cb.set_label("Densidad")

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas Hexbin", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# ---------- Cuarto grid ----------
cols = 4
rows = 4
fig, axes = plt.subplots(rows, cols,  figsize=(15, 15))
axes = axes.flatten()
sns.set_theme(style="white")
k = 0

for i in range(7,n):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1

        hb = ax.hexbin(
            dt2[columnas[j]],
            dt2[columnas[i]],
            gridsize=30,
            cmap="plasma",
            mincnt=1,
            linewidths=0.5
        )

        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

        cb = fig.colorbar(hb, ax=ax)
        cb.set_label("Densidad")

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas Hexbin", fontsize=28, fontweight="bold", y=1.02)
plt.show()

plt.scatter(dt2['Num.LLamadas'],dt2['Duraci√≥n (s)'])
plt.xlabel('Num.LLamadas')
plt.ylabel('Duraci√≥n (s)')

from pyod.models.knn import KNN

"""Tambi√©n podr√≠amos buscar relaciones tridimensionales entre nuestros datos. Aunque la matriz de Spearman, la de Pearson y los gr√°ficos de disprsi√≥n nos hayan proporcionado unas ideas bastantes claras sobre los costes generados y que tipo de trabajador es el que mas costes genera, no vendria mal hacer un scatter plot sobre las tres variables numero de llamadas, duraci√≥n y coste, junto con un gr√°fico de contorno que nos permitir√° ver donde se acumulan los costes dentro del uso de telefonos.

Empecemos por ver la distribuci√≥n de los tres datos.
"""

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')

# Scatter 3D
sc = ax.scatter(
    datos["Num.LLamadas"],
    datos["Duraci√≥n (s)"],
    datos["Coste"],
    c=datos["Coste"],
    cmap='viridis',
    s=50,
    alpha=0.8,
    edgecolor='k'
)

# Labels y t√≠tulo
ax.set_xlabel('Num.LLamadas', fontsize=12)
ax.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax.set_zlabel('Coste', fontsize=12)
ax.set_title('3D Scatter Plot Interactivo', fontsize=16)

# Barra de color
cbar = fig.colorbar(sc, ax=ax, pad=0.1, shrink=0.6)
cbar.set_label('Coste', fontsize=12)

# Esto activa la interacci√≥n
plt.ion()  # modo interactivo
plt.show()

"""Vemos que la nube de datos esta aplanada. Esto se debe al grafico de dispersi√≥n entre numero de llamadas y duraci√≥n de las mismas, que la mayor√≠a de llamadas de larga duraci√≥n requerieron de pocas llamadas, no obstante se apreciaba tanto en el gr√°fico de spearman como en la de dispersi√≥n logaritmica, que cuantas mas llamadas se realizaban mas duraci√≥n se estaba al telefono. Seguramente esta concentraci√≥n se deba a la escala de variables y a la influencia de nuestros outliers, es decir, pocos terminales determinaba la duraci√≥n total de nuestros datos, como se apreciaba en la curva de pareto. Sin embargo lo mas importante de este gr√°fico son los costes. Obviamete se ve que un aumento de la duraci√≥n de llamadas  genera elevados costes. Aunque un amuento de llamadas genera normalmente un aumento de coste, no es tan proporcional como en el caso de duraci√≥n. En esta gr√°fica tambi√©n se observan  terminales que han estado bastante al teelfono pero que han generado poco coste.
Aunque normalmente el crecimeinto de coste se vea asociado a la duraci√≥n del mismo, ha habido un terminal que ha generado costes muy elevados pero no ha usado mucho el telefono, sobretodo en cuanto al n√∫mero de llamadas y tampoco tanto en cuanto a la duraci√≥n, comparandolo con otrso casos que si los han usado. Para ver mejor estas conclusiones lo optimo ser√≠a escalar los tres datos a logaritmica
"""

from pyod.models.pca import PCA
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
n=100
A=np.random.randn(n)
B=np.random.randn(n)
C=A+5*B+0.35*np.random.randn(n)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')

# Scatter 3D
sc = ax.scatter(
    dt2["Num.LLamadas"],
    dt2["Duraci√≥n (s)"],
    dt2["Coste"],
    c=dt2["Coste"],
    cmap='viridis',
    s=50,
    alpha=0.8,
    edgecolor='k'
)

# Labels y t√≠tulo
ax.set_xlabel('Num.LLamadas', fontsize=12)
ax.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax.set_zlabel('Coste', fontsize=12)
ax.set_title('3D Scatter Plot Interactivo', fontsize=16)

# Barra de color
cbar = fig.colorbar(sc, ax=ax, pad=0.1, shrink=0.6)
cbar.set_label('Coste', fontsize=12)

# Esto activa la interacci√≥n
plt.ion()  # modo interactivo
plt.show()

"""Aqui se puede areciar un comportamiento de terminales de forma mas comprensible, no como el anterior caso por los outliers que estaban bastante alejados de la nuve de puntos. En este caso se aprecia que los terminales que mas han usado el telefono tanto para el numero de llamadas como duarci√≥n de las mismas no han genarado  apenas costes. Lo mas probable es que estos terminales hayan realizado muchas llamadas internas y de telefono fijo. Se ve qeu los costes van aumentando con un aumento de duraci√≥n de llamadas y disminuci√≥ndel numero pero hasta cierto punto. A partir de ah√≠ un aumento de llamadas y un amuento de duraci√≥n generan un aumento de costes. Tambi√©n se pueden apreciar termianles de muchas llamadas, poca duraci√≥n y poco coste, ya que por la matriz de spearman y los diagramas de dispersi√≥n la duraci√≥n tenia mas influencia en el comportamiento monotono del coste que el numero de llamadas. Sin embagro se aprecia que un amuento de duraci√≥n y de llamadas progersivo generan bastantes costes. Los casos mas llamativos de costes se deben a casos de terminales que no han usado mucho el telefono pero que cuando lo han usado han generado bastantes costes seguramente por realizar llamadas m√≥viles y de larga duraci√≥n cuando ellos mismos las han realizado. Para ver mejor un comportamiento general del coste con respercto a duraci√≥n y n√∫mero de llamadas, nos conviene realizar un gr√°fico de contorno"""

import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import griddata

# Datos
x = datos["Num.LLamadas"]
y = datos["Duraci√≥n (s)"]
z = datos["Coste"]

# Crear malla m√°s fina
n_points = 200  # aumenta resoluci√≥n
xi = np.linspace(min(x), max(x), n_points)
yi = np.linspace(min(y), max(y), n_points)
X, Y = np.meshgrid(xi, yi)

# Interpolaci√≥n c√∫bica
Z = griddata((x, y), z, (X, Y), method='cubic')

# Gr√°fico de contorno mejorado
plt.figure(figsize=(10, 7))
contourf = plt.contourf(X, Y, Z, levels=30, cmap='plasma', alpha=0.9)
contour = plt.contour(X, Y, Z, levels=15, colors='k', linewidths=0.5, alpha=0.7)  # l√≠neas de nivel

# Barra de color
cbar = plt.colorbar(contourf)
cbar.set_label('Coste', fontsize=12)

# Etiquetas y t√≠tulo
plt.xlabel("Num.LLamadas", fontsize=12)
plt.ylabel("Duraci√≥n (s)", fontsize=12)
plt.title("Gr√°fico de Contorno", fontsize=16)

plt.tight_layout()
plt.show()

"""Esta gr√°fica de contorno ser√≠a una representaci√≥n en el plano de una funci√≥n suavizada que representa el diagrama de dispersi√≥n tridimenasional que hicimos anteriormente sin aplicar la funci√≥n logartimo. Los colores violeta fuertes frios corresponden a terminales de nulo coste mientras que los mas c√°lidos son los terminales que mas costes han generado. Lo que mas me ha llamada la atenciopn es que los terminales que m√°s tiempo han estado al telefono y tambi√©n han realizado la mayor√≠a de llamadas son de los que menos costes han generado. Esto no implica necesariamente que el crecimiento del coste no se vea influenciado por el crecimiento de num de llamadas y duraci√≥n cde las mismas. Los terminales con mas coste se acumulan en la diagonal inferior de nuestra figura, esto implica que aquellos terminales que mas costes han generado son los que hacen un n√∫mero de llamadas proporcional al tiempo que est√°n al teelfono. Pero esto no significa que cuanto mas tiempo y n√∫mero de llamadas hagan m√°s costes generar√°n. Es decir, ha habido terminales que han estado mas de 80 horas acumuladas al telefono durante determinado mes y ha realizado y/o recibido mas de 300000 llamadas, pero dichos terminales han ocasionado poco o nulo costo. Los terminales que mas costes han generado, han realizado un n√∫mero de llamadas entre 10000 y 30000 y han estado al telefono entre 25 y 65 horas aproximadamente. La conclusi√≥n que sacamos es que en nuestro dataset los terminales que mas horas han estado al telefono y mas llamadas han realizdo no necesariamente son los que mas costes han generado. Adem√°s un n√∫mero de llamadas proporcional a la duraci√≥n de las mismas normalmente salvo casos donde se ha usado mucho el telefono, conducen a un amuento del costo, que seguramente por la matriz de Spearman, esos terminales corresponder√°n a trabajadores que hayan usado  demasiado el telefono movil y en las llamadas que han realizado hayan estado al t√©fono bastante tiempo. Por el contexto de nuestros datos, cuantas m√°s llamadas m√≥viles hagas m√°s llamadas acumuladas tendr√°s aunque no sea del todo fuerte la relaci√≥n monotona. Tambi√©n concluimos que el coste no es proporcional ni al n√∫mero de llamadas ni al tiempo de uso total, pero zona de coste m√°ximo se encuentra en un rango medio tanto de llamadas como de duraci√≥n. Seguramente los terminales que mas uso han hecho del telefono hayan realizado un gran n√∫mero de llamadas dentro de la organizaci√≥n, o por  determinados trabajadores (como centralita) no se han generado costes.

# Aplicaci√≥n de algoritmos de detenci√≥n de anomalias

El objetivo principal de esta secci√≥n es a partir de la informaci√≥n y analisis exploratorio que hemos hecho antes en nuestros datos, podamos describir el comportamiento de modelos que aplicaremos para detenci√≥n de anomalias. Primero empezaremos detectando tel√©fonos que se usen anormalemnte poco a partir de la frecuencia de uso y con esas frecuencias haremos un mapa de palabras para cada caso conreto, viendo tambi√©n si tenemos terminales que no hayan sido usados. Despu√©s implementaremos algoritmos de machine learning de la libreria pyod [], describiendo por qu√© dichos algoritmos han deetectado determinados anomales como poco frecuentes y comparandolos entre si y con el mapa de palabras. Por √∫ltimo aplicaremos una tecina llamada analisis de Pareto, variante de la curva de Pareto que es en ciertois casos muy √∫til para detectar anomalias. Sacaremos conclusiones claras sobre nuestros resultados tambi√©n mediremos para cada modelo no solamente el rendimiento sino tambi√©n el tiempo computacional y el almacenamiento que este modelo ha requerido.

## Wordmap

En esta subsecci√≥n lo primero que haremos es detectar los registros que no han usado el telefono para cada mes y a√±o correspondiente. Posteriormente implementaremos un m√©todo basado en el mapa (o nube) de palabras para detectar terminales con poco uso en comparaci√≥n con el resto.
Por el contexto de los datos, la suma de las variables asociadas al n√∫mero de tipo de llamadas es NUm.llamadas y se sobrentiende que si no hay llamadas, no hay duraci√≥n (s) para ese registro. Implementemos el c√≥digo.
"""

x1=datos[datos['Num.LLamadas']==0]['Extensi√≥n'].to_numpy(dtype='object')
x2=datos[datos['Num.LLamadas']==0]['mes'].to_numpy(dtype='object')
x3=datos[datos['Num.LLamadas']==0]['a√±o'].to_numpy(dtype='object')
n=len(x1)
X_ceros=list()
for i in range(n):
  X_ceros.append(x1[i]+'-'+x2[i]+'-'+x3[i])
df=pd.DataFrame({
    "Registros": X_ceros,
    "Num.LLamadas":[0]*n
})
df_style = (
    df.style
    .set_properties(**{
        'background-color': '#fdfdfd',
        'border': '1px solid #ccc',
        'padding': '8px'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
    .highlight_min(subset=['Num.LLamadas'], color='#d0f0d0')
)

df_style

"""Estos son los terminales que no usaron el telefono por cada mes y a√±o de los datos originales Si supongasemos que los terminales que no aparecen en nuestros datos en cada mes y a√±o fueran inactivos, la situaci√≥n cambia por completo.

Para los datos2 tenemos la siguiente tabla
"""

import pandas as pd
import math

# ===== 1. Filtrar datos con Num.LLamadas == 0 =====
x1 = datos2[datos2['Num.LLamadas'] == 0]['Extensi√≥n'].to_numpy(dtype='object')
x2 = datos2[datos2['Num.LLamadas'] == 0]['mes'].to_numpy(dtype='object')
x3 = datos2[datos2['Num.LLamadas'] == 0]['a√±o'].to_numpy(dtype='object')

n = len(x1)

# ===== 2. Crear lista de registros =====
X_ceros = []
for i in range(n):
    X_ceros.append(f"{x1[i]}-{x2[i]}-{x3[i]}")

# ===== 3. Organizar en horizontal (10 filas por columna) =====
filas_por_col = 10
num_cols = math.ceil(n / filas_por_col)

data = {}
for i in range(num_cols):
    inicio = i * filas_por_col
    fin = inicio + filas_por_col
    bloque = X_ceros[inicio:fin]

    # Rellenar con None si la columna queda incompleta
    if len(bloque) < filas_por_col:
        bloque += [None] * (filas_por_col - len(bloque))

    data[f'Registros_{i+1}'] = bloque

df_horizontal = pd.DataFrame(data)

# ===== 4. Aplicar estilo =====
df_style = (
    df_horizontal.style
    .set_properties(**{
        'background-color': '#fdfdfd',
        'border': '1px solid #ccc',
        'padding': '8px'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
)

# ===== 5. Mostrar tabla =====
df_style

"""Calculemos el n√∫mero total de registros inactivos bajo esta suposici√≥n"""

print("N√∫mero de registros inactivos: ",df.shape[0])

"""Podemos agrupar los registros por mes y a√±o"""

from IPython.display import display
import pandas as pd

# Listas de meses y a√±os
meses = [
    "enero", "febrero", "marzo", "abril", "mayo", "junio",
    "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre"
]
years = ['2024', '2025']

# DataFrame base
df = pd.DataFrame({
    'Registro': x1,
    'mes': x2,
    'a√±o': x3
})

# ===== Crear columnas por mes-a√±o =====
columnas = {}

for year in years:
  x=datos_num2[datos_num2['a√±o']==year]['mes'].unique()
  for mes in x:
        filtro = df[(df['a√±o'] == year) & (df['mes'] == mes)]
        col_name = f"{mes.capitalize()} {year}"
        columnas[col_name] = filtro['Registro'].reset_index(drop=True)

# ===== Unir todo en un solo DataFrame horizontal =====
df_final = pd.DataFrame(columnas)

# ===== Funci√≥n para colorear celdas =====
def colorear_celdas(val):
    if pd.isna(val):
        return 'background-color: #f0f0f0'  # Gris claro para NaN
    else:
        return 'background-color: #d0f0d0'  # Verde claro para registros

# ===== Aplicar estilo =====
df_style = (
    df_final.style
    .applymap(colorear_celdas)  # Aplica color a cada celda seg√∫n valor
    .set_properties(**{
        'border': '1px solid #ccc',
        'padding': '8px',
        'text-align': 'center'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px'),
                ('text-align', 'center')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
)

# ===== Mostrar =====
display(df_style)

"""Podemos hacer un gr√°fico de barras para cada a√±o de la cantidad de terminales inicativos para cada mes"""

lista2024=list()
lista2025=list()
for i in meses:
  df1=df[(df['a√±o']=='2024')&(df['mes']==i)]
  lista2024.append(df1.shape[0])
for i in meses[:7]:
  df1=df[(df['a√±o']=='2025')&(df['mes']==i)]
  lista2025.append(df1.shape[0])

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
plt.subplots_adjust(wspace=0.3, hspace=0.4)

# Gr√°fico 2024
axes[0].bar(meses, lista2024, color='blue', edgecolor='black')
axes[0].set_title("N√∫mero de terminales no usados en 2024", fontsize=14, fontweight='bold')
axes[0].set_xlabel("Meses", fontsize=12)
axes[0].tick_params(axis='x', rotation=45)


# Gr√°fico 2025
axes[1].bar(meses[:7],lista2025, color='blue', edgecolor='black')
axes[1].set_title("N√∫mero de terminales no usados en 2025", fontsize=14, fontweight='bold')
axes[1].set_xlabel("Meses", fontsize=12)
axes[1].tick_params(axis='x', rotation=45)

"""En 2024 no hay gran diferencia entre los meses en cuanto al n√∫mero de terminales sin usar, salvo agosto que es el mes donde m√°s terminales no han realizado ninguna llamada. El mes donde hay menos terminales que no hayan usado el telefono es octubre.

En 2025, el mes con mayor n√∫mero de terminales inactivos es julio.

Tambi√©n podemos realizar una grafica que muestre la evoluci√≥n temporal del n√∫mero de terminales inactivos a lo largo de los meses de 2024 y 2025.
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))  # Tama√±o mejorado
plt.style.use("ggplot")

# ---- Estilo de las l√≠neas ----
plt.plot(
    meses,
    lista2024,
    color='seagreen',
    marker='o',
    markersize=7,
    linewidth=2,
    label='A√±o 2024'
)

plt.plot(
    meses[0:7],
    lista2025,
    color='indianred',
    marker='s',
    markersize=7,
    linewidth=2,
    label='A√±o 2025'
)

# ---- T√≠tulos y etiquetas ----
plt.title("acumulaci√≥n de terminales inactivos por mes", fontsize=12, fontweight='bold')
plt.xlabel("Mes", fontsize=10)
plt.ylabel("Duraci√≥n (s) ", fontsize=10)

# ---- Escala, rejilla y rotaci√≥n ----
plt.yscale('log')
plt.xticks(rotation=45)
plt.grid(alpha=0.3, linewidth=0.7)

# ---- Leyenda ----
plt.legend(loc="upper left", fontsize=9)

plt.tight_layout()
plt.show()

"""En 2024 hay muchas variaciones entre los primeros meses. Ya en verano se va estabilizando la curva, pero se produce una gran subida en agosto con una gran bajada hasta octubre y otra subida tambi√©n pronunciada en diciembre.

A diferencia de 2024, 2025 no sufre grandes cambios entre los meses, hasta que llegamos a julio, donde hay una gran subida. Se observa que el el m√≠nimo de n√∫mero de registros inactivos es octubre de 2024 y el m√°ximo julio de 2025.

Pasemos ahora a crear una nube de palabras con los terminales.  Esta nube se basara en todas las caracter√≠sticas de nuestros datos, de forma que el terminal que tenga menos llamadas y haya durado menos en las llamadas ser√° el m√°s grande dentro de la nuve de palabras.

Tambi√©n haremos otras dos nubes de palabras solamente con la variable N√∫mero de LLamadas y Duraci√≥n (s) respectivamente. Veremos si se aprecian muchas diferencias o no.

Creo que lo m√°s conveniente ser√≠a hacer esa nuve de puntos para los terminales de mi dataset original y no el modificado bajo nuestra suposici√≥n, ya que hay una gran cantidad de registros inactivos. La nuve de palabras para este dataset ser√° muy confusa.

Primero empezaremos creando la nube de palabras en funci√≥n de la variable Num.LLamadas
"""

from wordcloud import WordCloud
#invertimos los valores de NUm.llamadas, ya que wordcloud considerar√° los mas grandes como los mas frecuentes. En la inversi√≥n le sumamos 1 ya que vimos que hay registros nulos.
datos3=datos.copy()
datos3["peso"] = 1 / (datos3["Num.LLamadas"]+1)
#Creamos otra columna que contenga registro, mes y a√±o junto
x1=datos3['Extensi√≥n'].to_numpy(dtype='object')
x2=datos3['mes'].to_numpy(dtype='object')
x3=datos3['a√±o'].to_numpy(dtype='object')
n=len(x1)
X_ceros=list()
for i in range(n):
  X_ceros.append(x1[i]+'-'+x2[i]+'-'+x3[i])
datos3['Registros Completos']=X_ceros
frecuencias = dict(zip(datos3["Registros Completos"], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en la variable Num.LLamadas', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'      # Tipo de letra limpio
)
plt.axis("off")
plt.show()

"""Como era obvio, los registros mas grandes son los registros inactivos que extra√≠ de mi dataset original. Veamos que pasa con la duraci√≥n (s)"""

from wordcloud import WordCloud
#invertimos los valores de NUm.llamadas, ya que wordcloud considerar√° los mas grandes como los mas frecuentes. En la inversi√≥n le sumamos 1 ya que vimos que hay registros nulos.
datos3=datos.copy()
datos3["peso"] = 1 / (datos3["Duraci√≥n (s)"]+1)
#Creamos otra columna que contenga registro, mes y a√±o junto
x1=datos3['Extensi√≥n'].to_numpy(dtype='object')
x2=datos3['mes'].to_numpy(dtype='object')
x3=datos3['a√±o'].to_numpy(dtype='object')
n=len(x1)
X_ceros=list()
for i in range(n):
  X_ceros.append(x1[i]+'-'+x2[i]+'-'+x3[i])
datos3['Registros Completos']=X_ceros
frecuencias = dict(zip(datos3["Registros Completos"], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en la variable Duraci√≥n(s) ', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'      # Tipo de letra limpio
)
plt.axis("off")
plt.show()

"""Puede ocurrir que haya registros que la nube de palabras haya detectado como poco frecuentes ya que probablemente haya habido terminales que hayan realizado varias llamadas pero queno hayan durado en total lo suficiente para ser recogidas en nuestro dataset original. Tambi√©n puede ocurrir que dicho terminal haya recibido o realizado llamadas que no hayan sido atendidas. Lo m√°s coherente ser√≠a pensar en el segundo caso.
Pasemos a una visualizaci√≥n de los registros inactivos en cuanto a la duraci√≥n de llamadas

"""

import pandas as pd
import math

# ===== 1. Filtrar datos con Duraci√≥n (s) == 0 =====
x1 = datos2[datos2['Duraci√≥n (s)'] == 0]['Extensi√≥n'].to_numpy(dtype='object')
x2 = datos2[datos2['Duraci√≥n (s)'] == 0]['mes'].to_numpy(dtype='object')
x3 = datos2[datos2['Duraci√≥n (s)'] == 0]['a√±o'].to_numpy(dtype='object')

n = len(x1)

# ===== 2. Crear lista de registros =====
X_ceros = []
for i in range(n):
    X_ceros.append(f"{x1[i]}-{x2[i]}-{x3[i]}")

# ===== 3. Organizar en horizontal (10 filas por columna) =====
filas_por_col = 10
num_cols = math.ceil(n / filas_por_col)

data = {}
for i in range(num_cols):
    inicio = i * filas_por_col
    fin = inicio + filas_por_col
    bloque = X_ceros[inicio:fin]

    # Rellenar con None si la columna queda incompleta
    if len(bloque) < filas_por_col:
        bloque += [None] * (filas_por_col - len(bloque))

    data[f'Registros_{i+1}'] = bloque

df_horizontal = pd.DataFrame(data)

# ===== 4. Aplicar estilo =====
df_style = (
    df_horizontal.style
    .set_properties(**{
        'background-color': '#fdfdfd',
        'border': '1px solid #ccc',
        'padding': '8px'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
)

# ===== 5. Mostrar tabla =====
df_style

"""Agrupemoslos por mes y a√±o"""

from IPython.display import display
import pandas as pd

# Listas de meses y a√±os
meses = [
    "enero", "febrero", "marzo", "abril", "mayo", "junio",
    "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre"
]
years = ['2024', '2025']

# DataFrame base
df = pd.DataFrame({
    'Registro': x1,
    'mes': x2,
    'a√±o': x3
})

# ===== Crear columnas por mes-a√±o =====
columnas = {}

for year in years:
  x=datos_num2[datos_num2['a√±o']==year]['mes'].unique()
  for mes in x:
        filtro = df[(df['a√±o'] == year) & (df['mes'] == mes)]
        col_name = f"{mes.capitalize()} {year}"
        columnas[col_name] = filtro['Registro'].reset_index(drop=True)

# ===== Unir todo en un solo DataFrame horizontal =====
df_final = pd.DataFrame(columnas)

# ===== Funci√≥n para colorear celdas =====
def colorear_celdas(val):
    if pd.isna(val):
        return 'background-color: #f0f0f0'  # Gris claro para NaN
    else:
        return 'background-color: #d0f0d0'  # Verde claro para registros

# ===== Aplicar estilo =====
df_style = (
    df_final.style
    .applymap(colorear_celdas)  # Aplica color a cada celda seg√∫n valor
    .set_properties(**{
        'border': '1px solid #ccc',
        'padding': '8px',
        'text-align': 'center'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px'),
                ('text-align', 'center')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
)

# ===== Mostrar =====
display(df_style)

"""Podemos hacer un gr√°fico de barras para cada a√±o de la cantidad de terminales inicativos para cada mes en cuanto a duraci√≥n (s)."""

lista2024=list()
lista2025=list()
for i in meses:
  df1=df[(df['a√±o']=='2024')&(df['mes']==i)]
  lista2024.append(df1.shape[0])
for i in meses[:7]:
  df1=df[(df['a√±o']=='2025')&(df['mes']==i)]
  lista2025.append(df1.shape[0])

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
plt.subplots_adjust(wspace=0.3, hspace=0.4)

# Gr√°fico 2024
axes[0].bar(meses, lista2024, color='blue', edgecolor='black')
axes[0].set_title("N√∫mero de terminales no usados en 2024", fontsize=14, fontweight='bold')
axes[0].set_xlabel("Meses", fontsize=12)
axes[0].tick_params(axis='x', rotation=45)


# Gr√°fico 2025
axes[1].bar(meses[:7],lista2025, color='blue', edgecolor='black')
axes[1].set_title("N√∫mero de terminales no usados en 2025", fontsize=14, fontweight='bold')
axes[1].set_xlabel("Meses", fontsize=12)
axes[1].tick_params(axis='x', rotation=45)

"""En este caso en 2024 hay mayor diferencia entre los meses. El mes donde mas registros inactivos ha habido es febrero y el que menos enero. Curioso ya que hay un cambio brusco entre dos meses seguidos.

En 2025 la diferencia es m√°s moderada, siendo enero el mes donde ha habido m√°s registros inactivos y mayo donde menos.

Como en el caso de Num.llamadas podemos realizar una grafica que muestre la evoluci√≥n temporal del n√∫mero de terminales inactivos a lo largo de los meses de 2024 y 2025.
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))  # Tama√±o mejorado
plt.style.use("ggplot")

# ---- Estilo de las l√≠neas ----
plt.plot(
    meses,
    lista2024,
    color='seagreen',
    marker='o',
    markersize=7,
    linewidth=2,
    label='A√±o 2024'
)

plt.plot(
    meses[0:7],
    lista2025,
    color='indianred',
    marker='s',
    markersize=7,
    linewidth=2,
    label='A√±o 2025'
)

# ---- T√≠tulos y etiquetas ----
plt.title("acumulaci√≥n de terminales inactivos en cuanto a duraci√≥n (s) por mes", fontsize=12, fontweight='bold')
plt.xlabel("Mes", fontsize=10)
plt.ylabel("Duraci√≥n (s) ", fontsize=10)

# ---- Escala, rejilla y rotaci√≥n ----
plt.yscale('log')
plt.xticks(rotation=45)
plt.grid(alpha=0.3, linewidth=0.7)

# ---- Leyenda ----
plt.legend(loc="upper left", fontsize=9)

plt.tight_layout()
plt.show()

"""En 2024 hay una gran variaci√≥n de n√∫mero de terminales sobretodo al principio. En cambio, en 2025 se muestra una variaci√≥n leve entre los meses, salvo los tres √∫ltimos, que es m√°s notoria pero menos significativa que las apreciadas en 2024.

Debido a la existencia de registros que hayan recibido o realizado llamadas pero que estas no hayan sido atendidas, haremos un mapa de palabras mostrando los registros menos activos en funci√≥n de duraci√≥n (s) y Num.LLamadas. Pero antes, debido a que Duraci√≥n (s) tiene un tama√±o bastante superior a NUm.llamadas, es necesario hacer un escalado de variables. Debido a la existencia de outliers de gran tama√±o en ambas variables y al peso e influencia que estos tienen, por lo mostrado en las gr√°ficas anteriores (diagrama de Pareto, boxplots,etc.), el escalado de variables m√°s recomendado es el robusto. Sin embargo para este m√©todo hay un problema: el escalado robusto nos puede dar valores negativos y en este m√©todo concreto necesitamos que todos los valores sean positivos. Todos los dem√°s m√©todos de estandarizaci√≥n, escalado y normalizaci√≥n son sensibles a outliers. Pero entre los m√©todos minmax, log,  normalized y standarsclaer, el que menos se ve perjudicado por los outliers es el logaritmo.

Ya habiamos aplicado este m√©todo para visualizar los histogramas y boxplots con mayor claridad. Veamos las caracter√≠sticas principales de nuestro conjunto de datos al que le habiamos aplicado la funci√≥n logaritmo
"""

datos_num_log.describe()

"""Vemos que todav√≠a hay  diferencia entre los tama√±os de las variables, sobretodo entre las de duraci√≥n y las de n√∫mero de llamadas. Una idea ser√≠a volver a aplicar la funci√≥n logaritmo para reducir significativamente la diferencia entre escalas."""

datos_num_log_log=datos_num_log.copy()
columnasnumlog=datos_num_log.select_dtypes(include=['int', 'float']).columns
datos_num_log_log[columnasnumlog]=datos_num_log_log[columnasnumlog].apply(lambda x:np.log(x+1))#sumo 1 para evitar el problema log(0)

"""Ahora se puede apreciar una menor diferencia entre las variables"""

datos_num_log_log.describe()

"""Vamos a crear una nube de palabras que detecte terminales que se usen anormalmente poco, basado en las variables duraci√≥n (s) y num.llamadas, aplicando este m√©todo a nuestro dataset original escalado."""

datos3=datos_num_log_log.copy()
datos3["peso"] = 1 / (datos3["Num.LLamadas"]+datos3["Duraci√≥n (s)"]+1e-6)
#a√±ado 1e-6 para detectar aqueyos terminales con nulo uso y diferenciarles con el resto.
#Creamos otra columna que contenga registro, mes y a√±o junto
x1=datos3['Extensi√≥n'].to_numpy(dtype='object')
x2=datos3['mes'].to_numpy(dtype='object')
x3=datos3['a√±o'].to_numpy(dtype='object')
n=len(x1)
X_ceros=list()
for i in range(n):
  X_ceros.append(x1[i]+'-'+x2[i]+'-'+x3[i])
datos3['Registros Completos']=X_ceros
frecuencias = dict(zip(datos3["Registros Completos"], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en tlas variables duraci√≥n(s) y Num.LLamadas', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'      # Tipo de letra limpio
)
plt.axis("off")
plt.show()

"""Los registros de gran tama√±o son los mismos que registramos para el caso de n√∫mero de llamadas debido a que si no hay llamadas, obviamente no hay duraci√≥n (s). Tambi√©n se aprecen con menor tama√±o pero considerable, los registros que han llamado y recibido llamadas pero no han sido atendidas.

Pasemos ahora a implementar este m√©todo considerando todas las variables.
"""

datos3=datos_num_log_log.copy()
columnas=datos3.select_dtypes(include=['int', 'float']).columns
n=len(columnas)
dt=datos3[columnas[0]]
for i in range(1,n):
  dt=datos3[columnas[i]]+dt
datos3["peso"] = 1 / (dt+1e-6)
#a√±ado 1e-6 para detectar aqueyos terminales con nulo uso y diferenciarles con el resto.
#Creamos otra columna que contenga registro, mes y a√±o junto
x1=datos3['Extensi√≥n'].to_numpy(dtype='object')
x2=datos3['mes'].to_numpy(dtype='object')
x3=datos3['a√±o'].to_numpy(dtype='object')
n=len(x1)
X_ceros=list()
for i in range(n):
  X_ceros.append(x1[i]+'-'+x2[i]+'-'+x3[i])
datos3['Registros Completos']=X_ceros
frecuencias = dict(zip(datos3["Registros Completos"], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en todas las categorias num√©ricas', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'    # Tipo de letra limpio
)
plt.show()

"""En este caso se siguen distinguiendo los terminales que no se han usado ni han recibido llamadas.

LAs desventajas que presenta este m√©todo para detenci√≥n de tel√©fonos que se usan anormalmente poco con respecto a los implementados por pyod es que da la misma importancia a las variables y no intenta encontrar patrones entre ellas para detectar anomalias mas dificiles de apreciar a simple vista.

Sin embargo la gran ventaja que presenta este m√©todo es que no se ve influenciado por outliers de gran tama√±o a diferencia de algunos de los m√©todos de pyod.

Podemos tambi√©n hacer nubes de palabras para detectar los terminales que se usan poco, pero considerando su uso total durante los 19 meses que contiene nuestro dataset original. En este caso aplicar√© mi hip√≥tesis de que los terminales que no fueron registrados en cada mes y a√±o espec√≠fico no han sido usados.
"""

x=datos_num2['Extensi√≥n'].unique()
n=len(x)
c1=datos_num2.select_dtypes(include=[int,float]).columns
datosacumulados={'Extensi√≥n': [],
 'Nacionales': [],
 'M√≥viles': [],
 'Salientes Duraci√≥n (s)': [],
 'Entrantes Num.LLamadas': [],
 'Entrantes Duraci√≥n (s)': [],
 'Internas Num.LLamadas': [],
 'Internas Duraci√≥n (s)': [],
 'Entrantes': [],
 'Salientes': [],
 'Internas salientes': [],
 'Num.LLamadas': [],
 'Duraci√≥n (s)': [],
 'Coste': []}
for i in x:
  dt12=datos_num2[datos_num2['Extensi√≥n']==i]
  datosacumulados['Extensi√≥n'].append(i)
  for j in c1:
    datosacumulados[j].append(dt12[j].sum())
datosacumulados=pd.DataFrame(datosacumulados)

datosacumulados.head(10)

"""Primero comprobemos si hay registros sin uso tanto para Num.LLamadas como Duraci√≥n (s)"""

x1 = datosacumulados[datosacumulados['Num.LLamadas'] == 0]['Extensi√≥n'].to_numpy(dtype='object')
n=len(x1)
df = pd.DataFrame({
    "Registros": x1,
    "Num.LLamadas": [0] * n
})
df_style = (
    df.style
    .set_properties(**{
        'background-color': '#fdfdfd',
        'border': '1px solid #ccc',
        'padding': '8px'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
    .highlight_min(subset=['Num.LLamadas'], color='#d0f0d0')
)

df_style

"""No se observan registros sin llamadas. Veamos si hay registros que no hayan atendido a las llamadas

### TABLA ANOMALIAS
"""

x1 = datosacumulados[datosacumulados['Duraci√≥n (s)'] == 0]['Extensi√≥n'].to_numpy(dtype='object')
n=len(x1)
df = pd.DataFrame({
    "Registros": x1,
    "Duraci√≥n (s)": [0] * n,
    "Nacionales": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'Nacionales'].iloc[0] for ext in x1],
"M√≥viles": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'M√≥viles'].iloc[0] for ext in x1],
"Entrantes Num.LLamadas": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'Entrantes Num.LLamadas'].iloc[0] for ext in x1],
"Internas Num.LLamadas": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'Internas Num.LLamadas'].iloc[0] for ext in x1],
"Entrantes": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'Entrantes'].iloc[0] for ext in x1],
"Salientes": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'Salientes'].iloc[0] for ext in x1],
"Internas salientes": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'Internas salientes'].iloc[0] for ext in x1],
"Num.LLamadas": [datosacumulados.loc[datosacumulados['Extensi√≥n'] == ext, 'Num.LLamadas'].iloc[0] for ext in x1]

})
print("TABLA")
df_style = (
    df.style
    .set_properties(**{
        'background-color': '#fdfdfd',
        'border': '1px solid #ccc',
        'padding': '8px'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
    .highlight_min(subset=['Duraci√≥n (s)'], color='#d0f0d0')
)

df_style

dt2 = datos_num2.copy()
col = dt2.select_dtypes(include=['int', 'float']).columns
dt2[col] = dt2[col].apply(lambda x: np.log(x + 1))

Q3N = dt2['Num.LLamadas'].quantile(0.75)
IQR = Q3N - dt2['Num.LLamadas'].quantile(0.25)
Q3D = dt2['Duraci√≥n (s)'].quantile(0.75)
IQR1 = Q3D - dt2['Duraci√≥n (s)'].quantile(0.25)

LIMNum = IQR * 1.5 + Q3N
LIMDur = IQR1 * 1.5 + Q3D

x = dt2[(dt2['Num.LLamadas'] >= LIMNum) & (dt2['Duraci√≥n (s)'] >= LIMDur)]['Extensi√≥n'].to_numpy(dtype='object')
xmes=dt2[(dt2['Num.LLamadas'] >= LIMNum) & (dt2['Duraci√≥n (s)'] >= LIMDur)]['mes'].to_numpy(dtype='object')
xyear=dt2[(dt2['Num.LLamadas'] >= LIMNum) & (dt2['Duraci√≥n (s)'] >= LIMDur)]['a√±o'].to_numpy(dtype='object')
n=len(x)
X=list()
for i in range(n):
  X.append(x[i]+'-'+xmes[i]+'-'+xyear[i])
df = pd.DataFrame({
    "Registros": X,
    "Duraci√≥n (s)": [datos.loc[datos['Extensi√≥n'] == ext, 'Duraci√≥n (s)'].iloc[0] for ext in x],
    "Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Num.LLamadas'].iloc[0] for ext in x],
    "Nacionales": [datos.loc[datos['Extensi√≥n'] == ext, 'Nacionales'].iloc[0] for ext in x],
    "M√≥viles": [datos.loc[datos['Extensi√≥n'] == ext, 'M√≥viles'].iloc[0] for ext in x],
    "Entrantes Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Entrantes Num.LLamadas'].iloc[0] for ext in x],
    "Internas Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Internas Num.LLamadas'].iloc[0] for ext in x],
    "Entrantes": [datos.loc[datos['Extensi√≥n'] == ext, 'Entrantes'].iloc[0] for ext in x],
    "Salientes": [datos.loc[datos['Extensi√≥n'] == ext, 'Salientes'].iloc[0] for ext in x],
    "Internas salientes": [datos.loc[datos['Extensi√≥n'] == ext, 'Internas salientes'].iloc[0] for ext in x]
})

print("TABLA")
df_style = (
    df.style
    .set_properties(**{
        'border': '1px solid #ccc',
        'padding': '8px'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
    # Pintar toda la columna Duraci√≥n (s) y Num.LLamadas en cian
    .applymap(lambda x: 'background-color: #00FFFF', subset=['Duraci√≥n (s)', 'Num.LLamadas'])
    # Pintar toda la columna Registros (Extensi√≥n) en verde claro
    .applymap(lambda x: 'background-color: #b3ffb3', subset=['Registros'])
)

df_style


xx = dt2[(dt2['Num.LLamadas'] >= LIMNum) & (dt2['Duraci√≥n (s)'] < LIMDur)]['Extensi√≥n'].to_numpy(dtype='object')
xxmes=dt2[(dt2['Num.LLamadas'] >= LIMNum) & (dt2['Duraci√≥n (s)'] < LIMDur)]['mes'].to_numpy(dtype='object')
xxyear=dt2[(dt2['Num.LLamadas'] >= LIMNum) & (dt2['Duraci√≥n (s)'] < LIMDur)]['a√±o'].to_numpy(dtype='object')
n=len(xx)
XX=list()
for i in range(n):
  XX.append(xx[i]+'-'+xxmes[i]+'-'+xxyear[i])
df1 = pd.DataFrame({
    "Registros": XX,
    "Duraci√≥n (s)": [datos.loc[datos['Extensi√≥n'] == ext, 'Duraci√≥n (s)'].iloc[0] for ext in xx],
    "Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Num.LLamadas'].iloc[0] for ext in xx],
    "Nacionales": [datos.loc[datos['Extensi√≥n'] == ext, 'Nacionales'].iloc[0] for ext in xx],
    "M√≥viles": [datos.loc[datos['Extensi√≥n'] == ext, 'M√≥viles'].iloc[0] for ext in xx],
    "Entrantes Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Entrantes Num.LLamadas'].iloc[0] for ext in xx],
    "Internas Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Internas Num.LLamadas'].iloc[0] for ext in xx],
    "Entrantes": [datos.loc[datos['Extensi√≥n'] == ext, 'Entrantes'].iloc[0] for ext in xx],
    "Salientes": [datos.loc[datos['Extensi√≥n'] == ext, 'Salientes'].iloc[0] for ext in xx],
    "Internas salientes": [datos.loc[datos['Extensi√≥n'] == ext, 'Internas salientes'].iloc[0] for ext in xx]
})




xxx = dt2[(dt2['Num.LLamadas'] < LIMNum) & (dt2['Duraci√≥n (s)'] >= LIMDur)]['Extensi√≥n'].to_numpy(dtype='object')
xxxmes=dt2[(dt2['Num.LLamadas'] < LIMNum) & (dt2['Duraci√≥n (s)'] >= LIMDur)]['mes'].to_numpy(dtype='object')
xxxyear=dt2[(dt2['Num.LLamadas'] < LIMNum) & (dt2['Duraci√≥n (s)'] >= LIMDur)]['a√±o'].to_numpy(dtype='object')
n=len(xxx)
XXX=list()
for i in range(n):
  XXX.append(x[i]+'-'+xmes[i]+'-'+xyear[i])
df2 = pd.DataFrame({
    "Registros": XXX,
    "Duraci√≥n (s)": [datos.loc[datos['Extensi√≥n'] == ext, 'Duraci√≥n (s)'].iloc[0] for ext in xxx],
    "Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Num.LLamadas'].iloc[0] for ext in xxx],
    "Nacionales": [datos.loc[datos['Extensi√≥n'] == ext, 'Nacionales'].iloc[0] for ext in xxx],
    "M√≥viles": [datos.loc[datos['Extensi√≥n'] == ext, 'M√≥viles'].iloc[0] for ext in xxx],
    "Entrantes Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Entrantes Num.LLamadas'].iloc[0] for ext in xxx],
    "Internas Num.LLamadas": [datos.loc[datos['Extensi√≥n'] == ext, 'Internas Num.LLamadas'].iloc[0] for ext in xxx],
    "Entrantes": [datos.loc[datos['Extensi√≥n'] == ext, 'Entrantes'].iloc[0] for ext in xxx],
    "Salientes": [datos.loc[datos['Extensi√≥n'] == ext, 'Salientes'].iloc[0] for ext in xxx],
    "Internas salientes": [datos.loc[datos['Extensi√≥n'] == ext, 'Internas salientes'].iloc[0] for ext in xxx]
})

"""## TABLA ANOMALIAS MUCHO USO"""

# A√±adimos columna Grupo para identificar cada tabla
df['Grupo'] = 'Alto-Alto'
df1['Grupo'] = 'Alto-Bajo'
df2['Grupo'] = 'Bajo-Alto'

# Concatenamos los tres DataFrames
df_total = pd.concat([df, df1, df2], ignore_index=True)

# Estilizamos seg√∫n el grupo

def color_registros(grupo):
    if grupo == 'Alto-Alto':
        return 'background-color: #b3ffb3'  # verde claro
    elif grupo == 'Alto-Bajo':
        return 'background-color: #FF66FF'  # magenta
    elif grupo == 'Bajo-Alto':
        return 'background-color: #FFFFCC'  # amarillo blanqueado
    else:
        return ''

df_total_style = (
    df_total.style
    .set_properties(**{'border': '1px solid #ccc', 'padding': '8px'})
    .set_table_styles([
        {'selector': 'th',
         'props': [('background-color', '#4A90E2'),
                   ('color', 'white'),
                   ('font-size', '14px'),
                   ('padding', '8px')]},
        {'selector': 'tr:hover',
         'props': [('background-color', '#e6f2ff')]}
    ])
    # Columnas Duraci√≥n y Num.LLamadas en cian
    .applymap(lambda x: 'background-color: #00FFFF',
              subset=['Duraci√≥n (s)', 'Num.LLamadas'])
    # Colorear Registros seg√∫n el Grupo
    .apply(
        lambda row: [
            color_registros(row['Grupo']) if col == 'Registros' else ''
            for col in df_total.columns
        ],
        axis=1
    )
)
df_total_style = df_total_style.hide(['Grupo'], axis=1)
df_total_style

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Columnas para clustering
X_cluster = dt2[['Num.LLamadas', 'Duraci√≥n (s)']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

# KMeans con 15 clusters
kmeans = KMeans(n_clusters=15, random_state=42)
dt2['Cluster'] = kmeans.fit_predict(X_scaled)

# Colores para 15 clusters
cmap = plt.get_cmap('tab20')  # 20 colores diferenciables
colors = [cmap(i % 20) for i in dt2['Cluster']]

plt.figure(figsize=(12,8))
plt.scatter(dt2['Num.LLamadas'], dt2['Duraci√≥n (s)'], c=colors, s=50, alpha=0.8)

# Opcional: poner el n√∫mero del cluster sobre cada punto
for i, row in dt2.iterrows():
    plt.text(row['Num.LLamadas'], row['Duraci√≥n (s)'], str(row['Cluster']),
             fontsize=8, ha='center', va='center')

plt.xlabel('Num.LLamadas')
plt.ylabel('Duraci√≥n (s)')
plt.title('Clusters de Num.LLamadas vs Duraci√≥n (15 clusters)')
plt.show()

"""## TABLA ANOMALIAS MUCHO USO BASADA EN CLUSTERS"""

# Filtramos solo los datos del cluster 11
cluster_11 = dt2[dt2['Cluster'] == 11]
df=cluster_11
x=cluster_11['Extensi√≥n'].to_numpy(dtype='object')
xmes=cluster_11['mes'].to_numpy(dtype='object')
xyear=cluster_11['a√±o'].to_numpy(dtype='object')
X=list()
n=len(cluster_11)
for i in range(n):
  X.append(x[i]+'-'+xmes[i]+'-'+xyear[i])
cols_num = cluster_11.select_dtypes(include=['float', 'int']).columns
cluster_11[cols_num] = cluster_11[cols_num].apply(lambda x: np.exp(x) - 1)
# Creamos el DataFrame usando SOLO columnas de cluster_11
df = pd.DataFrame({
    "Registros": X,
    "Duraci√≥n (s)": cluster_11['Duraci√≥n (s)'],
    "Num.LLamadas": cluster_11['Num.LLamadas'],
    "Nacionales": cluster_11['Nacionales'],
    "M√≥viles": cluster_11['M√≥viles'],
    "Entrantes Num.LLamadas": cluster_11['Entrantes Num.LLamadas'],
    "Internas Num.LLamadas": cluster_11['Internas Num.LLamadas'],
    "Entrantes": cluster_11['Entrantes'],
    "Salientes": cluster_11['Salientes'],
    "Internas salientes": cluster_11['Internas salientes']
})
df_style = (
    df.style
    .set_properties(**{
        'border': '1px solid #ccc',
        'padding': '8px'
    })
    .set_table_styles([
        {
            'selector': 'th',
            'props': [
                ('background-color', '#4A90E2'),
                ('color', 'white'),
                ('font-size', '14px'),
                ('padding', '8px')
            ]
        },
        {
            'selector': 'tr:hover',
            'props': [('background-color', '#e6f2ff')]
        }
    ])
    # Pintar toda la columna Duraci√≥n (s) y Num.LLamadas en cian
    .applymap(lambda x: 'background-color: #00FFFF', subset=['Duraci√≥n (s)', 'Num.LLamadas'])
    # Pintar toda la columna Registros (Extensi√≥n) en verde claro
    .applymap(lambda x: 'background-color: #b3ffb3', subset=['Registros'])
)

df_style

"""En este caso hay un total de 35 registros que no han estado al telefono durante los 19 meses, desde enero de 2024 hasta julio de 2025. Han recibido o realizado llamadas que no han sido contestadas. Todos los terminales han recibido llamadas, excepto 7566, que no han sido contestadas, ya que no hay ning√∫n terminal que las haya realizado. Ha habido algunos terminales que aparte de no contestar llamadas entrantes, han realizado otros tipos de llamadas ("Provinciales","Otras","Internas entrantes") que quit√© de mi dataset porque consider√© que dichas variables eran irrelevantes debido a la escasez de terminales que realizaron o recibieron ese tipo de llamadas. La conclusi√≥n que saco es que normalmente estos terminales no han contestado a las llamadas que han recibido durante a√±o y medio (19 meses). No ha habido ningun terminal de nulo uso que haya realizado llamadas pero que estas no hayan sido atendidas. Los terminales que mas me llaman la atenci√≥n son 3138 y 7312. Han recibibo ambos m√°s de 100 llamadas y ninguna ha sido contestada.

Veamos como son los mapas de palabras para la variable NUm.LLamadas, la variable Duraci√≥n (s), las variables juntas NUm.LLamadas y Duraci√≥n (s) y para el resto de variables.
"""

from wordcloud import WordCloud
#invertimos los valores de NUm.llamadas, ya que wordcloud considerar√° los mas grandes como los mas frecuentes. En la inversi√≥n le sumamos 1 ya que vimos que hay registros nulos.
datos3=datosacumulados.copy()
datos3["peso"] = 1 / (datos3["Num.LLamadas"]+1)
frecuencias = dict(zip(datos3['Extensi√≥n'], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en la variable Num.LLamadas', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'      # Tipo de letra limpio
)
plt.axis("off")
plt.show()

"""Los registros que menos llamadas telef√≥nicas han realizado o recibido son 5002 y 7453. Pasemos a ver los terminales que menos tiempo han estado al tel√©fono."""

from wordcloud import WordCloud
#invertimos los valores de NUm.llamadas, ya que wordcloud considerar√° los mas grandes como los mas frecuentes. En la inversi√≥n le sumamos 1 ya que vimos que hay registros nulos.
datos3=datosacumulados.copy()
datos3["peso"] = 1 / (datos3["Duraci√≥n (s)"]+1)
frecuencias = dict(zip(datos3['Extensi√≥n'], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en la variable Duraci√≥n (s)', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'      # Tipo de letra limpio
)
plt.axis("off")
plt.show()

"""Los registros que menos tiempo han estado al telefono son 3138 y 7284. estos registros al igual que muchos otros que tambi√©n aparecen en grande pero en menor tama√±o, son los de la tabla donde extra√≠a terminales de nulo uso. Veamos que registros de menor uso me detecta la nube de palabras en funci√≥n de NUm.llamadas y Duraci√≥n (s)"""

from wordcloud import WordCloud
#invertimos los valores de NUm.llamadas, ya que wordcloud considerar√° los mas grandes como los mas frecuentes. En la inversi√≥n le sumamos 1 ya que vimos que hay registros nulos.
#pero antes hagamos un escalado de variables con log
datosacumulados_log=datosacumulados.copy()
columnasnumlog=datosacumulados_log.select_dtypes(include=['int', 'float']).columns
datosacumulados_log[columnasnumlog]=datosacumulados_log[columnasnumlog].apply(lambda x:np.log(x+1))#sumo 1 para evitar el problema log(0)
datos3=datosacumulados_log.copy()

datos3["peso"] = 1 / (datos3["Duraci√≥n (s)"]+datos3["Num.LLamadas"]+1e-6)
frecuencias = dict(zip(datos3['Extensi√≥n'], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en la variable Duraci√≥n (s) vs Num.LLamadas', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'      # Tipo de letra limpio
)
plt.axis("off")
plt.show()

"""Esta nube de palabras me detecta el terminal 7453 como de poco uso, el mismo terminal que aparec√≠a en la nube de palabras de la variable Num.LLamadas. Sin embargo no me detecta el terminal 5002. Seguramente este terminal habr√° realizado pocas llamadas pero de mucha duraci√≥n en comparaci√≥n con el resto.

Veamos que ocurre si usamos todas las variables
"""

from wordcloud import WordCloud
#invertimos los valores de NUm.llamadas, ya que wordcloud considerar√° los mas grandes como los mas frecuentes. En la inversi√≥n le sumamos 1 ya que vimos que hay registros nulos.
#pero antes hagamos un escalado de variables con log
datosacumulados_log=datosacumulados.copy()
columnasnumlog=datosacumulados_log.select_dtypes(include=['int', 'float']).columns
datosacumulados_log[columnasnumlog]=datosacumulados_log[columnasnumlog].apply(lambda x:np.log(np.log(x+1)+1))#sumo 1 para evitar el problema log(0)
datos3=datosacumulados_log.copy()
columnas=datos3.select_dtypes(include=['int', 'float']).columns
n=len(columnas)
dt=datos3[columnas[0]]
for i in range(1,n):
  dt=datos3[columnas[i]]+dt
datos3["peso"] = 1 / (dt+1e-6)
frecuencias = dict(zip(datos3['Extensi√≥n'], datos3["peso"]))
wordcloud = WordCloud(width=800, height=400, background_color="white")
wordcloud = wordcloud.generate_from_frequencies(frecuencias)

# Mostramos la nube
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(
    'Detenci√≥n de anomalias con nube de palabras basadas en todas las variables Num√©ricas', # Texto curioso + salto de l√≠nea
    fontsize=20,          # Tama√±o grande para claridad
    fontweight='bold',    # Negrita para impacto
    color='#2c3e50',      # Un color oscuro pero no negro puro (m√°s elegante)
    loc='center',         # Alineaci√≥n (left, center, right)
    pad=20,               # Espacio entre el t√≠tulo y el gr√°fico (muy importante para la claridad)
    fontname='DejaVu Sans'      # Tipo de letra limpio
)
plt.axis("off")
plt.show()

"""Se sigue registrando el terminal 7453 como la menos usada. Tambi√©n se registra el terminal 7689. En el caso de duraci√≥n (s) vs Num.LLamadas se detectaba como registro de poco uso elterminal 7689 pero no tan anomalo como  7453. Este es una de las grandes desventajas  con respecto a pyod. Este m√©todo da la misma importancia a todas las variables. Puede ser que el terminal 7689 sea my poco usado pero no tanto como otros terminales. Tambi√©n ha habido terminales como el 3138 que no ha sido usado pero que ha recibido muchas llamadas. Es por tanto que estos tipos de algoritmos no son del todo fiables. Hay que buscar algoritmos de machine learning que den menos importancia a la variable Num.llamadas con respecto a la variabl Duraci√≥n (s).

## PYOD, Algoritmos de Machine Learning para detenci√≥n de anomalias.

### Preprocesamiento y normalizaci√≥n de datos para entrenamiento de modelos

El objetivo prinicpal para implementar estos modelos es detectar terminales de poco uso en comparaci√≥n con lo esperado. Vimos en nuestro analisis de datos como outlires de gran tama√±o distorsionaban de manera abrupta las representaciones visuales de distribuciones univariadas (boxplots e histogramas) como bivariadas( diagramas de dispersi√≥n). Adem√°s por lo qwue coment√© en la seccci√≥n de analisis bivariado, la subsecci√≥n num√©ricas vs num√©ricas, hab√≠a outliers de gran tama√±o que alteraban esageradamente los valores proporcionados por nuestra matriz de correlaciones.  Por otro lado tambi√©n vimos como ciertos outliers en ciertas variable scomo numero de llamadas y duraci√≥n (s) alargaban las colas de nuestras variables e influ√≠an en la  varianza de las mismas  (violinplots). En general tambi√©n se pod√≠a ver que el comportamiento de nuestra variable se ve√≠a afectado por los outliers de mayor tama√±o8 (gr√°fica de Pareto). Todo esto me lleva a la conclusi√≥n de que las mejores opciones ser√≠an eliminar o whinsorizar los outliers. Vemos en nuestro boxplot que las variables que teni√°n outliers por arriba eran entrantes numero de llamadas, entrantes duraci√≥n (s), internas numero de llamadas, internas duraci√≥n (s), entrantes, salientes, internas salientes, duraci√≥n(s) y n√∫mero de llamadas.

No tengo en cuenta coste debido a que al final he decidido eliminara de mi dataset para detenci√≥n de anomalias. Considero que puede distorsionar mis algoritmos de machine learnign sobretodo los basados en distancias como KNN, ya que coste esta correlacionada casi linealmnet con duraci√≥n (S) y duraci√≥n salientes en (s). Adem√°s costes elevados no conduc√≠a normalemnte a duraciones elevadas de terminales, debido a que esos trabajadores seguramente realizaran llamadas internas. Por otro lado no nos ayudar√≠a a determinar terminales de poco uso ya que coste esta relacionada en el sentido creciente con duraci√≥n de salidas. Ademas por la gr√°fica de contorno, se observan elevados costes en al region donde hay linealidad con duraci√≥n y n√∫mero de llamadas. esto har√° que si usaramos la variable coste en  nuestro problema, los algoritmos de machine learning se centraran mas en costes inusualemnte elevcados relacionados con bastante uso en duraci√≥n de salientes y m√≥viles, y no dar√° tanta importancia a los pocos usos.  

Tambi√©n voy a eliminar de mi data set las columnas que indicaban la duraci√≥n de las llamadas en horas, minutos seguundso y d√≠as. Como ya tenemos variables que indican la duraci√≥n de esas llamadas solo en segundos no son necesarias. Adem√°s no se podr√≠an implementar estos modelos con estes tipos de variables en mi dataset.

Eliminar√© para entrenar mis modelos las columnas entrantes y salientes ya que tanto los histogramas como violines y diagramas de cajas incluso separados por mes y a√±o muestran que hay una gran acumulaci√≥n de 0's, que no hay gran variablididad, que las colas son muy cortas en comparaci√≥n con otras variables y que por tanto no hay mucho rango de valores, es decir, que la mayor√≠a de terminales no han realizado llamadas de estos tipos espec√≠ficos y si las han realizado lo mas comun es que no superen las 5 llamadas totales al mes. Estas variables por consiguiente no proporcionar√°n mucha informaci√≥n a cerca del uso de terminales y si las utilizaramos seguramente perjudicar√≠an los modelos de machine learning para detenci√≥n de anomalias.
"""

dtt=datos_num2.select_dtypes(include='timedelta64[ns]')
dtt.columns

datos_num2=datos_num2.drop(columns=dtt.columns)
datos_num2=datos_num2.drop(columns=['Coste'])
datos_num2.head(5)

datos_num2.columns

"""No he eliminado de momento las columnas categ√≥ricas ya que quiero comprobar si necesariamemte los outliers de gran tama√±o coinciden entre las variables. Vamos a verlo detectando outliers con el test de Tukey."""

dt=datos_num_log.drop('Coste',axis=1)
dt=dt.drop(columns=dtt.columns)
columnas = dt.select_dtypes(include=['int', 'float']).columns

for i in columnas:
    Q1 = dt[i].quantile(0.25)
    Q3 = dt[i].quantile(0.75)
    IQ = Q3 - Q1

    # Filtrar outliers
    outliers = dt[dt[i] > Q3 + 1.5 * IQ]

    # Crear tabla con las columnas que quieres mostrar
    tabla = outliers[['Extensi√≥n', 'a√±o', 'mes']]

    # T√≠tulo
    print(f"\n=== Tabla de OUTLIERS para la variable '{i}' ===")

    # Mostrar tabla en horizontal
    display(tabla.T)

"""Se puede apreciar que no todos los registros considerados como anomalos en el sentido de mucho uso no coinciden entre las variables. NO ser√≠a recomendado eliminar aqueyos registros  ya que puede ocurrir la existencia de terminales que hayan realizado o recibido muchas llamadas pero que haya estado poco tiempo al telefono. Eso ser√≠a un caso de poco uso anomalo. Es por tanto que la mejor opcio√≥n es que para variables que contienen los outliers, sustituirlos por los valores del bigote de nuestro boxplot. Con eso no perdemos esos casos espec√≠ficos y reducimos el efecto que ten√≠an esos grandes outliers en la distorsi√≥n de nuestros algoritmos."""

def winsonorizar(dt1,i):
  Q1 = dt1[i].quantile(0.25)
  Q3 = dt1[i].quantile(0.75)
  IQ = Q3 - Q1
  maximo=Q3+1.5*IQ
  x=dt1[i].clip(upper=maximo)
  return x

for i in columnas:
   datos_num2[i]=winsonorizar(datos_num2,i)
datos_num2.describe()

"""Ahora nos falta escalar los datos. El escaldo o normalizaci√≥n de variables es esencialmente √∫til ya que normalmente hay varias variables que no est√° en la misma escala y al aplicar algoritmos basados en distancias, algunas variables tendr√°n mucho mas peso que otras y por tanto ser√°n las que tomen las decisiones. En nuestro caso concreto las variables asociadas a duraci√≥n de  las llamadas predominar√°n sobre las de n√∫mero de llamadas. Pero el problema para nuestro objtevio es que puede haber terminales que hayan realizado muy pocas llamadas pero hayan estado bastante tiempo al telefono ya sea por una necesidad urgente o simplemente por realizar otras actividades fuera de sus objetivos laborales. De todas formas nos interesar√≠a detectar esos terminales como anomales de poco uso pero si no escalaramos los datos, no se podr√≠an detectar dichos anomales y los algoritmos detectar√≠an anomales los terminales que en cada llamada estuvieron como mucho 10 segundos pero que dicho terminal realizo mas de 100 llamadas. Eso ser√≠a exactamente un caso donde no se considerar√≠a segun el contexto de nuestros datos como anomalo pero si lo conisderar√≠a estos algoritmos sin un previo escaldo de variables.

Voy a dar una descripci√≥n de los tipos de escalado que hay y a partir de ah√≠ deducir cual es el m√°s recomendado.

**MinMax scaler**

Esta t√©cnia de escalamiento de datos consiste en cambiar el rango total de valores al del rango $[0,1]$ mediante la siguiente formula:
$$x_c=\frac{x-x_{min}}{x_{max}-x_{min}}$$
donde $x_c$ son los datos escalados, $x$ los datos originales y $x_{max},x_{min}$ los m√°ximos y m√≠nimos repscetivamente de los datos orginales.

Este escalamiento es  muy util cuando quieres poner a todos los datos en la misma escala y darlos la misma importancia. Sin embargo tambi√©n tiene dos inconvenietes que van ligados de la mano.
1. No funciona bien cuando hay variables con elevado sesgo.
El sesgo se entiende como la asmietr√≠a que hay en nuestars variables y se puede interpretar como que tan alejado esta de la forma de distribuci√≥n de una normal gausiana.
El escalada minMax comprime la distribucion de los datos a un rango m√°s peque√±o de valores y asi desaprobechamos todo el rango de valores disponibles al realizar escalamiento.  En nuestros histogramas se aprecian variables como m¬¥viles, nacionales e incluso duraci√≥n (s) que pose√©n un sesgo significativo. Este escalameinto de datos har√≠a que ese sesgo se redujera reduciendo las colas para varables importantes como duraci√≥n (s).
Por ejemplo podemos visualizar los histogramas de la varible Duraci√≥n(s) normal (la lognormalizada da la misma informaci√≥n pero de manera mas visual sin descartar los efectos de los outliers pero tampoco sin dejarse influir por estos) y despu√©s aplicando el escalado minmax

"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()
sns.histplot(datos_num2["Duraci√≥n (s)"])

"""En este caso como no aplique la funci√≥n logaritmo, la mayor√≠a de valores se concentran en torno a 0, nos da a enteneder este histograna que sigue una distribuci√≥n exponencial pero no es as√≠ ya que se ve influenciadad por terminales de excesibo uso. Vemos que de todas formas al quitar outliers se aprecian termianles de mucho y poco uso. Veamos que ocurre si aplicasemos el escalado minmax

"""

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x=scaler.fit_transform(datos_num2[['Duraci√≥n (s)']])
sns.histplot(x)

"""Vemos que hay una compresi√≥n de datos significativa asi que desparovechamos todo el rango de valores disponibles. Este defecto no es muy influyente para nuestro objtevio ya que se mantiene el sesgo que es lo importante a la hora de implementar m√©todos de detenci√≥n de anomalias. El porblema es el siguiente

2. Sensibilidad a outliers.

Este m√©todo de escalameintod de datos es sensible a outliers. Como MInMax Scaler se basa en el maximo y m√≠nimo de nuestros datos, pueden ser estos prefectamente nuestros outliers. Grafiquemos por ejemplo Duraci√≥n de llamadas sin haber aplicado winsonorizaci√≥n
"""

sns.histplot(datos[['Duraci√≥n (s)']])

"""Vemos que los datos est√°n muy comprimidos en torno al cero debido a la presencia de terminales que usaron bastante el telefono. AL hacer escalado comprimiremos la distribuci√≥n de los datos"""

x=scaler.fit_transform(datos[['Duraci√≥n (s)']])
sns.histplot(x)

"""Esto hace que en nuestro caso concreto sea bastante dificil detectar terminales de muy poco uso ya que este escalamiento por diferentes escalas comprime el rango de valores tanto para las de duraci√≥n como n√∫mero de llamadas.

Pasemos al siguiente escalamiento

**Standar Scaler**

Se escalan los datos esta vez a partir de la media y la varianza de los mismos
$$x_c=\frac{x-\mu}{œÉ}$$
donde $x_c$ son los datos escalado, $x$ los datos originales y $\mu,œÉ$ la media y varianza respecvtivamente de nuestros datos originales.

Este m√©todo supone que la distribuci√≥n de nuestros datos siguen una distribuci√≥n normal gaussiana. Asi que
1. Si la distribuci√≥n tiene un sesgo, se podr√≠a utilizar este escalado pero no es lo idel ya que no seguir√≠a una dsitribuci√≥n gaussiana o con nulo sesgo. LO que pasar√° al aplicar este escalado es que se reducir√°n los rangos pero la distancia entre valores relativa no se ver√° tan perjudicada como en el caso de el escalado minmax y mas en nuestros datos, ya que el m√≠nimo normalmente es 0 y el m√°ximo se mantiene demasiado alejado de la mayor√≠a de nuestros datos.

2. Este m√©todo es sensible a outliers.

Se sabe de antemano que la media y la desviaci√≥n t√≠pica muestrales ( aporximantes de la media y varianza real) calculados en python son muy sensibles a outliers. Por tanto no es muy recomendable. Veamoslo con un ejemplo en python con nuestros datos
"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x=datos[['Num.LLamadas']]
sns.histplot(x)

"""En este caso por la curva de pareto y los violinplots de la secci√≥n de analisis bivariado y univariado respectivamente, hab√≠a outliers de gran tama√±o que tenian mucha influencia sobre nuestra varibale Numero de llamadas. Esto ocasiona un gran cambio en la media y la desviaci√≥n tipica y por la presencia de estos outliers, al hacer estandarizaci√≥n de nuestros datos, la mayor√≠a de ellos se han comprimido. Si aplicaramos este escalado a nuestros datos, los modelos no podr√≠an distinguir entre terminales de poco uso y terminales de uso 'normal'. Por tanto este escalado no es recomendable

**Transformaci√≥n log**

Esta transofrmaci√≥n me ha resultado de gran ayuda para visualizar con mayor detalle tanto los histogramas como boxplots y violinplots. Adem√°s hice unos diagramas de dispersi√≥n para mostrar la variablidida monotona entre variables y la acumulaci√≥n de terminales en un valor. Basicamente log actua como una lupa. Es √∫til para apreciar con detalle el comportamiento de nuestros valores cuando la escala es muy grande y hay terminales de mucho uso como en las llamadas. Sin emabrgo no ser√≠a √∫til para detenci√≥n de anomalias ya que aplasta los valores grandes pero tambi√©n los peque√±os. Is aplicasemos la transformaci√≥n logaritmica a nuestros datos, los modelos de machine laernig basados en distancias entender√°n que muchos terminales son de nulo uso auqnue ahay bastantes que sean de uso normal. No comprime tanto los datos como los otros caso pero la compresi√≥n es significativa. Adem√°s auqnue reduzac el efecto de outliers, es sensible a ellos y esa reducci√≥n tambi√©n puede afectar nuestro modelo para detenci√≥n de anomalias.

**RobustScaler**

A diferencia de los tres anteriores este escalado no reduce el efecto de outliers ni tampoc se ve influenciado por ellos. Es por tanto la mejor opci√≥n para escalara nuestras variables y asi implementar los algoritmos de machine learnign para detenci√≥n de anomalias, sobretodo los basados en distancias. Se consstruye de la siguiente forma
$$x_c=\frac{x-x_{mediana}}{IQR}$$
donde $x_c$ son los datos escalado, $x$ los datos originales, $x_{mediana}$ la mediana de nuestros datos originales y IQR el rango intercuart√≠lico. Estos dos valores a diferencia de la media y desviaci√≥n t√≠pica no se ven influenciados por los outliers de gran tama√±o ya que:
1.1.La mediana es el valor que divide los datos en dos mitades iguales
1.2.El cuartil 1 es el valor por debajo del cual se encuentra el 25% de los datos
1.3.El cuartil 3 es el valor por debajo del cual se encuentra el 75% de los datos
1.4.El rango intercuartilico (IQR) es la diferencia entre el cuartil 3 y el cuartil 1

Tambi√©n hay otros escalados de datos sensibles a outliers como QuantileTransformer. Pero a diferencia de RobustScaler, QuantileTransformer tambi√©n comprimir√° autom√°ticamente cualquier valor at√≠pico ajust√°ndolo a los l√≠mites de rango definidos a priori (0 y 1), incluido los valores de menor valor. Es por tanto que este escalado no es para nada recomendable ya que nuestro objetivo es detectar anomalias de poco uso.

Implementemos ahora el escalado Robusto
"""

datos_num2.columns

from sklearn.preprocessing import RobustScaler
scaler=RobustScaler()
dt=datos_num2.select_dtypes(include=['int','float'])
columnas=dt.columns
x=scaler.fit_transform(dt)
dtML=pd.DataFrame(x,columns=columnas)
dtML.head(5)

"""Como hemos aplicado winsonorzaici√≥n, hemos reducido el efecto que tenian los outliers sobre las correlaciones entre pares de variables. Podr√≠amos ver la matrzi de correlaciones. Cabe aclarar que dicha matriz no se ver√° alterada por el escalado robusto ya que la correlaci√≥n por lo que vimos antes es invariante frente a transformaciones lineales"""

import seaborn as sns
import matplotlib.pyplot as plt

df1 = dtML.corr()

fig, axes = plt.subplots(1, 1, figsize=(25, 10))
sns.set_theme(style="white")

# --- Heatmap 1 ---
sns.heatmap(df1,
            ax=axes,  # <-- sin [0]
            cmap="coolwarm",
            annot=True,
            square=True,
            linewidths=0.3,
            cbar_kws={"shrink": 0.8}
           )

axes.set_title("Correlaciones (Original)", fontsize=18, pad=20)
axes.tick_params(axis='x', labelrotation=90, labelsize=10)
axes.tick_params(axis='y', labelsize=10)
plt.tight_layout()
plt.show()

"""Esta matriz de correlaciones es m√°s fiable que la oiginal debido a que al haber quitado los outiliers no hay una gran alteraci√≥n en las correlaciones. Vemos que casi todas las variables no mantienen relaciones lineales entre si o estas son muy pobres. La unica situaci√≥n donde hay una correlaci√≥n fuerte es entre internas duraci√≥n (s) e internas n√∫mero de llamadas. SIn ebargo no es suficiente para descartar una de las dos variables para nuestros modelos.

Empecemos ahora a implementar tecnicas de machine learning para detenci√≥n de anomalias. Primero me basar√© en m√©todos de dsitancias y luego en m√©todos de clasificaci√≥n, pasando por m√©todos influencias por angulos y m√©todos construidos por redes neuronales. Todos ellos integrados en la libreria de pyod.

## Detenci√≥n de Anomalias por pyod

Empecemnos primero visualizando tanto el diagrama de dispersi√≥n como el hexbin de las variables Num.LLamadas y Duraci√≥n (s) de nuestros datos a los que hemos winsonorizado nuestros outliers de gran tama√±o y les aplicamos RobustScaler para tenerlos a la misma escala y reducir el efecto de outliers sin necesidad de reducir la varibaidad de los datos.
"""

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

# Scatter plot
axes[0].scatter(dtML['Num.LLamadas'], dtML['Duraci√≥n (s)'])
axes[0].set_title('Diagrama de dispersi√≥n', fontsize=13)
axes[0].set_xlabel("Num.LLamadas")
axes[0].set_ylabel("Duraci√≥n (s)")
axes[0].grid(alpha=0.3)

# Hexbin plot con colorbar
hb = axes[1].hexbin(
    dtML['Num.LLamadas'],
    dtML['Duraci√≥n (s)'],
    gridsize=100,
    cmap="plasma",
    mincnt=1,
    linewidths=0.5
)
axes[1].set_title('Diagrama de hexbin', fontsize=13)
axes[1].set_xlabel("Num.LLamadas")
axes[1].set_ylabel("Duraci√≥n (s)")
axes[1].grid(alpha=0.3)

# Agregar barra de color
fig.colorbar(hb, ax=axes[1], label='Frecuencia')

plt.tight_layout()
plt.show()

"""Vemos en el diagrama de dispersi√≥n una tendencia lineal tal y como se aprecia en nuestra matriz de correlaciones con estos nuevos datos pero con la diferencia que quizas se vea levemente afectada por la winsonorizaci√≥n pero era necesario debido a los outliers de gran tama√±o que teniamos en nuestro dataset, y que estoys ejercian gran influencia en nuestras dos variables por lo observado tanto en las gr√°ficas de pareto (un aplanamiento bastante r√°pido de las curvas) como en los violinplots (colas muy alargadas a la hora de graficar la funci√≥n de densidad. Dichas colas alteran la forma de nuestars distribuciones).

Aunque se aprecien bien en el diagram de dispersi√≥n, he elevaorado a la izquierda un hexbinplot para visualizar terminales de poco uso por posibles superposiciones. Lo primero que vemos es que hay una gran acumulaci√≥n de 0's. Esto posiblemente se deba a la gran cantidad de terminales de pocos usos para un mes y a√±o espec√≠fico., que fueron identificados en las primeras tablas de la anterior seccci√≥n en la parte de wordmap. Esto significa que los algoritmos de machine learning que implementaremos para este conjunto de datos no detectara los de nulo uso tanto en numero de llamadas como duraci√≥n de las mismas. Pero posiblemente identifique los terminales que hayan hecho un n√∫mero determinado de llamads pero con poco tiempo puesto al telefono y al reves, terminales con un tiempo considerable puesto al telefono pero habiendo realizado muy pocas llamadas.

Aunque en los algoritmos de machine learning utilize solamente el gr√°fico de dispersi√≥n de estas dos variables, ser√°n utilizadas tambien las otras variables ya que contienen informaci√≥n √∫til de por que dichos terminales han sido catalogados como de poco uso.

Podemos hacer una matriz de dispersi√≥n de nuestro dataset para ver que relaciones se mantienen. AUnque la matriz de correlaciones es muy √∫til en este caso, me parece imprescinidble este gr√°fico para ver la variablidad ya que hay variables que en un principio eran discretas y puede ocurrir que dichas variables discretas tomen pocos valores
"""

import seaborn as sns
import matplotlib.pyplot as plt

cols = columnas  # columnas que quieres mostrar

# Crear PairGrid
g = sns.PairGrid(dtML[cols])

# Scatter en la diagonal inferior
for i, j in zip(*np.tril_indices_from(g.axes, -1)):  # indices de la diagonal inferior
    g.axes[i, j].scatter(dtML[cols[j]], dtML[cols[i]], alpha=0.6)

# Ocultar la mitad superior y la diagonal principal
for i in range(len(cols)):
    for j in range(len(cols)):
        if i <= j:
            g.axes[i, j].set_visible(False)

# Poner nombre a los ejes
for i in range(len(cols)):
    for j in range(len(cols)):
        if i > j:
            g.axes[i, j].set_xlabel(cols[j])
            g.axes[i, j].set_ylabel(cols[i])
plt.suptitle("Matriz de dispersi√≥n (diagonal inferior)", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

"""Como vemos la variable Internas salientes mantiene poca variablidad ya que se pueden ditingir pocos grupos que speraen los terminales que tenemos segun las llmadas internas que hayan realizado. Tambi√©n se pod√≠a aoreciar en nuestroas histogramas e incluso boxplots que en dicha variable se apeciaba que los datos se aproximaban al  siguiendo una distribuci√≥n exponencial pero con una cola muy corta. Al principio me dio la impresi√≥n de que Internas salientes era una variable con poco variabilidad pero no la descarte ya que hab√≠a otras como entrantes y salientes que tenian menos variablidad y me parecio en un principio que no era suficente como para descartarla. Pero viendo ahora nuestro hsitograma, podemos concluir que es necesario descartar dicha variable de nuestro dataset ya que no proporciona informaci√≥n util para detenci√≥n de anomalias de poco uso, al haber poca variablidad debido al ser Internas salientes una variable discreta que toma pocos valores distintos. Se puede pensar lo mismo con Nacionales, solo que la diferencia es que Nacionales tiene bastantes mas valores distintos. Ire experimentando con la variable nacionales e ir viendo que tan bien rienden los modelos con y sin ella.

Aunque se aprecien varias variables discretas, estas pueden proporcionar informaci√≥n √∫til a la hora de detectar terminales de poco uso. Lo que haremos es implementar los modelos con y sin reducci√≥n de dimensionalidad y ver en que casos se ha apreciado mejor rendimiento basandonos en la coherencia de nuestros datos.

## ISOLATION FOREST
"""

import pyod
from pyod.models.iforest import IForest
from pyod.models.pca import PCA

modelo1=IForest()
columnasnum=datos_num_log.select_dtypes(include=['int','float']).columns
dt=datos_num_log[columnasnum]
n=len(columnasnum)
X= dt[columnasnum[0]].values.reshape(-1,1)
for i in range(1,n):
  X1=dt[columnasnum[i]].values.reshape(-1,1)
  X=np.concatenate((X,X1),axis=1)
modelo1.fit(X)
scores_pred = modelo1.decision_function(X)*-1
dt['Puntuaci√≥nAnomalia']=scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier']=y_pred

fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()
scatter=axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_ylabel('Duraci√≥n (s)')
axes[1].set_xlabel('Num.LLamadas')
cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(18, 8))

# ================= SUBPLOT 1: SCORE CONTINUO =================
ax1 = fig.add_subplot(1, 2, 1, projection='3d')

sc = ax1.scatter(
    dt["Num.LLamadas"],
    dt["Duraci√≥n (s)"],
    dt["Coste"],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    s=50,
    alpha=0.8,
    edgecolor='k'
)

ax1.set_xlabel('Num. Llamadas', fontsize=12)
ax1.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax1.set_zlabel('Coste', fontsize=12)
ax1.set_title('3D Scatter ‚Äì Puntuaci√≥n de Anomal√≠a', fontsize=14)

# barra de color SOLO para este subplot
cbar = fig.colorbar(sc, ax=ax1, pad=0.1, shrink=0.6)
cbar.set_label('Puntuaci√≥n Anomal√≠a', fontsize=12)


# ================= SUBPLOT 2: CATEG√ìRICO =================
ax2 = fig.add_subplot(1, 2, 2, projection='3d')

# separar clases
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

ax2.scatter(
    dt.loc[inliers, "Num.LLamadas"],
    dt.loc[inliers, "Duraci√≥n (s)"],
    dt.loc[inliers, "Coste"],
    c='blue',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='No Outlier'
)

ax2.scatter(
    dt.loc[outliers, "Num.LLamadas"],
    dt.loc[outliers, "Duraci√≥n (s)"],
    dt.loc[outliers, "Coste"],
    c='red',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='Outlier'
)

ax2.set_xlabel('Num. Llamadas', fontsize=12)
ax2.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax2.set_zlabel('Coste', fontsize=12)
ax2.set_title('3D Scatter ‚Äì Outlier / No Outlier', fontsize=14)

ax2.legend()


# ================= AJUSTES FINALES =================
plt.tight_layout()
plt.ion()   # modo interactivo
plt.show()

from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

modelo1 = IForest()

# Cambiado datos_num_log por dtML
columnasnum = dtML.select_dtypes(include=['int', 'float']).columns
dt = dtML[columnasnum]
n = len(columnasnum)

X = dt[columnasnum[0]].values.reshape(-1, 1)
for i in range(1, n):
    X1 = dt[columnasnum[i]].values.reshape(-1, 1)
    X = np.concatenate((X, X1), axis=1)

modelo1.fit(X)
scores_pred = modelo1.decision_function(X) * -1
dt['Puntuaci√≥nAnomalia'] = scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier'] = y_pred

# =================== GRAFICOS 2D ===================
fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()

scatter = axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_xlabel('Num. Llamadas')
axes[1].set_ylabel('Duraci√≥n (s)')

cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')

plt.tight_layout()
plt.show()

"""## PCA"""

modelo2=PCA()
columnasnum=datos_num_log.select_dtypes(include=['int','float']).columns
dt=datos_num_log[columnasnum]
n=len(columnasnum)
X= dt[columnasnum[0]].values.reshape(-1,1)
for i in range(1,n):
  X1=dt[columnasnum[i]].values.reshape(-1,1)
  X=np.concatenate((X,X1),axis=1)
modelo2.fit(X)
scores_pred = modelo2.decision_function(X)*-1
dt['Puntuaci√≥nAnomalia']=scores_pred
y_pred = modelo2.predict(X)
dt['Outlier/NoOutlier']=y_pred


fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()
scatter=axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_ylabel('Duraci√≥n (s)')
axes[1].set_xlabel('Num.LLamadas')
cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')
plt.tight_layout()
plt.show()


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(18, 8))

# ================= SUBPLOT 1: SCORE CONTINUO =================
ax1 = fig.add_subplot(1, 2, 1, projection='3d')

sc = ax1.scatter(
    dt["Num.LLamadas"],
    dt["Duraci√≥n (s)"],
    dt["Coste"],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    s=50,
    alpha=0.8,
    edgecolor='k'
)

ax1.set_xlabel('Num. Llamadas', fontsize=12)
ax1.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax1.set_zlabel('Coste', fontsize=12)
ax1.set_title('3D Scatter ‚Äì Puntuaci√≥n de Anomal√≠a', fontsize=14)

# barra de color SOLO para este subplot
cbar = fig.colorbar(sc, ax=ax1, pad=0.1, shrink=0.6)
cbar.set_label('Puntuaci√≥n Anomal√≠a', fontsize=12)


# ================= SUBPLOT 2: CATEG√ìRICO =================
ax2 = fig.add_subplot(1, 2, 2, projection='3d')

# separar clases
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

ax2.scatter(
    dt.loc[inliers, "Num.LLamadas"],
    dt.loc[inliers, "Duraci√≥n (s)"],
    dt.loc[inliers, "Coste"],
    c='blue',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='No Outlier'
)

ax2.scatter(
    dt.loc[outliers, "Num.LLamadas"],
    dt.loc[outliers, "Duraci√≥n (s)"],
    dt.loc[outliers, "Coste"],
    c='red',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='Outlier'
)

ax2.set_xlabel('Num. Llamadas', fontsize=12)
ax2.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax2.set_zlabel('Coste', fontsize=12)
ax2.set_title('3D Scatter ‚Äì Outlier / No Outlier', fontsize=14)

ax2.legend()


# ================= AJUSTES FINALES =================
plt.tight_layout()
plt.ion()   # modo interactivo
plt.show()

from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

modelo2 = PCA()

# Cambiado datos_num_log por dtML
columnasnum = dtML.select_dtypes(include=['int', 'float']).columns
dt = dtML[columnasnum]
n = len(columnasnum)

X = dt[columnasnum[0]].values.reshape(-1, 1)
for i in range(1, n):
    X1 = dt[columnasnum[i]].values.reshape(-1, 1)
    X = np.concatenate((X, X1), axis=1)

modelo2.fit(X)
scores_pred = modelo2.decision_function(X) * -1
dt['Puntuaci√≥nAnomalia'] = scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier'] = y_pred

# =================== GRAFICOS 2D ===================
fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()

scatter = axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_xlabel('Num. Llamadas')
axes[1].set_ylabel('Duraci√≥n (s)')

cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')

plt.tight_layout()
plt.show()

"""# Datos acumulados"""

datosacumulados.describe()

datosacumulados_log.describe()

datosacumulados.info()

plt.figure(figsize=(3,3))
axes = datosacumulados.hist(
    figsize=(12, 10),
    bins=20,
    color='tomato',
    edgecolor='black'
)


for ax in axes.flatten():
    ax.set_title(ax.get_title(), fontsize=14, fontweight='bold')
    ax.set_xlabel(ax.get_xlabel(), fontsize=12)
    ax.set_ylabel("Frecuencia", fontsize=12)


plt.suptitle("Distribuci√≥n de Variables Num√©ricas", fontsize=18, y=1.02)

plt.tight_layout()
plt.show()

datosacumulados_log=datosacumulados.copy()
c=datosacumulados_log.select_dtypes(include=['int','float']).columns
datosacumulados_log[c]=datosacumulados_log[c].apply(lambda x:np.log(x+1))
plt.figure(figsize=(3,3))
axes = datosacumulados_log.hist(
    figsize=(12, 10),
    bins=20,
    color='tomato',
    edgecolor='black'
)


for ax in axes.flatten():
    ax.set_title(ax.get_title(), fontsize=14, fontweight='bold')
    ax.set_xlabel(ax.get_xlabel(), fontsize=12)
    ax.set_ylabel("Frecuencia", fontsize=12)


plt.suptitle("Distribuci√≥n de Variables Num√©ricas", fontsize=18, y=1.02)

plt.tight_layout()
plt.show()

plt.figure(figsize=(10,10))
ax = datosacumulados.boxplot(figsize=(12, 8), patch_artist=True)


for box in ax.artists:
    box.set(facecolor='lightblue', edgecolor='black', alpha=0.7)


for line in ax.lines:
    line.set(color='black', linewidth=1)



plt.title("Distribuci√≥n de Variables Num√©ricas (Boxplots)", fontsize=18, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.ylabel("Valor", fontsize=12)

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,10))
ax = datosacumulados_log.boxplot(figsize=(12, 8), patch_artist=True)


for box in ax.artists:
    box.set(facecolor='lightblue', edgecolor='black', alpha=0.7)


for line in ax.lines:
    line.set(color='black', linewidth=1)



plt.title("Distribuci√≥n de Variables Num√©ricas (Boxplots)", fontsize=18, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.ylabel("Valor", fontsize=12)

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import math

# Usamos datos_acumulados en lugar de datos_num2
dt2 = datosacumulados.select_dtypes(include=['int', 'float'])
columnas = dt2.columns
n = len(columnas)

# -------------------
# Primer bloque
cols = 5
rows = 5
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
sns.set_theme(style="white")
k = 0

for i in range(2):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# -------------------
# Segundo bloque
cols = 5
rows = 4
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
k = 0

for i in range(2,4):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# -------------------
# Tercer bloque
cols = 5
rows = 5
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
k = 0

for i in range(4,7):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# -------------------
# Cuarto bloque
cols = 4
rows = 4
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
k = 0

for i in range(7,n):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import math

# Usamos datos_acumulados en lugar de datos_num2
dt2 = datosacumulados_log.select_dtypes(include=['int', 'float'])
columnas = dt2.columns
n = len(columnas)

# -------------------
# Primer bloque
cols = 5
rows = 5
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
sns.set_theme(style="white")
k = 0

for i in range(2):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# -------------------
# Segundo bloque
cols = 5
rows = 4
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
k = 0

for i in range(2,4):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# -------------------
# Tercer bloque
cols = 5
rows = 5
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
k = 0

for i in range(4,7):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

# -------------------
# Cuarto bloque
cols = 4
rows = 4
fig, axes = plt.subplots(rows, cols, figsize=(15, 15))
axes = axes.flatten()
k = 0

for i in range(7,n):
    for j in range(i+1, n):
        ax = axes[k]
        k += 1
        ax.scatter(
            dt2[columnas[j]],
            dt2[columnas[i]],
            s=30,
            alpha=0.4,
            color="#4C72B0",
            edgecolor="none"
        )
        ax.set_xlabel(columnas[j])
        ax.set_ylabel(columnas[i])
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

for ax in axes[k:]:
    ax.axis("off")

plt.tight_layout()
plt.suptitle("Diagramas de Dispersi√≥n", fontsize=28, fontweight="bold", y=1.02)
plt.show()

"""## ISOLATION FOREST"""

import pyod
from pyod.models.iforest import IForest
from pyod.models.pca import PCA
modelo1=IForest()
columnasnum=datosacumulados_log.select_dtypes(include=['int','float']).columns
dt=datosacumulados_log[columnasnum]
n=len(columnasnum)
X= dt[columnasnum[0]].values.reshape(-1,1)
for i in range(1,n):
  X1=dt[columnasnum[i]].values.reshape(-1,1)
  X=np.concatenate((X,X1),axis=1)
modelo1.fit(X)
scores_pred = modelo1.decision_function(X)*-1
dt['Puntuaci√≥nAnomalia']=scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier']=y_pred
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(18, 8))

fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()
scatter=axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_ylabel('Duraci√≥n (s)')
axes[1].set_xlabel('Num.LLamadas')
cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')
plt.tight_layout()
plt.show()

import pyod
from pyod.models.iforest import IForest
from pyod.models.pca import PCA
modelo1=IForest()
columnasnum=datosacumulados_log.select_dtypes(include=['int','float']).columns
dt=datosacumulados_log[columnasnum]
n=len(columnasnum)
X= dt[columnasnum[0]].values.reshape(-1,1)
for i in range(1,n):
  X1=dt[columnasnum[i]].values.reshape(-1,1)
  X=np.concatenate((X,X1),axis=1)
modelo1.fit(X)
scores_pred = modelo1.decision_function(X)*-1
dt['Puntuaci√≥nAnomalia']=scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier']=y_pred
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(18, 8))

# ================= SUBPLOT 1: SCORE CONTINUO =================
ax1 = fig.add_subplot(1, 2, 1, projection='3d')

sc = ax1.scatter(
    dt["Num.LLamadas"],
    dt["Duraci√≥n (s)"],
    dt["Coste"],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    s=50,
    alpha=0.8,
    edgecolor='k'
)

ax1.set_xlabel('Num. Llamadas', fontsize=12)
ax1.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax1.set_zlabel('Coste', fontsize=12)
ax1.set_title('3D Scatter ‚Äì Puntuaci√≥n de Anomal√≠a', fontsize=14)

# barra de color SOLO para este subplot
cbar = fig.colorbar(sc, ax=ax1, pad=0.1, shrink=0.6)
cbar.set_label('Puntuaci√≥n Anomal√≠a', fontsize=12)

# ================= SUBPLOT 2: CATEG√ìRICO =================
ax2 = fig.add_subplot(1, 2, 2, projection='3d')

# separar clases
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

ax2.scatter(
    dt.loc[inliers, "Num.LLamadas"],
    dt.loc[inliers, "Duraci√≥n (s)"],
    dt.loc[inliers, "Coste"],
    c='blue',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='No Outlier'
)

ax2.scatter(
    dt.loc[outliers, "Num.LLamadas"],
    dt.loc[outliers, "Duraci√≥n (s)"],
    dt.loc[outliers, "Coste"],
    c='red',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='Outlier'
)

ax2.set_xlabel('Num. Llamadas', fontsize=12)
ax2.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax2.set_zlabel('Coste', fontsize=12)
ax2.set_title('3D Scatter ‚Äì Outlier / No Outlier', fontsize=14)

ax2.legend()

# ================= AJUSTES FINALES =================
plt.tight_layout()
plt.ion()   # modo interactivo
plt.show()

def winsonorizar(dt1,i):
  Q1 = dt1[i].quantile(0.25)
  Q3 = dt1[i].quantile(0.75)
  IQ = Q3 - Q1
  maximo=Q3+1.5*IQ
  x=dt1[i].clip(upper=maximo)
  return x
datos_num22=datosacumulados.select_dtypes(include=['int','float'])
columnas=datos_num22.columns
for i in columnas:
   datos_num22[i]=winsonorizar(datos_num22,i)
datos_num22.describe()


from sklearn.preprocessing import RobustScaler
scaler=RobustScaler()
dt=datos_num22.select_dtypes(include=['int','float'])
columnas=dt.columns
x=scaler.fit_transform(dt)
dtML=pd.DataFrame(x,columns=columnas)
dtML.head(5)

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

# Scatter plot
axes[0].scatter(dtML['Num.LLamadas'], dtML['Duraci√≥n (s)'])
axes[0].set_title('Diagrama de dispersi√≥n', fontsize=13)
axes[0].set_xlabel("Num.LLamadas")
axes[0].set_ylabel("Duraci√≥n (s)")
axes[0].grid(alpha=0.3)

# Hexbin plot con colorbar
hb = axes[1].hexbin(
    dtML['Num.LLamadas'],
    dtML['Duraci√≥n (s)'],
    gridsize=100,
    cmap="plasma",
    mincnt=1,
    linewidths=0.5
)
axes[1].set_title('Diagrama de hexbin', fontsize=13)
axes[1].set_xlabel("Num.LLamadas")
axes[1].set_ylabel("Duraci√≥n (s)")
axes[1].grid(alpha=0.3)

# Agregar barra de color
fig.colorbar(hb, ax=axes[1], label='Frecuencia')

plt.tight_layout()
plt.show()

from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

modelo1 = IForest()

# Cambiado datos_num_log por dtML
columnasnum = dtML.select_dtypes(include=['int', 'float']).columns
dt = dtML[columnasnum]
n = len(columnasnum)

X = dt[columnasnum[0]].values.reshape(-1, 1)
for i in range(1, n):
    X1 = dt[columnasnum[i]].values.reshape(-1, 1)
    X = np.concatenate((X, X1), axis=1)

modelo1.fit(X)
scores_pred = modelo1.decision_function(X) * -1
dt['Puntuaci√≥nAnomalia'] = scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier'] = y_pred

# =================== GRAFICOS 2D ===================
fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()

scatter = axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_xlabel('Num. Llamadas')
axes[1].set_ylabel('Duraci√≥n (s)')

cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')

plt.tight_layout()
plt.show()

"""## PCA"""

import pyod
from pyod.models.iforest import IForest
from pyod.models.pca import PCA
modelo1=PCA()
columnasnum=datosacumulados_log.select_dtypes(include=['int','float']).columns
dt=datosacumulados_log[columnasnum]
n=len(columnasnum)
X= dt[columnasnum[0]].values.reshape(-1,1)
for i in range(1,n):
  X1=dt[columnasnum[i]].values.reshape(-1,1)
  X=np.concatenate((X,X1),axis=1)
modelo1.fit(X)
scores_pred = modelo1.decision_function(X)*-1
dt['Puntuaci√≥nAnomalia']=scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier']=y_pred
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(18, 8))

fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()
scatter=axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_ylabel('Duraci√≥n (s)')
axes[1].set_xlabel('Num.LLamadas')
cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')
plt.tight_layout()
plt.show()

import pyod
from pyod.models.iforest import IForest
from pyod.models.pca import PCA
modelo1=PCA()
columnasnum=datosacumulados_log.select_dtypes(include=['int','float']).columns
dt=datosacumulados_log[columnasnum]
n=len(columnasnum)
X= dt[columnasnum[0]].values.reshape(-1,1)
for i in range(1,n):
  X1=dt[columnasnum[i]].values.reshape(-1,1)
  X=np.concatenate((X,X1),axis=1)
modelo1.fit(X)
scores_pred = modelo1.decision_function(X)*-1
dt['Puntuaci√≥nAnomalia']=scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier']=y_pred
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(18, 8))

# ================= SUBPLOT 1: SCORE CONTINUO =================
ax1 = fig.add_subplot(1, 2, 1, projection='3d')

sc = ax1.scatter(
    dt["Num.LLamadas"],
    dt["Duraci√≥n (s)"],
    dt["Coste"],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    s=50,
    alpha=0.8,
    edgecolor='k'
)

ax1.set_xlabel('Num. Llamadas', fontsize=12)
ax1.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax1.set_zlabel('Coste', fontsize=12)
ax1.set_title('3D Scatter ‚Äì Puntuaci√≥n de Anomal√≠a', fontsize=14)

# barra de color SOLO para este subplot
cbar = fig.colorbar(sc, ax=ax1, pad=0.1, shrink=0.6)
cbar.set_label('Puntuaci√≥n Anomal√≠a', fontsize=12)

# ================= SUBPLOT 2: CATEG√ìRICO =================
ax2 = fig.add_subplot(1, 2, 2, projection='3d')

# separar clases
inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

ax2.scatter(
    dt.loc[inliers, "Num.LLamadas"],
    dt.loc[inliers, "Duraci√≥n (s)"],
    dt.loc[inliers, "Coste"],
    c='blue',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='No Outlier'
)

ax2.scatter(
    dt.loc[outliers, "Num.LLamadas"],
    dt.loc[outliers, "Duraci√≥n (s)"],
    dt.loc[outliers, "Coste"],
    c='red',
    s=50,
    alpha=0.8,
    edgecolor='k',
    label='Outlier'
)

ax2.set_xlabel('Num. Llamadas', fontsize=12)
ax2.set_ylabel('Duraci√≥n (s)', fontsize=12)
ax2.set_zlabel('Coste', fontsize=12)
ax2.set_title('3D Scatter ‚Äì Outlier / No Outlier', fontsize=14)

ax2.legend()

# ================= AJUSTES FINALES =================
plt.tight_layout()
plt.ion()   # modo interactivo
plt.show()

from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

modelo1 = PCA()

# Cambiado datos_num_log por dtML
columnasnum = dtML.select_dtypes(include=['int', 'float']).columns
dt = dtML[columnasnum]
n = len(columnasnum)

X = dt[columnasnum[0]].values.reshape(-1, 1)
for i in range(1, n):
    X1 = dt[columnasnum[i]].values.reshape(-1, 1)
    X = np.concatenate((X, X1), axis=1)

modelo1.fit(X)
scores_pred = modelo1.decision_function(X) * -1
dt['Puntuaci√≥nAnomalia'] = scores_pred
y_pred = modelo1.predict(X)
dt['Outlier/NoOutlier'] = y_pred

# =================== GRAFICOS 2D ===================
fig, axes = plt.subplots(1, 2, figsize=(18, 10))
plt.subplots_adjust(wspace=0.3, hspace=0.4)
axes = axes.flatten()

inliers = dt['Outlier/NoOutlier'] == 0
outliers = dt['Outlier/NoOutlier'] == 1

axes[0].scatter(
    dt.loc[inliers, 'Num.LLamadas'],
    dt.loc[inliers, 'Duraci√≥n (s)'],
    c='blue',
    edgecolor='k',
    label='No Outlier'
)

axes[0].scatter(
    dt.loc[outliers, 'Num.LLamadas'],
    dt.loc[outliers, 'Duraci√≥n (s)'],
    c='red',
    edgecolor='k',
    label='Outlier'
)

axes[0].set_title('Gr√°fico de dispersi√≥n Outlier / No Outlier')
axes[0].set_xlabel('Num. Llamadas')
axes[0].set_ylabel('Duraci√≥n (s)')
axes[0].legend()

scatter = axes[1].scatter(
    dt['Num.LLamadas'],
    dt['Duraci√≥n (s)'],
    c=dt['Puntuaci√≥nAnomalia'],
    cmap='viridis',
    edgecolor='k'
)
axes[1].set_title('Grafico de dispersi√≥n Puntuaci√≥n de Anomalias')
axes[1].set_xlabel('Num. Llamadas')
axes[1].set_ylabel('Duraci√≥n (s)')

cbar = fig.colorbar(scatter, ax=axes[1])
cbar.set_label('Score')

plt.tight_layout()
plt.show()

"""# TABLA TERMINALES FALTANTES"""

dt=datos[datos['a√±o']=='2024']
x=datos['Extensi√≥n'].unique()
L=list()
for i in x:
  dt1=dt[dt['Extensi√≥n']==i]
  if len(dt1)==12:
    L.append(i)

len(L)

dt=datos[datos['a√±o']=='2025']
x=dt['Extensi√≥n'].unique()
L1=list()
for i in x:
  dt1=dt[dt['Extensi√≥n']==i]
  if len(dt1)==7:
    L1.append(i)

len(L1)

dt=datos[datos['a√±o']=='2024']
x=datos['Extensi√≥n'].unique()
n = len(x)

tabla = {
    'Extensi√≥n': x,
    'enero': ['enero'] * n,
    'febrero': ['febrero'] * n,
    'marzo': ['marzo'] * n,
    'abril': ['abril'] * n,
    'mayo': ['mayo'] * n,
    'junio': ['junio'] * n,
    'julio': ['julio'] * n,
    'agosto': ['agosto'] * n,
    'septiembre': ['septiembre'] * n,
    'octubre': ['octubre'] * n,
    'noviembre': ['noviembre'] * n,
    'diciembre': ['diciembre'] * n
}

tabla=pd.DataFrame(tabla)

def pintar_faltantes(tabla, dt):
    def estilo_celda(row):
        estilos = ['']  # para la columna "Extensi√≥n"
        extension = row['Extensi√≥n']

        for col in tabla.columns[1:]:
            existe = len(
                dt[(dt['Extensi√≥n'] == extension) & (dt['mes'] == col)]
            ) > 0

            duracion = len(
                dt[(dt['Extensi√≥n'] == extension) & (dt['mes'] == col) & (dt['Duraci√≥n (s)'] == 0)&(dt['Num.LLamadas'] != 0)]
            ) > 0

            numllamadas = len(
                dt[(dt['Extensi√≥n'] == extension)  & (dt['mes'] == col)& (dt['Num.LLamadas'] == 0)]
            ) > 0

            # Prioridad de colores
            if not existe:
                estilos.append('background-color: lightgreen')
            elif duracion:
                estilos.append('background-color: lightblue')
            elif numllamadas:
                estilos.append('background-color: yellow')
            else:
                estilos.append('')

        return estilos

    return tabla.style.apply(estilo_celda, axis=1)
styled_tabla = pintar_faltantes(tabla, dt)
styled_tabla

styled_tabla.to_excel(
    'tabla_coloreada_2024.xlsx',
    engine='openpyxl',
    index=False
)

from google.colab import files
files.download('tabla_coloreada_2024.xlsx')

dt=datos[datos['a√±o']=='2025']
x=datos['Extensi√≥n'].unique()
n = len(x)

tabla = {
    'Extensi√≥n': x,
    'enero': ['enero'] * n,
    'febrero': ['febrero'] * n,
    'marzo': ['marzo'] * n,
    'abril': ['abril'] * n,
    'mayo': ['mayo'] * n,
    'junio': ['junio'] * n,
    'julio': ['julio'] * n
}

tabla=pd.DataFrame(tabla)

def pintar_faltantes(tabla, dt):
    def estilo_celda(row):
        estilos = ['']  # para la columna "Extensi√≥n"
        extension = row['Extensi√≥n']

        for col in tabla.columns[1:]:
            existe = len(
                dt[(dt['Extensi√≥n'] == extension) & (dt['mes'] == col)]
            ) > 0

            duracion = len(
                dt[(dt['Extensi√≥n'] == extension) & (dt['mes'] == col)& (dt['Duraci√≥n (s)'] == 0)&(dt['Num.LLamadas'] != 0)]
            ) > 0

            numllamadas = len(
                dt[(dt['Extensi√≥n'] == extension)& (dt['mes'] == col) & (dt['Num.LLamadas'] == 0)]
            ) > 0

            # Prioridad de colores
            if not existe:
                estilos.append('background-color: lightgreen')
            elif duracion:
                estilos.append('background-color: lightblue')
            elif numllamadas:
                estilos.append('background-color: yellow')
            else:
                estilos.append('')

        return estilos

    return tabla.style.apply(estilo_celda, axis=1)
styled_tabla = pintar_faltantes(tabla, dt)
styled_tabla

styled_tabla.to_excel(
    'tabla_coloreada_2025.xlsx',
    engine='openpyxl',
    index=False
)

from google.colab import files
files.download('tabla_coloreada_2025.xlsx')

dt=datos.copy()
x=datos['Extensi√≥n'].unique()
n = len(x)
tabla = {
    'Extensi√≥n': x,
    'enero2024': ['enero2024'] * n,
    'febrero2024': ['febrero2024'] * n,
    'marzo2024': ['marzo2024'] * n,
    'abril2024': ['abril2024'] * n,
    'mayo2024': ['mayo2024'] * n,
    'junio2024': ['junio2024'] * n,
    'julio2024': ['julio2024'] * n,
    'agosto2024': ['agosto2024'] * n,
    'septiembre2024': ['septiembre2024'] * n,
    'octubre2024': ['octubre2024'] * n,
    'noviembre2024': ['noviembre2024'] * n,
    'diciembre2024': ['diciembre2024'] * n,
    'enero2025': ['enero2025'] * n,
    'febrero2025': ['febrero2025'] * n,
    'marzo2025': ['marzo2025'] * n,
    'abril2025': ['abril2025'] * n,
    'mayo2025': ['mayo2025'] * n,
    'junio2025': ['junio2025'] * n,
    'julio2025': ['julio2025'] * n
}


tabla=pd.DataFrame(tabla)

def pintar_faltantes(tabla, dt):

    def estilo_celda(row):
        estilos = ['']  # para la columna Extensi√≥n
        extension = row['Extensi√≥n']

        for col in tabla.columns[1:]:
            mes = col[:-4]        # enero, febrero, etc.
            anio = col[-4:]       # 2024 o 2025

            dt_anio = dt[dt['a√±o'] == anio]

            existe = len(dt_anio[
                (dt_anio['Extensi√≥n'] == extension) &
                (dt_anio['mes'] == mes)
            ])>0

            duracion = len(dt_anio[
                (dt_anio['Extensi√≥n'] == extension) &
                (dt_anio['mes'] == mes) &
                (dt_anio['Duraci√≥n (s)'] == 0)&(dt_anio['Num.LLamadas'] != 0)
            ])>0

            numllamadas =len(dt_anio[
                (dt_anio['Extensi√≥n'] == extension) &
                (dt_anio['mes'] == mes) &
                (dt_anio['Num.LLamadas'] == 0)
            ])>0

            if not existe:
                estilos.append('background-color: lightgreen')
            elif duracion:
                estilos.append('background-color: lightblue')
            elif numllamadas:
                estilos.append('background-color: yellow')
            else:
                estilos.append('')

        return estilos

    return tabla.style.apply(estilo_celda, axis=1)

styled_tabla = pintar_faltantes(tabla,datos)
styled_tabla

styled_tabla.to_excel(
    'tabla_coloreada.xlsx',
    engine='openpyxl',
    index=False
)

from google.colab import files
files.download('tabla_coloreada.xlsx')

dt=datos[datos['a√±o']=='2025']
dt=dt[dt['Num.LLamadas']==0]
dt

x=datos['Extensi√≥n'].unique()
x=np.array(x,dtype=int)
x=np.sort(x)
x=pd.DataFrame(x,columns=['Extensi√≥n'])
mm = {
    'Terminales faltantes de 7100-7200': [],
    'Terminales faltantes de 7200-7300': [],
    'Terminales faltantes de 7300-7400': [],
    'Terminales faltantes de 7400-7500': [],
    'Terminales faltantes de 7500-7600': [],
    'Terminales faltantes de 7600-7700': [],
    'Terminales faltantes de 7700-7800': [],
    'Terminales faltantes de 7800-7900': []
}
columnas=list(mm.keys())
columnas
c=columnas[0]
int(c[-9:-5])
for i in columnas:
  n1=int(i[-9:-5])
  n2=int(i[-4:])
  existentes = x.loc[(x['Extensi√≥n'] >= n1) & (x['Extensi√≥n'] < n2), 'Extensi√≥n'].tolist()
  mm[i] = [n for n in range(n1, n2) if n not in existentes]
print('Terminales faltantes de 7100-7200: ', mm['Terminales faltantes de 7100-7200'])
print('Terminales faltantes de 7200-7300: ', mm['Terminales faltantes de 7200-7300'])
print('Terminales faltantes de 7300-7400: ', mm['Terminales faltantes de 7300-7400'])
print('Terminales faltantes de 7400-7500: ', mm['Terminales faltantes de 7400-7500'])
print('Terminales faltantes de 7500-7600: ', mm['Terminales faltantes de 7500-7600'])
print('Terminales faltantes de 7600-7700: ', mm['Terminales faltantes de 7600-7700'])
print('Terminales faltantes de 7700-7800: ', mm['Terminales faltantes de 7700-7800'])
print('Terminales faltantes de 7800-7900: ', mm['Terminales faltantes de 7800-7900'])

def generar_colores(n=612):
    colores = []
    for i in range(n):
        h = i / n
        r, g, b = [int(x * 255) for x in colorsys.hsv_to_rgb(h, 0.75, 0.9)]
        colores.append(f"#{r:02X}{g:02X}{b:02X}")
    return colores

import colorsys
vector612 = generar_colores()
print(vector612[:10])

datos2 = datos.copy()

dt = pd.crosstab(
    datos["Extensi√≥n"],
    [datos["a√±o"], datos["mes"]]
)

dt1 = dt.cumsum()

m1 = len(dt1.columns)
n1 = len(dt1.index)

dt11 = pd.DataFrame(columns=["Extensi√≥n", "a√±o", "mes"])
datos2 = pd.DataFrame(columns=datos.columns)

for j in range(m1):
    for i in range(1, n1):
        if dt1.loc[dt1.index[i], dt1.columns[j]] == dt1.loc[dt1.index[i-1], dt1.columns[j]]:
            dt2 = pd.DataFrame({
                'Extensi√≥n': [dt1.index[i]],
                'a√±o': [dt1.columns[j][0]],
                'mes': [dt1.columns[j][1]]
            })

            dt112 = pd.DataFrame({
                'Extensi√≥n': [dt1.index[i]],
                'Metropolitanas': 0,
                'Provinciales': 0,
                'Nacionales': 0,
                'Internacionales': 0,
                'M√≥viles': 0,
                'Otras': 0,
                'Desconocidas': 0,
                'Salientes Duraci√≥n': 0,
                'Salientes Duraci√≥n (s)': 0,
                'Entrantes Num.LLamadas': 0,
                'Entrantes Duraci√≥n': 0,
                'Entrantes Duraci√≥n (s)': 0,
                'Internas Num.LLamadas': 0,
                'Internas Duraci√≥n': 0,
                'Internas Duraci√≥n (s)': 0,
                'Entrantes': 0,
                'Salientes': 0,
                'Internas entrantes': 0,
                'Internas salientes': 0,
                'Num.LLamadas': 0,
                'Duraci√≥n': 0,
                'Duraci√≥n (s)': 0,
                'Coste': 0,
                'a√±o': [dt1.columns[j][0]],
                'mes': [dt1.columns[j][1]]
            })

            dt11 = pd.concat([dt11, dt2], ignore_index=True)
            datos2 = pd.concat([datos2, dt112], ignore_index=True)

x=datos2['Extensi√≥n'].unique()
for i in x:
  m=np.array(datos2[datos2['Extensi√≥n']==i])

import numpy as np
x=np.random.rand(10**4,10**5)

